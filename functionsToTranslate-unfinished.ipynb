{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BQR7UUO05yhh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
        "from processingDatasetDotless import *\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VkbgsTEJ6HMD"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad7f4f6d55a941eea0c183968225c7de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\awwab\\OneDrive\\Desktop\\Dotless\\venv-dotless\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\awwab\\.cache\\huggingface\\hub\\models--awwab-ahmed--bert-base-arabic-camelbert-mix-finetuned-AR-dotted-mediumPlus. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b868b8e08e484534962a0c0fd4b4b732",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/305k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4ce4720f21149b1bed6449e8de261f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/776k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "defb13a6c54a4e329343a77e0d9a4908",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d056a2f521944099ba46d0681fbc790",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\awwab\\OneDrive\\Desktop\\Dotless\\venv-dotless\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e28bf798f0c45b7b17805af69f344ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tf_model.h5:   0%|          | 0.00/530M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\awwab\\OneDrive\\Desktop\\Dotless\\venv-dotless\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at awwab-ahmed/bert-base-arabic-camelbert-mix-finetuned-AR-dotted-mediumPlus.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "new_checkpoint = \"awwab-ahmed/bert-base-arabic-camelbert-mix-finetuned-AR-dotted-mediumPlus\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(new_checkpoint)\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(new_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WH2_Di436cWH"
      },
      "outputs": [],
      "source": [
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "ArListem = ArabicLightStemmer()\n",
        "\n",
        "def get_candidate_word_probabilities(input_text, candidate_words):\n",
        "    tokenized_text = tokenizer.tokenize(input_text)\n",
        "    masked_word_index = tokenized_text.index('[MASK]')\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    input_ids = tf.constant([input_ids], dtype=tf.int32)  # tf.constant automatically adds batch dimension\n",
        "\n",
        "    # Perform prediction\n",
        "    output = model(input_ids)\n",
        "\n",
        "    # Extract logits for the masked word and apply softmax\n",
        "    predictions = tf.nn.softmax(output.logits[0, masked_word_index])\n",
        "\n",
        "    # Tokenize and verify candidate words\n",
        "    pre_tokenized_candidate_words = [tokenizer.tokenize(word) for word in candidate_words]\n",
        "    tokenized_candidate_words = []\n",
        "    verified_words = []\n",
        "    for word in pre_tokenized_candidate_words:\n",
        "        rep_word = candidate_words[pre_tokenized_candidate_words.index(word)]\n",
        "        if len(word) == 1:\n",
        "            tokenized_candidate_words.append(word[0])\n",
        "            verified_words.append(rep_word)\n",
        "\n",
        "        else:\n",
        "            #if rep_word not in arDictionary: continue\n",
        "            stem = ArListem.light_stem(rep_word)\n",
        "            root = ArListem.get_root()\n",
        "\n",
        "            if len(tokenizer.tokenize(stem)) == 1:\n",
        "                tokenized_candidate_words.append(tokenizer.tokenize(stem)[0])\n",
        "                verified_words.append(rep_word)\n",
        "\n",
        "            elif len(tokenizer.tokenize(root)) == 1:\n",
        "                tokenized_candidate_words.append(tokenizer.tokenize(root)[0])\n",
        "                verified_words.append(rep_word)\n",
        "\n",
        "    # Convert tokens to IDs\n",
        "    candidate_word_ids = [tokenizer.convert_tokens_to_ids([word]) for word in tokenized_candidate_words]\n",
        "\n",
        "    # Calculate probabilities for each candidate word\n",
        "    candidate_probabilities = {word: predictions[word_id].numpy() for word, word_id in zip(verified_words, candidate_word_ids)}\n",
        "\n",
        "    return candidate_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h-7x7pGV6dOY"
      },
      "outputs": [],
      "source": [
        "def generate_probabilties(example, gen_prob_func=get_candidate_word_probabilities):\n",
        "    input_text = example[\"Masked\"]\n",
        "    candidates = example[\"Options\"]\n",
        "\n",
        "    word_probabilities = gen_prob_func(input_text, candidates)\n",
        "\n",
        "    sorted_words = sorted(word_probabilities, key=word_probabilities.get, reverse=True)\n",
        "    if len(sorted_words) > 0:\n",
        "        most_probable_word = sorted_words[0]\n",
        "    else:\n",
        "        most_probable_word = None\n",
        "        #print(example[\"Target\"])\n",
        "\n",
        "    return word_probabilities, sorted_words, most_probable_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8aiGGrXD6fBy"
      },
      "outputs": [],
      "source": [
        "def single_test(specific_string=None, specific_index=None, num_eg=None, gen_prob_func=get_candidate_word_probabilities, example = None):\n",
        "    if example != None: example = example\n",
        "    if specific_string != None: example = mask_word(specific_string, specific_index)\n",
        "    word_probabilities, sorted_words, most_probable_word = generate_probabilties(example, gen_prob_func)\n",
        "    print(\"Length of words:\", len(sorted_words))\n",
        "    for word in sorted_words:\n",
        "        probability = word_probabilities[word]\n",
        "        print(f\"Word: '{word}', Probability: {probability:.10f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    print(\"Most probable word:\", most_probable_word)\n",
        "    print(\"Target word:\", example[\"Target\"])\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "    found = False\n",
        "    for i in range(len(sorted_words)):\n",
        "        if sorted_words[i] == example[\"Target\"]:\n",
        "            print(\"Sucess at probability level:\", i)\n",
        "            found = True\n",
        "            sucess_level = i\n",
        "            break\n",
        "    if not found: print(\"Not found.\")\n",
        "\n",
        "    print(\"Masked:\", example[\"Masked\"])\n",
        "    print(\"Options:\", example[\"Options\"])\n",
        "    print(\"Target:\", example[\"Target\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "dmPvOB2D6gtQ",
        "outputId": "1870d705-f1d7-46e3-a3b4-fe4e53ae2ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of words: 6\n",
            "Word: 'وبركاته', Probability: 0.9642794132\n",
            "Word: 'وتركابة', Probability: 0.0000004786\n",
            "Word: 'وتركابه', Probability: 0.0000002465\n",
            "Word: 'وبركابه', Probability: 0.0000002465\n",
            "Word: 'وبركابة', Probability: 0.0000002465\n",
            "Word: 'وتركاته', Probability: 0.0000000292\n",
            "\n",
            "Most probable word: وبركاته\n",
            "Target word: وبركاته\n",
            "------------------------------------------\n",
            "Sucess at probability level: 0\n",
            "Masked: السلام عليكم ورحمة الله [MASK]\n",
            "Options: ['ؤثزكءثه', 'ؤثزكءثة', 'ؤثزكءته', 'ؤثزكءتة', 'ؤثزكءبه', 'ؤثزكءبة', 'ؤثزكآثه', 'ؤثزكآثة', 'ؤثزكآته', 'ؤثزكآتة', 'ؤثزكآبه', 'ؤثزكآبة', 'ؤثزكإثه', 'ؤثزكإثة', 'ؤثزكإته', 'ؤثزكإتة', 'ؤثزكإبه', 'ؤثزكإبة', 'ؤثزكأثه', 'ؤثزكأثة', 'ؤثزكأته', 'ؤثزكأتة', 'ؤثزكأبه', 'ؤثزكأبة', 'ؤثزكاثه', 'ؤثزكاثة', 'ؤثزكاته', 'ؤثزكاتة', 'ؤثزكابه', 'ؤثزكابة', 'ؤثركءثه', 'ؤثركءثة', 'ؤثركءته', 'ؤثركءتة', 'ؤثركءبه', 'ؤثركءبة', 'ؤثركآثه', 'ؤثركآثة', 'ؤثركآته', 'ؤثركآتة', 'ؤثركآبه', 'ؤثركآبة', 'ؤثركإثه', 'ؤثركإثة', 'ؤثركإته', 'ؤثركإتة', 'ؤثركإبه', 'ؤثركإبة', 'ؤثركأثه', 'ؤثركأثة', 'ؤثركأته', 'ؤثركأتة', 'ؤثركأبه', 'ؤثركأبة', 'ؤثركاثه', 'ؤثركاثة', 'ؤثركاته', 'ؤثركاتة', 'ؤثركابه', 'ؤثركابة', 'ؤتزكءثه', 'ؤتزكءثة', 'ؤتزكءته', 'ؤتزكءتة', 'ؤتزكءبه', 'ؤتزكءبة', 'ؤتزكآثه', 'ؤتزكآثة', 'ؤتزكآته', 'ؤتزكآتة', 'ؤتزكآبه', 'ؤتزكآبة', 'ؤتزكإثه', 'ؤتزكإثة', 'ؤتزكإته', 'ؤتزكإتة', 'ؤتزكإبه', 'ؤتزكإبة', 'ؤتزكأثه', 'ؤتزكأثة', 'ؤتزكأته', 'ؤتزكأتة', 'ؤتزكأبه', 'ؤتزكأبة', 'ؤتزكاثه', 'ؤتزكاثة', 'ؤتزكاته', 'ؤتزكاتة', 'ؤتزكابه', 'ؤتزكابة', 'ؤتركءثه', 'ؤتركءثة', 'ؤتركءته', 'ؤتركءتة', 'ؤتركءبه', 'ؤتركءبة', 'ؤتركآثه', 'ؤتركآثة', 'ؤتركآته', 'ؤتركآتة', 'ؤتركآبه', 'ؤتركآبة', 'ؤتركإثه', 'ؤتركإثة', 'ؤتركإته', 'ؤتركإتة', 'ؤتركإبه', 'ؤتركإبة', 'ؤتركأثه', 'ؤتركأثة', 'ؤتركأته', 'ؤتركأتة', 'ؤتركأبه', 'ؤتركأبة', 'ؤتركاثه', 'ؤتركاثة', 'ؤتركاته', 'ؤتركاتة', 'ؤتركابه', 'ؤتركابة', 'ؤبزكءثه', 'ؤبزكءثة', 'ؤبزكءته', 'ؤبزكءتة', 'ؤبزكءبه', 'ؤبزكءبة', 'ؤبزكآثه', 'ؤبزكآثة', 'ؤبزكآته', 'ؤبزكآتة', 'ؤبزكآبه', 'ؤبزكآبة', 'ؤبزكإثه', 'ؤبزكإثة', 'ؤبزكإته', 'ؤبزكإتة', 'ؤبزكإبه', 'ؤبزكإبة', 'ؤبزكأثه', 'ؤبزكأثة', 'ؤبزكأته', 'ؤبزكأتة', 'ؤبزكأبه', 'ؤبزكأبة', 'ؤبزكاثه', 'ؤبزكاثة', 'ؤبزكاته', 'ؤبزكاتة', 'ؤبزكابه', 'ؤبزكابة', 'ؤبركءثه', 'ؤبركءثة', 'ؤبركءته', 'ؤبركءتة', 'ؤبركءبه', 'ؤبركءبة', 'ؤبركآثه', 'ؤبركآثة', 'ؤبركآته', 'ؤبركآتة', 'ؤبركآبه', 'ؤبركآبة', 'ؤبركإثه', 'ؤبركإثة', 'ؤبركإته', 'ؤبركإتة', 'ؤبركإبه', 'ؤبركإبة', 'ؤبركأثه', 'ؤبركأثة', 'ؤبركأته', 'ؤبركأتة', 'ؤبركأبه', 'ؤبركأبة', 'ؤبركاثه', 'ؤبركاثة', 'ؤبركاته', 'ؤبركاتة', 'ؤبركابه', 'ؤبركابة', 'وثزكءثه', 'وثزكءثة', 'وثزكءته', 'وثزكءتة', 'وثزكءبه', 'وثزكءبة', 'وثزكآثه', 'وثزكآثة', 'وثزكآته', 'وثزكآتة', 'وثزكآبه', 'وثزكآبة', 'وثزكإثه', 'وثزكإثة', 'وثزكإته', 'وثزكإتة', 'وثزكإبه', 'وثزكإبة', 'وثزكأثه', 'وثزكأثة', 'وثزكأته', 'وثزكأتة', 'وثزكأبه', 'وثزكأبة', 'وثزكاثه', 'وثزكاثة', 'وثزكاته', 'وثزكاتة', 'وثزكابه', 'وثزكابة', 'وثركءثه', 'وثركءثة', 'وثركءته', 'وثركءتة', 'وثركءبه', 'وثركءبة', 'وثركآثه', 'وثركآثة', 'وثركآته', 'وثركآتة', 'وثركآبه', 'وثركآبة', 'وثركإثه', 'وثركإثة', 'وثركإته', 'وثركإتة', 'وثركإبه', 'وثركإبة', 'وثركأثه', 'وثركأثة', 'وثركأته', 'وثركأتة', 'وثركأبه', 'وثركأبة', 'وثركاثه', 'وثركاثة', 'وثركاته', 'وثركاتة', 'وثركابه', 'وثركابة', 'وتزكءثه', 'وتزكءثة', 'وتزكءته', 'وتزكءتة', 'وتزكءبه', 'وتزكءبة', 'وتزكآثه', 'وتزكآثة', 'وتزكآته', 'وتزكآتة', 'وتزكآبه', 'وتزكآبة', 'وتزكإثه', 'وتزكإثة', 'وتزكإته', 'وتزكإتة', 'وتزكإبه', 'وتزكإبة', 'وتزكأثه', 'وتزكأثة', 'وتزكأته', 'وتزكأتة', 'وتزكأبه', 'وتزكأبة', 'وتزكاثه', 'وتزكاثة', 'وتزكاته', 'وتزكاتة', 'وتزكابه', 'وتزكابة', 'وتركءثه', 'وتركءثة', 'وتركءته', 'وتركءتة', 'وتركءبه', 'وتركءبة', 'وتركآثه', 'وتركآثة', 'وتركآته', 'وتركآتة', 'وتركآبه', 'وتركآبة', 'وتركإثه', 'وتركإثة', 'وتركإته', 'وتركإتة', 'وتركإبه', 'وتركإبة', 'وتركأثه', 'وتركأثة', 'وتركأته', 'وتركأتة', 'وتركأبه', 'وتركأبة', 'وتركاثه', 'وتركاثة', 'وتركاته', 'وتركاتة', 'وتركابه', 'وتركابة', 'وبزكءثه', 'وبزكءثة', 'وبزكءته', 'وبزكءتة', 'وبزكءبه', 'وبزكءبة', 'وبزكآثه', 'وبزكآثة', 'وبزكآته', 'وبزكآتة', 'وبزكآبه', 'وبزكآبة', 'وبزكإثه', 'وبزكإثة', 'وبزكإته', 'وبزكإتة', 'وبزكإبه', 'وبزكإبة', 'وبزكأثه', 'وبزكأثة', 'وبزكأته', 'وبزكأتة', 'وبزكأبه', 'وبزكأبة', 'وبزكاثه', 'وبزكاثة', 'وبزكاته', 'وبزكاتة', 'وبزكابه', 'وبزكابة', 'وبركءثه', 'وبركءثة', 'وبركءته', 'وبركءتة', 'وبركءبه', 'وبركءبة', 'وبركآثه', 'وبركآثة', 'وبركآته', 'وبركآتة', 'وبركآبه', 'وبركآبة', 'وبركإثه', 'وبركإثة', 'وبركإته', 'وبركإتة', 'وبركإبه', 'وبركإبة', 'وبركأثه', 'وبركأثة', 'وبركأته', 'وبركأتة', 'وبركأبه', 'وبركأبة', 'وبركاثه', 'وبركاثة', 'وبركاته', 'وبركاتة', 'وبركابه', 'وبركابة']\n",
            "Target: وبركاته\n"
          ]
        }
      ],
      "source": [
        "single_test('السلام عليكم ورحمة الله وبركاته', 4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
