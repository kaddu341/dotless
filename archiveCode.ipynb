{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_word_probabilities(input_text, candidate_words):\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    masked_word_index = tokenized_text.index('[MASK]')\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze_(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "\n",
    "    predictions = output.logits[0, masked_word_index].softmax(dim=0)\n",
    "\n",
    "    candidate_probabilities = {}\n",
    "    word_idss = []\n",
    "    probss = []\n",
    "    avg_probs = []\n",
    "    for word in candidate_words:\n",
    "        # Tokenize the candidate word\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "\n",
    "        # Convert the tokens to ids\n",
    "        word_ids = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "        word_idss.append(word_ids)\n",
    "        \n",
    "        sum = 0\n",
    "        for word_id in word_ids:\n",
    "            prediction_w = predictions[word_id].item()\n",
    "            probss.append(prediction_w)\n",
    "            sum = sum + prediction_w\n",
    "            \n",
    "        # Calculate the average probability of the tokens\n",
    "        avg_prob = sum / len(word_ids)\n",
    "        avg_probs.append(avg_prob)\n",
    "        candidate_probabilities[word] = avg_prob\n",
    "        \n",
    "    print(word_idss)\n",
    "    idtowordIDS = [tokenizer.convert_ids_to_tokens(word_id) for word_id in word_idss]\n",
    "    print(idtowordIDS)\n",
    "    print(probss)\n",
    "    print(avg_probs)\n",
    "    \"\"\"\n",
    "    # Tokenize and convert all candidate words to ids at once\n",
    "\n",
    "    tokenized_candidate_words_uc = [tokenizer.tokenize(word)for word in candidate_words]\n",
    "    print(tokenized_candidate_words_uc)\n",
    "    \n",
    "    \n",
    "   \n",
    "    tokenized_candidate_words = [\"\".join(word) for word in tokenized_candidate_words_uc]\n",
    "    print(tokenized_candidate_words)\n",
    "    \n",
    "    candidate_word_ids = [tokenizer.convert_tokens_to_ids(TCW) for TCW in tokenized_candidate_words]\n",
    "    print(candidate_word_ids)\n",
    "    \n",
    "    c_candidate_word_ids = [word_id for word_id in candidate_word_ids if word_id != 1]\n",
    "    print(c_candidate_word_ids)\n",
    "    \n",
    "    t_txken = tokenized_candidate_words[0]\n",
    "    print(t_txken)\n",
    "    \n",
    "    t_predict = predictions[tokenizer.convert_tokens_to_ids(t_txken)].item()\n",
    "    print(t_predict)\n",
    "    \n",
    "    \n",
    "    # Use a list comprehension to populate the dictionary\n",
    "    candidate_probabilities = {word: predictions[word_id].item() for word, word_id in zip(candidate_words, candidate_word_ids)}\n",
    "    \"\"\"\n",
    "\n",
    "    return candidate_probabilities\n",
    "\n",
    "\n",
    "def generate_probabilties_end(example):\n",
    "    input_text = example[\"Masked\"]\n",
    "    candidates = example[\"Options\"]\n",
    "\n",
    "            \n",
    "    word_probabilities = get_candidate_word_probabilities(input_text, candidates)\n",
    "\n",
    "    sorted_words = sorted(word_probabilities, key=word_probabilities.get, reverse=True)\n",
    "    most_probable_word = sorted_words[0]\n",
    "\n",
    "    return word_probabilities, sorted_words, most_probable_word"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
