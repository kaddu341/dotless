{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import * #suprisingly the only import needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of arabic letters from dotted to dotless. (credit: @kaddu341)\n",
    "letter_dict = {\n",
    "    \"ا\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ب\": \"ب\",\n",
    "    \"ت\": \"ب\",\n",
    "    \"ث\": \"ب\",\n",
    "    \"ج\": \"ح\",\n",
    "    \"ح\": \"ح\",\n",
    "    \"خ\": \"ح\",\n",
    "    \"د\": \"د\",\n",
    "    \"ذ\": \"د\",\n",
    "    \"ر\": \"ر\",\n",
    "    \"ز\": \"ر\",\n",
    "    \"س\": \"س\",\n",
    "    \"ش\": \"س\",\n",
    "    \"ص\": \"ص\",\n",
    "    \"ض\": \"ص\",\n",
    "    \"ط\": \"ط\",\n",
    "    \"ظ\": \"ط\",\n",
    "    \"ع\": \"ع\",\n",
    "    \"غ\": \"ع\",\n",
    "    \"ف\": \"ف\",\n",
    "    \"ق\": \"ف\",\n",
    "    \"ك\": \"ك\",\n",
    "    \"ل\": \"ل\",\n",
    "    \"م\": \"م\",\n",
    "    \"ن\": \"ن\",\n",
    "    \"و\": \"و\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ه\": \"ه\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ي\": \"ى\",\n",
    "    \"ى\": \"ى\",\n",
    "    \"ئ\": \"ى\",\n",
    "    \" \": \" \",\n",
    "}\n",
    "letter_set = set(letter_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces multiple spaces, or whatever char designated by the perameter into one char. (helper function)\n",
    "def replace(string, char):\n",
    "    while char+char in string:\n",
    "        string = string.replace(char+char, char)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential function, parses arabic dotted text into dotless text. \n",
    "# Returns a dict with keys \"clean\" and \"dotless\". \n",
    "# This format is required for the datasets.map() function.\n",
    "\n",
    "#unused, see below functions, also this is not batched\n",
    "def parse_text(text):\n",
    "    \n",
    "  text = text[\"text\"].strip(\" \")\n",
    "  \n",
    "  clean_chars = [char for char in text if char in letter_set]\n",
    "  clean_string = ''.join(clean_chars)\n",
    "\n",
    "  dotless_chars = [letter_dict[char] for char in clean_chars]\n",
    "  dotless_string = ''.join(dotless_chars)\n",
    "      \n",
    "  clean_string = replace(clean_string, ' ')\n",
    "  dotless_string = replace(dotless_string, '_')\n",
    "  \n",
    "  return {\"clean\": clean_string, \"dotless\": dotless_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_text(example):\n",
    "  \n",
    "  clean = []\n",
    "  \n",
    "  for text in example[\"text\"]:\n",
    "    text = text.strip(\" \")\n",
    "    \n",
    "    clean_chars = [char for char in text if char in letter_set]\n",
    "    clean_string = ''.join(clean_chars)\n",
    "    \n",
    "    clean_string = replace(clean_string, ' ')\n",
    "    clean.append(clean_string)\n",
    "  \n",
    "  return {\"clean\": clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dotless_text(example):\n",
    "  dotless = []\n",
    "  for text in example[\"clean\"]:\n",
    "    dotless_chars = [letter_dict[char] for char in text]\n",
    "    dotless_string = ''.join(dotless_chars)\n",
    "        \n",
    "    dotless_string = replace(dotless_string, '_')\n",
    "    dotless.append(dotless_string)\n",
    "  \n",
    "  return {\"dotless\": dotless}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "text = {\"text\": \"ولا تنسى التسجيل معنا لتستفيد بكل جديد\"}\n",
    "print(text, \"\\n\")\n",
    "\n",
    "text = parse_clean_text(text)\n",
    "print(text, \"\\n\")\n",
    "\n",
    "text = parse_dotless_text(text)\n",
    "print(text, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the datasets.filter() function to filter out examples/rows that have examples of less than 20 chars.\n",
    "#def filterEmpty(x):\n",
    "#    return not len(x['clean']) < 20\n",
    "\n",
    "#Not used, lambda function used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, saving, and trimming Oscar (unshuffled_deduplicated_ar) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the oscar unshuffled_deduplicated_ar dataset from huggingface \n",
    "#datasetLoad = load_dataset(\"oscar\", \"unshuffled_deduplicated_ar\") #Oscar Arabic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset to \"ar-arrow-datasets/\" Apache Arrow datafolder\n",
    "#datasetLoad.save_to_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from file\n",
    "full_dataset = load_from_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying maps, filters, and splits\n",
    "This process of applying maps and filters is very slow especially for big datasets. \n",
    "\n",
    "It is possible to implement a batched approach similar ot batched tokenization that would speed it up greatly. \n",
    "\n",
    "In the .map() add perameter batched=True. However the function called needs to be compatible with this approach.\n",
    "\n",
    "I will implement this soon.\n",
    "\n",
    "Generally this is not the way to handle big data. However this is fine for the AR-dotless-small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the index of the #th space.\n",
    "def num_word(text, num):\n",
    "    space_count = 0\n",
    "    index = -1\n",
    "    for i in range(num):\n",
    "        index = text.find(' ', index + 1)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "exampleToWordsLen = [] # just to keep track of how many chunks an old  made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exampleToWordsLen[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunks examples into smaller examples, at @words_per_chuck words each\n",
    "def chunk_examples(examples, words_per_chunk=8):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"clean\"]:\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "        exampleToWordsLen.append(math.ceil(num_words/words_per_chunk)) # just to keep track of how many chunks an old example made\n",
    "        start = 0\n",
    "        while start < num_words:\n",
    "            end = min(start + words_per_chunk, num_words)\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "    \n",
    "    return {\"clean\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = full_dataset['train'].select(range(100000)) #Trim to number of examples\n",
    "dataset = dataset.remove_columns([\"id\"])\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['text']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')\n",
    "        \n",
    "dataset = dataset.map(parse_clean_text, batched=True, remove_columns=\"text\") #Apply the function (convert dotted to dotless)\n",
    "set_state_1 = dataset[\"clean\"][0]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 40:\n",
    "        a += 1\n",
    "print(a, '\\n')\n",
    "\n",
    "dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names) # applies teh chuncking\n",
    "set_state_2 = dataset[\"clean\"][0:exampleToWordsLen[0]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.map(parse_dotless_text, batched=True) #Apply the function (convert dotted to dotless)\n",
    "set_state_3 = dataset[\"dotless\"][0:exampleToWordsLen[0]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['clean']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set_state_1,\"\\n\\n\")\n",
    "print(set_state_2,\"\\n\\n\")\n",
    "print(set_state_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 20:\n",
    "        a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clean = dataset.train_test_split(train_size= 0.999999999) #create 80/20 split into train and test datasets\n",
    "del dataset_clean['test'] # no longer using a test set\n",
    "dataset_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and upload to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 4446330/4446330 [00:14<00:00, 313548.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#save the cleaned dataset to \"clean-arrow-datasets/\" Apache Arrow datafolder\n",
    "dataset_clean.save_to_disk(\"AR-dotless-mediumPlus-arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2224/2224 [00:27<00:00, 81.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2224/2224 [00:26<00:00, 84.88ba/s]]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 2/2 [01:36<00:00, 48.38s/it]\n"
     ]
    }
   ],
   "source": [
    "#required token id. Run 'huggingface-cli login'\n",
    "dataset_clean.push_to_hub(\"dot-ammar/AR-dotless-mediumPlus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
