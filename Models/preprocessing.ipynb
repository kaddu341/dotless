{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ammar/Developer/transformers-course/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import * #suprisingly the only import needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of arabic letters from dotted to dotless. (credit: @kaddu341)\n",
    "letter_dict = {\n",
    "    \"ا\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ب\": \"ب\",\n",
    "    \"ت\": \"ب\",\n",
    "    \"ث\": \"ب\",\n",
    "    \"ج\": \"ح\",\n",
    "    \"ح\": \"ح\",\n",
    "    \"خ\": \"ح\",\n",
    "    \"د\": \"د\",\n",
    "    \"ذ\": \"د\",\n",
    "    \"ر\": \"ر\",\n",
    "    \"ز\": \"ر\",\n",
    "    \"س\": \"س\",\n",
    "    \"ش\": \"س\",\n",
    "    \"ص\": \"ص\",\n",
    "    \"ض\": \"ص\",\n",
    "    \"ط\": \"ط\",\n",
    "    \"ظ\": \"ط\",\n",
    "    \"ع\": \"ع\",\n",
    "    \"غ\": \"ع\",\n",
    "    \"ف\": \"ف\",\n",
    "    \"ق\": \"ف\",\n",
    "    \"ك\": \"ك\",\n",
    "    \"ل\": \"ل\",\n",
    "    \"م\": \"م\",\n",
    "    \"ن\": \"ن\",\n",
    "    \"و\": \"و\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ه\": \"ه\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ي\": \"ى\",\n",
    "    \"ى\": \"ى\",\n",
    "    \"ئ\": \"ى\",\n",
    "    \" \": \" \",\n",
    "}\n",
    "letter_set = set(letter_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces multiple spaces, or whatever char designated by the perameter into one char. (helper function)\n",
    "def replace(string, char):\n",
    "    while char+char in string:\n",
    "        string = string.replace(char+char, char)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential function, parses arabic dotted text into dotless text. \n",
    "# Returns a dict with keys \"clean\" and \"dotless\". \n",
    "# This format is required for the datasets.map() function.\n",
    "\n",
    "def parse_text(text):\n",
    "    \n",
    "  text = text[\"text\"].strip(\" \")\n",
    "  \n",
    "  clean_chars = [char for char in text if char in letter_set]\n",
    "  clean_string = ''.join(clean_chars)\n",
    "\n",
    "  output_chars = [letter_dict[char] for char in clean_chars]\n",
    "  output_string = ''.join(output_chars)\n",
    "      \n",
    "  clean_string = replace(clean_string, ' ')\n",
    "  output_string = replace(output_string, '_')\n",
    "  \n",
    "  return {\"clean\": clean_string, \"dotless\": output_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the datasets.filter() function to filter out examples/rows that have examples of less than 20 chars.\n",
    "def filterEmpty(x):\n",
    "    return not len(x[\"clean\"]) < 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, saving, and trimming Oscar (unshuffled_deduplicated_ar) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the oscar unshuffled_deduplicated_ar dataset from huggingface \n",
    "datasetLoad = load_dataset(\"oscar\", \"unshuffled_deduplicated_ar\") #Oscar Arabic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (67/67 shards): 100%|██████████| 9006977/9006977 [01:20<00:00, 112363.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#save the dataset to \"ar-arrow-datasets/\" Apache Arrow datafolder\n",
    "datasetLoad.save_to_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from file\n",
    "full_dataset = load_from_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 9006977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying maps, filters, and splits\n",
    "This process of applying maps and filters is very slow especially for big datasets. \n",
    "\n",
    "It is possible to implement a batched approach similar ot batched tokenization that would speed it up greatly. \n",
    "\n",
    "In the .map() add perameter batched=True. However the function called needs to be compatible with this approach.\n",
    "\n",
    "I will implement this soon.\n",
    "\n",
    "Generally this is not the way to handle big data. However this is fine for the AR-dotless-small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text'],\n",
      "    num_rows: 100000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:22<00:00, 4495.29 examples/s]\n",
      "Filter: 100%|██████████| 100000/100000 [00:02<00:00, 42861.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 99573\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = full_dataset['train'].select(range(100000)) #Trim to number of examples\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.map(parse_text) #Apply the function (convert dotted to dotless)\n",
    "dataset = dataset.remove_columns([\"text\", \"id\"]) #remove the \"text\" and \"id\" columns\n",
    "dataset = dataset.filter(filterEmpty) #Apply the filter to remove useless empty examples.\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 20:\n",
    "        a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 79658\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 19915\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean = dataset.train_test_split(train_size= 0.8) #create 80/20 split into train and test datasets\n",
    "dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and upload to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 79658/79658 [00:00<00:00, 251503.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 19915/19915 [00:00<00:00, 242508.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#save the cleaned dataset to \"clean-arrow-datasets/\" Apache Arrow datafolder\n",
    "dataset_clean.save_to_disk(\"clean-arrow-datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:01<00:00, 37.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:01<00:00, 37.47ba/s]s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 2/2 [00:29<00:00, 14.70s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 2/2 [00:00<00:00,  8.21it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 36.05ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:04<00:00,  4.77s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Downloading metadata: 100%|██████████| 622/622 [00:00<00:00, 6.41MB/s]\n"
     ]
    }
   ],
   "source": [
    "#required token id. Run 'huggingface-cli login'\n",
    "dataset_clean.push_to_hub(\"dot-ammar/AR-dotless-small\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
