{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ammar/Developer/transformers-course/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import * #suprisingly the only import needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of arabic letters from dotted to dotless. (credit: @kaddu341)\n",
    "letter_dict = {\n",
    "    \"ا\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ب\": \"ب\",\n",
    "    \"ت\": \"ب\",\n",
    "    \"ث\": \"ب\",\n",
    "    \"ج\": \"ح\",\n",
    "    \"ح\": \"ح\",\n",
    "    \"خ\": \"ح\",\n",
    "    \"د\": \"د\",\n",
    "    \"ذ\": \"د\",\n",
    "    \"ر\": \"ر\",\n",
    "    \"ز\": \"ر\",\n",
    "    \"س\": \"س\",\n",
    "    \"ش\": \"س\",\n",
    "    \"ص\": \"ص\",\n",
    "    \"ض\": \"ص\",\n",
    "    \"ط\": \"ط\",\n",
    "    \"ظ\": \"ط\",\n",
    "    \"ع\": \"ع\",\n",
    "    \"غ\": \"ع\",\n",
    "    \"ف\": \"ف\",\n",
    "    \"ق\": \"ف\",\n",
    "    \"ك\": \"ك\",\n",
    "    \"ل\": \"ل\",\n",
    "    \"م\": \"م\",\n",
    "    \"ن\": \"ن\",\n",
    "    \"و\": \"و\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ه\": \"ه\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ي\": \"ى\",\n",
    "    \"ى\": \"ى\",\n",
    "    \"ئ\": \"ى\",\n",
    "    \" \": \" \",\n",
    "}\n",
    "letter_set = set(letter_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces multiple spaces, or whatever char designated by the perameter into one char. (helper function)\n",
    "def replace(string, char):\n",
    "    while char+char in string:\n",
    "        string = string.replace(char+char, char)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential function, parses arabic dotted text into dotless text. \n",
    "# Returns a dict with keys \"clean\" and \"dotless\". \n",
    "# This format is required for the datasets.map() function.\n",
    "\n",
    "#unused, see below functions\n",
    "def parse_text(text):\n",
    "    \n",
    "  text = text[\"text\"].strip(\" \")\n",
    "  \n",
    "  clean_chars = [char for char in text if char in letter_set]\n",
    "  clean_string = ''.join(clean_chars)\n",
    "\n",
    "  dotless_chars = [letter_dict[char] for char in clean_chars]\n",
    "  dotless_string = ''.join(dotless_chars)\n",
    "      \n",
    "  clean_string = replace(clean_string, ' ')\n",
    "  dotless_string = replace(dotless_string, '_')\n",
    "  \n",
    "  return {\"clean\": clean_string, \"dotless\": dotless_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_text(text):\n",
    "    \n",
    "  text = text[\"text\"].strip(\" \")\n",
    "  \n",
    "  clean_chars = [char for char in text if char in letter_set]\n",
    "  clean_string = ''.join(clean_chars)\n",
    "      \n",
    "  clean_string = replace(clean_string, ' ')\n",
    "\n",
    "  \n",
    "  return {\"clean\": clean_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dotless_text(text):\n",
    "    \n",
    "  text = text[\"clean\"].strip(\" \")\n",
    "\n",
    "  dotless_chars = [letter_dict[char] for char in text]\n",
    "  dotless_string = ''.join(text)\n",
    "      \n",
    "  dotless_string = replace(dotless_string, '_')\n",
    "  \n",
    "  return {\"dotless\": dotless_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the datasets.filter() function to filter out examples/rows that have examples of less than 20 chars.\n",
    "#def filterEmpty(x):\n",
    "#    return not len(x['clean']) < 20\n",
    "\n",
    "#Not used, lambda function used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, saving, and trimming Oscar (unshuffled_deduplicated_ar) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the oscar unshuffled_deduplicated_ar dataset from huggingface \n",
    "#datasetLoad = load_dataset(\"oscar\", \"unshuffled_deduplicated_ar\") #Oscar Arabic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset to \"ar-arrow-datasets/\" Apache Arrow datafolder\n",
    "#datasetLoad.save_to_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from file\n",
    "full_dataset = load_from_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 9006977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying maps, filters, and splits\n",
    "This process of applying maps and filters is very slow especially for big datasets. \n",
    "\n",
    "It is possible to implement a batched approach similar ot batched tokenization that would speed it up greatly. \n",
    "\n",
    "In the .map() add perameter batched=True. However the function called needs to be compatible with this approach.\n",
    "\n",
    "I will implement this soon.\n",
    "\n",
    "Generally this is not the way to handle big data. However this is fine for the AR-dotless-small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the index of the #th space.\n",
    "def num_word(text, num):\n",
    "    space_count = 0\n",
    "    index = -1\n",
    "    for i in range(num):\n",
    "        index = text.find(' ', index + 1)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks examples into smaller examples, at @words_per_chuck words each\n",
    "def chunk_examples(examples, words_per_chunk=8):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"clean\"]:\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "        start = 0\n",
    "        while start < num_words:\n",
    "            end = min(start + words_per_chunk, num_words)\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "    \n",
    "    return {\"clean\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['clean'],\n",
      "    num_rows: 100\n",
      "}) \n",
      "\n",
      "0 \n",
      "\n",
      "Dataset({\n",
      "    features: ['clean'],\n",
      "    num_rows: 4602\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 4602\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 4567\n",
      "}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = full_dataset['train'].select(range(100)) #Trim to number of examples\n",
    "dataset = dataset.remove_columns([\"id\"])\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['text']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.map(parse_clean_text, remove_columns=\"text\") #Apply the function (convert dotted to dotless)\n",
    "set_state_1 = dataset[\"clean\"][0]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 40:\n",
    "        a += 1\n",
    "print(a, '\\n')\n",
    "\n",
    "dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names) # applies teh chuncking\n",
    "set_state_2 = [dataset[\"clean\"][0], dataset[\"clean\"][1]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.map(parse_dotless_text) #Apply the function (convert dotted to dotless)\n",
    "set_state_3 = [dataset[\"dotless\"][0], dataset[\"dotless\"][1]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['clean']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل معنا لتستفيد ب \n",
      "\n",
      "\n",
      "['مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة', 'معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل'] \n",
      "\n",
      "\n",
      "['مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة', 'معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل']\n"
     ]
    }
   ],
   "source": [
    "print(set_state_1,\"\\n\\n\")\n",
    "print(set_state_2,\"\\n\\n\")\n",
    "print(set_state_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 20:\n",
    "        a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 79658\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 19915\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean = dataset.train_test_split(train_size= 0.8) #create 80/20 split into train and test datasets\n",
    "dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and upload to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 79658/79658 [00:00<00:00, 251503.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 19915/19915 [00:00<00:00, 242508.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#save the cleaned dataset to \"clean-arrow-datasets/\" Apache Arrow datafolder\n",
    "dataset_clean.save_to_disk(\"clean-arrow-datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:01<00:00, 37.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:01<00:00, 37.47ba/s]s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 2/2 [00:29<00:00, 14.70s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 2/2 [00:00<00:00,  8.21it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 36.05ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:04<00:00,  4.77s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Downloading metadata: 100%|██████████| 622/622 [00:00<00:00, 6.41MB/s]\n"
     ]
    }
   ],
   "source": [
    "#required token id. Run 'huggingface-cli login'\n",
    "dataset_clean.push_to_hub(\"dot-ammar/AR-dotless-small\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
