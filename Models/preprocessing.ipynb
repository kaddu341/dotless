{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import * #suprisingly the only import needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of arabic letters from dotted to dotless. (credit: @kaddu341)\n",
    "letter_dict = {\n",
    "    \"ا\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ب\": \"ب\",\n",
    "    \"ت\": \"ب\",\n",
    "    \"ث\": \"ب\",\n",
    "    \"ج\": \"ح\",\n",
    "    \"ح\": \"ح\",\n",
    "    \"خ\": \"ح\",\n",
    "    \"د\": \"د\",\n",
    "    \"ذ\": \"د\",\n",
    "    \"ر\": \"ر\",\n",
    "    \"ز\": \"ر\",\n",
    "    \"س\": \"س\",\n",
    "    \"ش\": \"س\",\n",
    "    \"ص\": \"ص\",\n",
    "    \"ض\": \"ص\",\n",
    "    \"ط\": \"ط\",\n",
    "    \"ظ\": \"ط\",\n",
    "    \"ع\": \"ع\",\n",
    "    \"غ\": \"ع\",\n",
    "    \"ف\": \"ف\",\n",
    "    \"ق\": \"ف\",\n",
    "    \"ك\": \"ك\",\n",
    "    \"ل\": \"ل\",\n",
    "    \"م\": \"م\",\n",
    "    \"ن\": \"ن\",\n",
    "    \"و\": \"و\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ه\": \"ه\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ي\": \"ى\",\n",
    "    \"ى\": \"ى\",\n",
    "    \"ئ\": \"ى\",\n",
    "    \" \": \" \",\n",
    "}\n",
    "letter_set = set(letter_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces multiple spaces, or whatever char designated by the perameter into one char. (helper function)\n",
    "def replace(string, char):\n",
    "    while char+char in string:\n",
    "        string = string.replace(char+char, char)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential function, parses arabic dotted text into dotless text. \n",
    "# Returns a dict with keys \"clean\" and \"dotless\". \n",
    "# This format is required for the datasets.map() function.\n",
    "\n",
    "#unused, see below functions, also this is not batched\n",
    "def parse_text(text):\n",
    "    \n",
    "  text = text[\"text\"].strip(\" \")\n",
    "  \n",
    "  clean_chars = [char for char in text if char in letter_set]\n",
    "  clean_string = ''.join(clean_chars)\n",
    "\n",
    "  dotless_chars = [letter_dict[char] for char in clean_chars]\n",
    "  dotless_string = ''.join(dotless_chars)\n",
    "      \n",
    "  clean_string = replace(clean_string, ' ')\n",
    "  dotless_string = replace(dotless_string, '_')\n",
    "  \n",
    "  return {\"clean\": clean_string, \"dotless\": dotless_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_text(example):\n",
    "  \n",
    "  clean = []\n",
    "  \n",
    "  for text in example[\"text\"]:\n",
    "    text = text.strip(\" \")\n",
    "    \n",
    "    clean_chars = [char for char in text if char in letter_set]\n",
    "    clean_string = ''.join(clean_chars)\n",
    "    \n",
    "    clean_string = replace(clean_string, ' ')\n",
    "    clean.append(clean_string)\n",
    "  \n",
    "  return {\"clean\": clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dotless_text(example):\n",
    "  dotless = []\n",
    "  for text in example[\"clean\"]:\n",
    "    dotless_chars = [letter_dict[char] for char in text]\n",
    "    dotless_string = ''.join(dotless_chars)\n",
    "        \n",
    "    dotless_string = replace(dotless_string, '_')\n",
    "    dotless.append(dotless_string)\n",
    "  \n",
    "  return {\"dotless\": dotless}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'ولا تنسى التسجيل معنا لتستفيد بكل جديد'} \n",
      "\n",
      "{'clean': ['و', 'ل', 'ا', '', 'ت', 'ن', 'س', 'ى', '', 'ا', 'ل', 'ت', 'س', 'ج', 'ي', 'ل', '', 'م', 'ع', 'ن', 'ا', '', 'ل', 'ت', 'س', 'ت', 'ف', 'ي', 'د', '', 'ب', 'ك', 'ل', '', 'ج', 'د', 'ي', 'د']} \n",
      "\n",
      "{'dotless': ['و', 'ل', 'ا', '', 'ب', 'ن', 'س', 'ى', '', 'ا', 'ل', 'ب', 'س', 'ح', 'ى', 'ل', '', 'م', 'ع', 'ن', 'ا', '', 'ل', 'ب', 'س', 'ب', 'ف', 'ى', 'د', '', 'ب', 'ك', 'ل', '', 'ح', 'د', 'ى', 'د']} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "text = {\"text\": \"ولا تنسى التسجيل معنا لتستفيد بكل جديد\"}\n",
    "print(text, \"\\n\")\n",
    "\n",
    "text = parse_clean_text(text)\n",
    "print(text, \"\\n\")\n",
    "\n",
    "text = parse_dotless_text(text)\n",
    "print(text, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the datasets.filter() function to filter out examples/rows that have examples of less than 20 chars.\n",
    "#def filterEmpty(x):\n",
    "#    return not len(x['clean']) < 20\n",
    "\n",
    "#Not used, lambda function used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, saving, and trimming Oscar (unshuffled_deduplicated_ar) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the oscar unshuffled_deduplicated_ar dataset from huggingface \n",
    "#datasetLoad = load_dataset(\"oscar\", \"unshuffled_deduplicated_ar\") #Oscar Arabic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset to \"ar-arrow-datasets/\" Apache Arrow datafolder\n",
    "#datasetLoad.save_to_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load from file\n",
    "full_dataset = load_from_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 9006977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying maps, filters, and splits\n",
    "This process of applying maps and filters is very slow especially for big datasets. \n",
    "\n",
    "It is possible to implement a batched approach similar ot batched tokenization that would speed it up greatly. \n",
    "\n",
    "In the .map() add perameter batched=True. However the function called needs to be compatible with this approach.\n",
    "\n",
    "I will implement this soon.\n",
    "\n",
    "Generally this is not the way to handle big data. However this is fine for the AR-dotless-small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the index of the #th space.\n",
    "def num_word(text, num):\n",
    "    space_count = 0\n",
    "    index = -1\n",
    "    for i in range(num):\n",
    "        index = text.find(' ', index + 1)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "exampleToWordsLen = [] # just to keep track of how many chunks an old  made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(exampleToWordsLen[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunks examples into smaller examples, at @words_per_chuck words each\n",
    "def chunk_examples(examples, words_per_chunk=8):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"clean\"]:\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "        exampleToWordsLen.append(math.ceil(num_words/words_per_chunk)) # just to keep track of how many chunks an old example made\n",
    "        start = 0\n",
    "        while start < num_words:\n",
    "            end = min(start + words_per_chunk, num_words)\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "    \n",
    "    return {\"clean\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2400\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2400\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['clean'],\n",
      "    num_rows: 2400\n",
      "}) \n",
      "\n",
      "15 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2400/2400 [00:00<00:00, 23594.88 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean'],\n",
      "    num_rows: 104246\n",
      "}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map: 100%|██████████| 104246/104246 [00:00<00:00, 261817.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 104246\n",
      "}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 104246/104246 [00:00<00:00, 581243.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 103404\n",
      "}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = full_dataset['train'].select(range(2400)) #Trim to number of examples\n",
    "dataset = dataset.remove_columns([\"id\"])\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['text']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')\n",
    "        \n",
    "dataset = dataset.map(parse_clean_text, batched=True, remove_columns=\"text\") #Apply the function (convert dotted to dotless)\n",
    "set_state_1 = dataset[\"clean\"][0]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 40:\n",
    "        a += 1\n",
    "print(a, '\\n')\n",
    "\n",
    "dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names) # applies teh chuncking\n",
    "set_state_2 = dataset[\"clean\"][0:exampleToWordsLen[0]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.map(parse_dotless_text, batched=True) #Apply the function (convert dotted to dotless)\n",
    "set_state_3 = dataset[\"dotless\"][0:exampleToWordsLen[0]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['clean']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل معنا لتستفيد بكل جديدأهلا وسهلا بك زائرنا الكريم أنت لم تقم بتسجيل الدخول بعد يشرفنا أن تقوم بالدخول أو التسجيل إذا رغبت بالمشاركة في المنتدىنرحب بكل الزائرين ونتمى لكم قضا وقت ممتع معنا يملأه الحب والود والاستفادة المتبادلة بيننا علميا وعمليا يسعدنا تسجيلكم معنا ومشاركتنا وشعارنا دوما نحب الخير لكل الناس مهما اختلفت الألوان والديانات والأجناس لي أربع شقيقات أنا أكثرهن غنى لكن لا أدري لماذا يأتي أقاربي لزيارة أخواتي بكثرةوحينما يأتي موعد زيارتي لا يأتي سوى القليلفهم يزورون أخواتي الأربع كل يومأماأنا أكثر أخواتي عطا لمن يأتيني لا أتهم أخواتي بالتقصير أبدا ولكن الكل يعرف أني أكثرهن عطاكثيرون ينصحون أقاربي بأن يأتوني فلدي خير كثير وأعطي بكرم من يأتيني ومع ذلك يبتعدون عني فلا حياة لمن تنادي اختر منتدىقناة عجباوي التلفزيونية التسويقية البرامج التلفزيونية لقناة عجباوي التسويقية تعرف على عجباوي سيرة ذاتية سابقة أعمال أرشيف تصميمات عجباوي إعلانات الشركات والمحلات أغلفة كتب تنسيقات المتن الداخلي للكتب الدفاتر والمستندات اللافتات الإعلانية السريلات والأختام والأكلشيهات برشور فلاير ستيكر منتجات كتالوجات بوستر كروت شخصية شركات أفراح ومناسبات إمساكيات رمضان عجباوي مول سوق تجاري وخدمات متكاملة بيت التحف والهدايا تحف وأنتيكات حفر ليزر إعلانات وظائف وظائف مطلوبة وظائف معروضة إعلانات مجانية خدمات المكاتب الإدارية خدمات الشركات الصناعية خدمات زراعية خدمات احداث ومناسبات خدمات شخصية خدمات منزلية بنا وعقارات مبيعات تسويق مالية واقتصادية إعلام ونشر خدمات تعليمية رحلات وترفيه رياضة سيارات ومركبات صحة مواد غذائية عجباوي مصمم جرافيك مجموعة برامج الجرافيك الفوتوشوب الكوريل درو اليستريتور الدريم وييفر الفلاش والسويش أدوات المصمم مجموعة برامج الأوفيس تعليم الأكسيس تعليم الوورد تعليم الإكسل أسرار وفنيات عالم الطباعة والنشر تصاميم الأعضا منتدى البرامج والعاب الكمبيوتروالمحمول عجباوي مهندس صيانة الأسئلة والاستفسارات منتدى برامج الكمبيوتر منتدى برامج الحمايه للكمبيوتر منتدى الخلفيات والثيمات والايقونات لجهاز الكمبيوتر منتدى شرح البرامج وتبادل خبرات الكمبيوتر منتدى برامج الملتيميديا والفيديو منتدى العاب الكمبيوتر منتدى الستالايت والقنوات الفضائية والترددات للمحطات منتدى الجوال برامج جوال منتدى رسائل الجوال اغاني وكليبات ونغمات للجوال منتدى التنمية البشرية وتطوير الذات منتدى الأستاذ الدكتور هشام الشريف التنمية البشرية وتطوير مهارات الذات الموارد البشرية والتنمية الإدارية منتدى الحسابات المالية المنتدى التعليمي معلم الأجيال الأستاذ يحيى خضر رسالة إلى رجال التعليم حوار مع طلاب العلم درس خصوصي مرحلة رياض الأطفال المرحلة الإبتدائية الصف الأول الإبتدائي الصف الثاني الإبتدائي الصف الثالث الإبتدائي الصف الرابع الإبتدائي الصف الخامس الإبتدائي الصف السادس الإبتدائي المرحلة الإعدادية الصف الأول الإعدادي الصف الثاني الإعدادي الصف الثالث الإعدادي المرحلة الثانوية الصف الأول الثانوي الصف الثاني الثانوي الصف الثالث الثانوي مرحلة التعليم الفني العالي مرحلة التعليم الجامعي الأعمال الأدبية للأستاذ يحيى خضر عجباوي والأسرة والمجتمع والناس فن الحياة الزوجية السعيدة العلاقة الزوجية الحميمة الأسرة والطفل منتدى المرأة الحامل والطفل منتدى صحة و مستلزمات الطفل منتدى اناقة الرجل منتدى المرأة والزينة منتدى الاشغال والاعمال اليدوية منتدى الخياطة والتطريز منتدى المطبخ والمأكولات المنتدى الطبي أريد حلا منتدى التهنئة والاهداات المنتدى الأدبي عجباوي فنان المقامات الموسيقية العربية منتدى دروس نظرية في الموسيقى الايقاع النغم النوتة الموسيقية الشاعر الكبير أحمد فؤاد نجم منتدى الشاعر الموهوب وحيد الدهشان الشاعرة المبدعة إيمان بكري موضوعات أدبية عامة منتدى الغنا الراقي زمن الفن الجميل إبداعات الأعضا عمالقة الفن والمقطوعات والسماعيات الموسيقيه والمعزوفات النادرة المنتدى العام للقصص والروايات الكتب منتدى البحوث الأكاديمية منتدى المسرح المنتدى الإسلامي القرآن الكريم نور البيان في تعليم القراة بالقرآن الأحاديث النبوية الشريفة ركن شهر رمضان المبارك منتدى الصوتيات والاناشيد الاسلامية منتدى الفيديو الإسلامي أخبار العالم الإسلامي موضوعات إسلامية عامة مشروع علم ينتفع به وقف لله تعالى دروس في العقيدة تربية الأبنا منتدى الموضوعات العامة منتدى علم النفس علم النفس دراسات علم النفس كتب علم النفس ابحاث علم النفس منتدى تفسير الاحلام منتدى علوم الفلك والابراج منتدى السياحة والسفر منتدى عجباوي للحوار الجاد منتدى الحوادث جرائم قتل فضائح أخبار مشاكل وقضايا موضوعات علمية منتدى الشخصيات التاريخية عجباوي الرياضي منتدى الملتيميديا عجباوي تيوب منتدى الصور منتدى الفنون الجميلة وابداعات الاعضا منتدى صيد الكاميرا منتدى الكاريكاتير منتدى الأفلام والمسلسلات والمسرحيات عجباوي قاعد فاضي نكت طرائف ضحك فرفشة منتدى الضحك والفرفشة منتدى الالغاز والمسابقات منتدى الالعاب الارشيف والمواضيع القديمة منتدى مجاني منتدى مجاني للدعم و المساعدة إتصل بنا التبليغ عن محتوى مخالف انشئ مدونة مجانيا عند التصوير يقوم الالة بمسح الاصول مرة واحده وتخزينها بالذاكره ثم تتم عملية التصوير لاي عدد من النسخ منتدى مجاني منتدى مجاني للدعم و المساعدة إتصل بنا التبليغ عن محتوى مخالف الحصول على مدونة \n",
      "\n",
      "\n",
      "['مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة', 'معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل', 'معنا لتستفيد بكل جديدأهلا وسهلا بك زائرنا الكريم', 'أنت لم تقم بتسجيل الدخول بعد يشرفنا أن', 'تقوم بالدخول أو التسجيل إذا رغبت بالمشاركة في', 'المنتدىنرحب بكل الزائرين ونتمى لكم قضا وقت ممتع', 'معنا يملأه الحب والود والاستفادة المتبادلة بيننا علميا', 'وعمليا يسعدنا تسجيلكم معنا ومشاركتنا وشعارنا دوما نحب', 'الخير لكل الناس مهما اختلفت الألوان والديانات والأجناس', 'لي أربع شقيقات أنا أكثرهن غنى لكن لا', 'أدري لماذا يأتي أقاربي لزيارة أخواتي بكثرةوحينما يأتي', 'موعد زيارتي لا يأتي سوى القليلفهم يزورون أخواتي', 'الأربع كل يومأماأنا أكثر أخواتي عطا لمن يأتيني', 'لا أتهم أخواتي بالتقصير أبدا ولكن الكل يعرف', 'أني أكثرهن عطاكثيرون ينصحون أقاربي بأن يأتوني فلدي', 'خير كثير وأعطي بكرم من يأتيني ومع ذلك', 'يبتعدون عني فلا حياة لمن تنادي اختر منتدىقناة', 'عجباوي التلفزيونية التسويقية البرامج التلفزيونية لقناة عجباوي التسويقية', 'تعرف على عجباوي سيرة ذاتية سابقة أعمال أرشيف', 'تصميمات عجباوي إعلانات الشركات والمحلات أغلفة كتب تنسيقات', 'المتن الداخلي للكتب الدفاتر والمستندات اللافتات الإعلانية السريلات', 'والأختام والأكلشيهات برشور فلاير ستيكر منتجات كتالوجات بوستر', 'كروت شخصية شركات أفراح ومناسبات إمساكيات رمضان عجباوي', 'مول سوق تجاري وخدمات متكاملة بيت التحف والهدايا', 'تحف وأنتيكات حفر ليزر إعلانات وظائف وظائف مطلوبة', 'وظائف معروضة إعلانات مجانية خدمات المكاتب الإدارية خدمات', 'الشركات الصناعية خدمات زراعية خدمات احداث ومناسبات خدمات', 'شخصية خدمات منزلية بنا وعقارات مبيعات تسويق مالية', 'واقتصادية إعلام ونشر خدمات تعليمية رحلات وترفيه رياضة', 'سيارات ومركبات صحة مواد غذائية عجباوي مصمم جرافيك', 'مجموعة برامج الجرافيك الفوتوشوب الكوريل درو اليستريتور الدريم', 'وييفر الفلاش والسويش أدوات المصمم مجموعة برامج الأوفيس', 'تعليم الأكسيس تعليم الوورد تعليم الإكسل أسرار وفنيات', 'عالم الطباعة والنشر تصاميم الأعضا منتدى البرامج والعاب', 'الكمبيوتروالمحمول عجباوي مهندس صيانة الأسئلة والاستفسارات منتدى برامج', 'الكمبيوتر منتدى برامج الحمايه للكمبيوتر منتدى الخلفيات والثيمات', 'والايقونات لجهاز الكمبيوتر منتدى شرح البرامج وتبادل خبرات', 'الكمبيوتر منتدى برامج الملتيميديا والفيديو منتدى العاب الكمبيوتر', 'منتدى الستالايت والقنوات الفضائية والترددات للمحطات منتدى الجوال', 'برامج جوال منتدى رسائل الجوال اغاني وكليبات ونغمات', 'للجوال منتدى التنمية البشرية وتطوير الذات منتدى الأستاذ', 'الدكتور هشام الشريف التنمية البشرية وتطوير مهارات الذات', 'الموارد البشرية والتنمية الإدارية منتدى الحسابات المالية المنتدى', 'التعليمي معلم الأجيال الأستاذ يحيى خضر رسالة إلى', 'رجال التعليم حوار مع طلاب العلم درس خصوصي', 'مرحلة رياض الأطفال المرحلة الإبتدائية الصف الأول الإبتدائي', 'الصف الثاني الإبتدائي الصف الثالث الإبتدائي الصف الرابع', 'الإبتدائي الصف الخامس الإبتدائي الصف السادس الإبتدائي المرحلة', 'الإعدادية الصف الأول الإعدادي الصف الثاني الإعدادي الصف', 'الثالث الإعدادي المرحلة الثانوية الصف الأول الثانوي الصف', 'الثاني الثانوي الصف الثالث الثانوي مرحلة التعليم الفني', 'العالي مرحلة التعليم الجامعي الأعمال الأدبية للأستاذ يحيى', 'خضر عجباوي والأسرة والمجتمع والناس فن الحياة الزوجية', 'السعيدة العلاقة الزوجية الحميمة الأسرة والطفل منتدى المرأة', 'الحامل والطفل منتدى صحة و مستلزمات الطفل منتدى', 'اناقة الرجل منتدى المرأة والزينة منتدى الاشغال والاعمال', 'اليدوية منتدى الخياطة والتطريز منتدى المطبخ والمأكولات المنتدى', 'الطبي أريد حلا منتدى التهنئة والاهداات المنتدى الأدبي', 'عجباوي فنان المقامات الموسيقية العربية منتدى دروس نظرية', 'في الموسيقى الايقاع النغم النوتة الموسيقية الشاعر الكبير', 'أحمد فؤاد نجم منتدى الشاعر الموهوب وحيد الدهشان', 'الشاعرة المبدعة إيمان بكري موضوعات أدبية عامة منتدى', 'الغنا الراقي زمن الفن الجميل إبداعات الأعضا عمالقة', 'الفن والمقطوعات والسماعيات الموسيقيه والمعزوفات النادرة المنتدى العام', 'للقصص والروايات الكتب منتدى البحوث الأكاديمية منتدى المسرح', 'المنتدى الإسلامي القرآن الكريم نور البيان في تعليم', 'القراة بالقرآن الأحاديث النبوية الشريفة ركن شهر رمضان', 'المبارك منتدى الصوتيات والاناشيد الاسلامية منتدى الفيديو الإسلامي', 'أخبار العالم الإسلامي موضوعات إسلامية عامة مشروع علم', 'ينتفع به وقف لله تعالى دروس في العقيدة', 'تربية الأبنا منتدى الموضوعات العامة منتدى علم النفس', 'علم النفس دراسات علم النفس كتب علم النفس', 'ابحاث علم النفس منتدى تفسير الاحلام منتدى علوم', 'الفلك والابراج منتدى السياحة والسفر منتدى عجباوي للحوار', 'الجاد منتدى الحوادث جرائم قتل فضائح أخبار مشاكل', 'وقضايا موضوعات علمية منتدى الشخصيات التاريخية عجباوي الرياضي', 'منتدى الملتيميديا عجباوي تيوب منتدى الصور منتدى الفنون', 'الجميلة وابداعات الاعضا منتدى صيد الكاميرا منتدى الكاريكاتير', 'منتدى الأفلام والمسلسلات والمسرحيات عجباوي قاعد فاضي نكت', 'طرائف ضحك فرفشة منتدى الضحك والفرفشة منتدى الالغاز', 'والمسابقات منتدى الالعاب الارشيف والمواضيع القديمة منتدى مجاني', 'منتدى مجاني للدعم و المساعدة إتصل بنا التبليغ', 'عن محتوى مخالف انشئ مدونة مجانيا عند التصوير', 'يقوم الالة بمسح الاصول مرة واحده وتخزينها بالذاكره', 'ثم تتم عملية التصوير لاي عدد من النسخ', 'منتدى مجاني منتدى مجاني للدعم و المساعدة إتصل', 'بنا التبليغ عن محتوى مخالف الحصول على مدونة'] \n",
      "\n",
      "\n",
      "['مرحبا بك عرىر الراىر نبمنى لك اوفابا سعىده', 'معنا وان نرداد سرفا بحدمبك ولا بنسى البسحىل', 'معنا لبسبفىد بكل حدىداهلا وسهلا بك راىرنا الكرىم', 'انب لم بفم ببسحىل الدحول بعد ىسرفنا ان', 'بفوم بالدحول او البسحىل ادا رعبب بالمساركه فى', 'المنبدىنرحب بكل الراىرىن ونبمى لكم فصا وفب ممبع', 'معنا ىملاه الحب والود والاسبفاده المببادله بىننا علمىا', 'وعملىا ىسعدنا بسحىلكم معنا ومساركبنا وسعارنا دوما نحب', 'الحىر لكل الناس مهما احبلفب الالوان والدىاناب والاحناس', 'لى اربع سفىفاب انا اكبرهن عنى لكن لا', 'ادرى لمادا ىابى افاربى لرىاره احوابى بكبرهوحىنما ىابى', 'موعد رىاربى لا ىابى سوى الفلىلفهم ىرورون احوابى', 'الاربع كل ىوماماانا اكبر احوابى عطا لمن ىابىنى', 'لا ابهم احوابى بالبفصىر ابدا ولكن الكل ىعرف', 'انى اكبرهن عطاكبىرون ىنصحون افاربى بان ىابونى فلدى', 'حىر كبىر واعطى بكرم من ىابىنى ومع دلك', 'ىببعدون عنى فلا حىاه لمن بنادى احبر منبدىفناه', 'عحباوى البلفرىونىه البسوىفىه البرامح البلفرىونىه لفناه عحباوى البسوىفىه', 'بعرف على عحباوى سىره دابىه سابفه اعمال ارسىف', 'بصمىماب عحباوى اعلاناب السركاب والمحلاب اعلفه كبب بنسىفاب', 'المبن الداحلى للكبب الدفابر والمسبنداب اللافباب الاعلانىه السرىلاب', 'والاحبام والاكلسىهاب برسور فلاىر سبىكر منبحاب كبالوحاب بوسبر', 'كروب سحصىه سركاب افراح ومناسباب امساكىاب رمصان عحباوى', 'مول سوف بحارى وحدماب مبكامله بىب البحف والهداىا', 'بحف وانبىكاب حفر لىرر اعلاناب وطاىف وطاىف مطلوبه', 'وطاىف معروصه اعلاناب محانىه حدماب المكابب الادارىه حدماب', 'السركاب الصناعىه حدماب رراعىه حدماب احداب ومناسباب حدماب', 'سحصىه حدماب منرلىه بنا وعفاراب مبىعاب بسوىف مالىه', 'وافبصادىه اعلام ونسر حدماب بعلىمىه رحلاب وبرفىه رىاصه', 'سىاراب ومركباب صحه مواد عداىىه عحباوى مصمم حرافىك', 'محموعه برامح الحرافىك الفوبوسوب الكورىل درو الىسبرىبور الدرىم', 'وىىفر الفلاس والسوىس ادواب المصمم محموعه برامح الاوفىس', 'بعلىم الاكسىس بعلىم الوورد بعلىم الاكسل اسرار وفنىاب', 'عالم الطباعه والنسر بصامىم الاعصا منبدى البرامح والعاب', 'الكمبىوبروالمحمول عحباوى مهندس صىانه الاسىله والاسبفساراب منبدى برامح', 'الكمبىوبر منبدى برامح الحماىه للكمبىوبر منبدى الحلفىاب والبىماب', 'والاىفوناب لحهار الكمبىوبر منبدى سرح البرامح وببادل حبراب', 'الكمبىوبر منبدى برامح الملبىمىدىا والفىدىو منبدى العاب الكمبىوبر', 'منبدى السبالاىب والفنواب الفصاىىه والبردداب للمحطاب منبدى الحوال', 'برامح حوال منبدى رساىل الحوال اعانى وكلىباب ونعماب', 'للحوال منبدى البنمىه البسرىه وبطوىر الداب منبدى الاسباد', 'الدكبور هسام السرىف البنمىه البسرىه وبطوىر مهاراب الداب', 'الموارد البسرىه والبنمىه الادارىه منبدى الحساباب المالىه المنبدى', 'البعلىمى معلم الاحىال الاسباد ىحىى حصر رساله الى', 'رحال البعلىم حوار مع طلاب العلم درس حصوصى', 'مرحله رىاص الاطفال المرحله الاببداىىه الصف الاول الاببداىى', 'الصف البانى الاببداىى الصف البالب الاببداىى الصف الرابع', 'الاببداىى الصف الحامس الاببداىى الصف السادس الاببداىى المرحله', 'الاعدادىه الصف الاول الاعدادى الصف البانى الاعدادى الصف', 'البالب الاعدادى المرحله البانوىه الصف الاول البانوى الصف', 'البانى البانوى الصف البالب البانوى مرحله البعلىم الفنى', 'العالى مرحله البعلىم الحامعى الاعمال الادبىه للاسباد ىحىى', 'حصر عحباوى والاسره والمحبمع والناس فن الحىاه الروحىه', 'السعىده العلافه الروحىه الحمىمه الاسره والطفل منبدى المراه', 'الحامل والطفل منبدى صحه و مسبلرماب الطفل منبدى', 'انافه الرحل منبدى المراه والرىنه منبدى الاسعال والاعمال', 'الىدوىه منبدى الحىاطه والبطرىر منبدى المطبح والماكولاب المنبدى', 'الطبى ارىد حلا منبدى البهنىه والاهدااب المنبدى الادبى', 'عحباوى فنان المفاماب الموسىفىه العربىه منبدى دروس نطرىه', 'فى الموسىفى الاىفاع النعم النوبه الموسىفىه الساعر الكبىر', 'احمد فواد نحم منبدى الساعر الموهوب وحىد الدهسان', 'الساعره المبدعه اىمان بكرى موصوعاب ادبىه عامه منبدى', 'العنا الرافى رمن الفن الحمىل ابداعاب الاعصا عمالفه', 'الفن والمفطوعاب والسماعىاب الموسىفىه والمعروفاب النادره المنبدى العام', 'للفصص والرواىاب الكبب منبدى البحوب الاكادىمىه منبدى المسرح', 'المنبدى الاسلامى الفران الكرىم نور البىان فى بعلىم', 'الفراه بالفران الاحادىب النبوىه السرىفه ركن سهر رمصان', 'المبارك منبدى الصوبىاب والاناسىد الاسلامىه منبدى الفىدىو الاسلامى', 'احبار العالم الاسلامى موصوعاب اسلامىه عامه مسروع علم', 'ىنبفع به وفف لله بعالى دروس فى العفىده', 'بربىه الابنا منبدى الموصوعاب العامه منبدى علم النفس', 'علم النفس دراساب علم النفس كبب علم النفس', 'ابحاب علم النفس منبدى بفسىر الاحلام منبدى علوم', 'الفلك والابراح منبدى السىاحه والسفر منبدى عحباوى للحوار', 'الحاد منبدى الحوادب حراىم فبل فصاىح احبار مساكل', 'وفصاىا موصوعاب علمىه منبدى السحصىاب البارىحىه عحباوى الرىاصى', 'منبدى الملبىمىدىا عحباوى بىوب منبدى الصور منبدى الفنون', 'الحمىله وابداعاب الاعصا منبدى صىد الكامىرا منبدى الكارىكابىر', 'منبدى الافلام والمسلسلاب والمسرحىاب عحباوى فاعد فاصى نكب', 'طراىف صحك فرفسه منبدى الصحك والفرفسه منبدى الالعار', 'والمسابفاب منبدى الالعاب الارسىف والمواصىع الفدىمه منبدى محانى', 'منبدى محانى للدعم و المساعده ابصل بنا الببلىع', 'عن محبوى محالف انسى مدونه محانىا عند البصوىر', 'ىفوم الاله بمسح الاصول مره واحده وبحرىنها بالداكره', 'بم ببم عملىه البصوىر لاى عدد من النسح', 'منبدى محانى منبدى محانى للدعم و المساعده ابصل', 'بنا الببلىع عن محبوى محالف الحصول على مدونه']\n"
     ]
    }
   ],
   "source": [
    "print(set_state_1,\"\\n\\n\")\n",
    "print(set_state_2,\"\\n\\n\")\n",
    "print(set_state_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 20:\n",
    "        a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 82723\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 20681\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean = dataset.train_test_split(train_size= 0.8) #create 80/20 split into train and test datasets\n",
    "dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and upload to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 82723/82723 [00:00<00:00, 525897.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 20681/20681 [00:00<00:00, 514461.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#save the cleaned dataset to \"clean-arrow-datasets/\" Apache Arrow datafolder\n",
    "dataset_clean.save_to_disk(\"clean-arrow-datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 83/83 [00:00<00:00, 95.99ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:07<00:00,  7.15s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 2/2 [00:00<00:00,  6.89it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 21/21 [00:00<00:00, 73.00ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "Deleting unused files from dataset repository: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Downloading metadata: 100%|██████████| 698/698 [00:00<00:00, 3.76MB/s]\n"
     ]
    }
   ],
   "source": [
    "#required token id. Run 'huggingface-cli login'\n",
    "dataset_clean.push_to_hub(\"dot-ammar/AR-dotless-small\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
