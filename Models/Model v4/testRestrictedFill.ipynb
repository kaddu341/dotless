{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from processingDatasetDotless import *\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tashaphyne.stemming import ArabicLightStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"/Users/ammar/Developer/git-repos/dotless/Models/Model v3/AR-multi-dotted-Small-arrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = []\n",
    "for i in range(10000):\n",
    "    length = len(dataset['train'][i][\"Options\"])\n",
    "    if length > 10 and length <= 20:\n",
    "        indicies.append(i)\n",
    "\n",
    "print(indicies)\n",
    "print(len(indicies))\n",
    "dataset['train'][indicies[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ammar/Developer/git-repos/dotless/Models/Model v4/roots.txt\", 'r') as file:\n",
    "    wordlist = set(file.read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"#\" in list(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-mix\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"fill-mask\", model=\"UBC-NLP/ARBERTv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"اللغة [MASK] هي لغة العرب\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArListem = ArabicLightStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = u'مؤمنون'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenID = tokenizer.convert_ids_to_tokens(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = ArListem.light_stem(word)\n",
    "root = ArListem.get_root()\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root in wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_word_probabilities(input_text, candidate_words):\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    masked_word_index = tokenized_text.index('[MASK]')\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze_(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "\n",
    "    predictions = output.logits[0, masked_word_index].softmax(dim=0)\n",
    "\n",
    "    # Tokenize and convert all candidate words to ids at once\n",
    "    \n",
    "    tokenized_candidate_words = [tokenizer.tokenize(word)for word in candidate_words]\n",
    "    #print(tokenized_candidate_words)\n",
    "    \n",
    "    tokenized_candidate_words = [word for word in tokenized_candidate_words if len(word) == 1]\n",
    "    #print(tokenized_candidate_words)\n",
    "    \n",
    "    flattened_tokenized_candidate_words = [word for sublist in tokenized_candidate_words for word in sublist]\n",
    "    #print(flattened_tokenized_candidate_words)\n",
    "    \n",
    "    candidate_word_ids = [tokenizer.convert_tokens_to_ids(TCW) for TCW in flattened_tokenized_candidate_words]\n",
    "    #print(candidate_word_ids)\n",
    "    \n",
    "    # Use a list comprehension to populate the dictionary\n",
    "    candidate_probabilities = {word: predictions[word_id].item() for word, word_id in zip(flattened_tokenized_candidate_words, candidate_word_ids)}\n",
    "    #print(candidate_probabilities)\n",
    "\n",
    "    return candidate_probabilities\n",
    "\n",
    "\n",
    "def generate_probabilties_end(example):\n",
    "    input_text = example[\"Masked\"]\n",
    "    candidates = example[\"Options\"]\n",
    "\n",
    "            \n",
    "    word_probabilities = get_candidate_word_probabilities(input_text, candidates)\n",
    "\n",
    "    sorted_words = sorted(word_probabilities, key=word_probabilities.get, reverse=True)\n",
    "    if len(sorted_words ) > 0:\n",
    "        most_probable_word = sorted_words[0]\n",
    "    else:\n",
    "        most_probable_word = None\n",
    "        #print(example[\"Target\"])\n",
    "        \n",
    "    return word_probabilities, sorted_words, most_probable_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eg = 0\n",
    "#example = dataset['train'][indicies[num_eg]]\n",
    "example = mask_random_word(\"قرأ\")\n",
    "word_probabilities, sorted_words, most_probable_word = generate_probabilties_end(example)\n",
    "print(\"Length of words:\", len(sorted_words))\n",
    "for word in sorted_words:\n",
    "    probability = word_probabilities[word]\n",
    "    print(f\"Word: '{word}', Probability: {probability:.10f}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "print(\"Most probable word:\", most_probable_word)\n",
    "print(\"Target word:\", example[\"Target\"])\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "found = False\n",
    "for i in range(len(sorted_words)):\n",
    "    if sorted_words[i] == example[\"Target\"]:\n",
    "        print(\"Sucess at probability level:\", i)\n",
    "        found = True\n",
    "        break\n",
    "if not found: print(\"Not found.\")\n",
    "sucess_level = i\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = np.zeros(1000000, dtype =int)\n",
    "#longWords = []\n",
    "#veryLongWords = []\n",
    "#veryVeryLongWords = []\n",
    "start = 0\n",
    "end = 101\n",
    "iterationCount = end - start - 1\n",
    "for i in range(start, end):\n",
    "    num_eg = i\n",
    "    #example1 = mask_random_word(\"التحدي الصعب يكمن في فهم التفاصيل الدقيقة لهذه الفكرة المعقدة\")\n",
    "    #example1 = dataset['train'][indicies[num_eg]]\n",
    "    example1 = dataset['train'][num_eg]\n",
    "\n",
    "    word_probabilities, sorted_words, most_probable_word = generate_probabilties_end(example1)\n",
    "    for word in sorted_words:\n",
    "        probability = word_probabilities[word]\n",
    "        \n",
    "    #if len(sorted_words) >= 500 and len(sorted_words) < 1000: longWords.append(example1[\"Target\"])\n",
    "    #elif len(sorted_words) >= 1000 and len(sorted_words) < 100000: veryLongWords.append(example1[\"Target\"])\n",
    "    #elif len(sorted_words) >= 100000 and len(sorted_words) <1000000: veryVeryLongWords.append(example1[\"Target\"])\n",
    "    #elif len(sorted_words) >= 1000000: veryVeryLongWords.append(example1[\"Target\"])\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(len(sorted_words)):\n",
    "        if sorted_words[j] == example1[\"Target\"]:\n",
    "            count[j] += 1\n",
    "            break\n",
    "        \n",
    "\n",
    "for k in range(len(count)):\n",
    "    numberCount = count[k]\n",
    "    if numberCount != 0:\n",
    "        print(numberCount, \"placments at level\", k, \"  \\t\", (numberCount/iterationCount) * 100, \"%\")\n",
    "        \n",
    "print()\n",
    "print(\"Number of examples tested:\", iterationCount)\n",
    "print(\"Number of examples that yielded a result:\", np.sum(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(text, index=None):\n",
    "\n",
    "    words = text.split()\n",
    "    if index == None: index = random.randint(0, len(words) - 1)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == index:\n",
    "            word_to_replace = words[i]\n",
    "            variations = allVariation(parse_dotless_text(word_to_replace))\n",
    "            words[i] = \"[MASK]\"\n",
    "            modified_string = ' '.join(words)\n",
    "            masked = (modified_string)\n",
    "            options = (variations)\n",
    "            targets = (word_to_replace)\n",
    "    \n",
    "    return {\"Masked\": masked, \"Options\": options, \"Target\": targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_word(\"قنام\", 0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dotless_text(\"قرأ أحمد كتابه ثم نام\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "def function123(input_text, cursor_position):\n",
    "    \n",
    "    # Get the text up to the cursor position\n",
    "    text_up_to_cursor = input_text[:cursor_position] \n",
    "\n",
    "    # Split the text into words\n",
    "    words = text_up_to_cursor.split()\n",
    "\n",
    "    # The word number that the user is currently editing is the number of words\n",
    "    word_number = len(words) - 1\n",
    "    current_word = words[word_number]\n",
    "    \n",
    "    print(\"current_word:\", current_word)\n",
    "    print(\"Cursor position:\", cursor_position)\n",
    "    print(\"Word Number:\", word_number)\n",
    "    print(\"input_text:\", input_text)\n",
    "    \n",
    "    Fexample =  mask_word(input_text, word_number)\n",
    "    Fword_probabilities, Fsorted_words, Fmost_probable_word = generate_probabilties_end(Fexample)\n",
    "    Msentence = Fexample[\"Masked\"]\n",
    "    \n",
    "    if Fmost_probable_word == None:\n",
    "        fill = words[word_number]\n",
    "    else:\n",
    "        fill = Fmost_probable_word\n",
    "        \n",
    "    sentence = Msentence.replace(\"[MASK]\", fill)\n",
    "    \n",
    "\n",
    "    print(\"example:\",Fexample)\n",
    "    print(\"most_probable_word:\", Fmost_probable_word)\n",
    "    print(\"fill:\", fill)\n",
    "    print(\"sentence:\", sentence)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def update_input(event):\n",
    "    \n",
    "    # Ignore non-character key presses\n",
    "    if event.char == \"\" or event.keysym == \"space\":\n",
    "        return\n",
    "\n",
    "    # Get the current text input from the Entry widget\n",
    "    input_text = entry.get()\n",
    "\n",
    "    # Call the function123() with the current input and get the processed string\n",
    "    cursor_position = entry.index(tk.INSERT)\n",
    "\n",
    "    processed_text = function123(input_text, cursor_position)\n",
    "\n",
    "    # Replace the text in the Entry widget with the processed string\n",
    "    entry.delete(0, tk.END)\n",
    "    entry.insert(0, processed_text)\n",
    "\n",
    "# Create the main window\n",
    "main = tk.Tk()\n",
    "main.geometry(\"800x600\")\n",
    "# Create the Entry widget\n",
    "entry = tk.Entry(main)\n",
    "\n",
    "# Bind the <Key> event to the update_input function\n",
    "\n",
    "entry.bind(\"<KeyRelease>\", update_input)\n",
    "\n",
    "# Pack the Entry widget to display it in the window\n",
    "entry.pack()\n",
    "\n",
    "# Start the main event loop\n",
    "main.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
