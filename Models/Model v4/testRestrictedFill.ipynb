{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from processingDatasetDotless import *\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"/Users/ammar/Developer/git-repos/dotless/Models/Model v3/AR-multi-dotted-Small-arrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Masked', 'Options', 'Target'],\n",
       "        num_rows: 500000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Masked': 'يوسف الذين دفنوا السلاح و المهمات العسكرية في [MASK] من جب بالعاصمة',\n",
       " 'Options': ['آكثر',\n",
       "  'آكثز',\n",
       "  'آكتر',\n",
       "  'آكتز',\n",
       "  'آكبر',\n",
       "  'آكبز',\n",
       "  'إكثر',\n",
       "  'إكثز',\n",
       "  'إكتر',\n",
       "  'إكتز',\n",
       "  'إكبر',\n",
       "  'إكبز',\n",
       "  'أكثر',\n",
       "  'أكثز',\n",
       "  'أكتر',\n",
       "  'أكتز',\n",
       "  'أكبر',\n",
       "  'أكبز',\n",
       "  'اكثر',\n",
       "  'اكثز',\n",
       "  'اكتر',\n",
       "  'اكتز',\n",
       "  'اكبر',\n",
       "  'اكبز'],\n",
       " 'Target': 'أكثر'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 28, 41, 57, 66, 92, 102, 118, 119, 122, 123, 126, 130, 133, 134, 152, 164, 174, 178, 179, 188, 189, 210, 216, 223, 237, 258, 263, 269, 270, 272, 278, 306, 320, 323, 330, 340, 348, 352, 356, 363, 396, 404, 428, 438, 443, 444, 451, 466, 473, 490, 497, 498, 502, 505, 510, 514, 520, 523, 529, 531, 537, 538, 540, 541, 554, 566, 569, 575, 587, 588, 598, 601, 607, 609, 613, 625, 626, 632, 640, 662, 664, 674, 677, 679, 688, 690, 692, 702, 709, 722, 723, 727, 728, 745, 755, 761, 767, 774, 782, 796, 798, 799, 804, 805, 818, 831, 832, 836, 846, 851, 853, 868, 878, 894, 895, 902, 923, 931, 944, 956, 965, 968, 993, 1001, 1012, 1015, 1033, 1043, 1049, 1057, 1060, 1062, 1070, 1082, 1095, 1103, 1109, 1115, 1117, 1118, 1125, 1126, 1142, 1151, 1155, 1159, 1165, 1169, 1177, 1196, 1197, 1200, 1204, 1205, 1210, 1212, 1216, 1234, 1241, 1245, 1251, 1253, 1260, 1275, 1286, 1290, 1302, 1310, 1316, 1328, 1330, 1331, 1332, 1337, 1338, 1341, 1343, 1351, 1362, 1364, 1368, 1377, 1387, 1394, 1398, 1403, 1425, 1427, 1428, 1431, 1432, 1442, 1454, 1456, 1477, 1489, 1494, 1496, 1508, 1514, 1516, 1519, 1521, 1522, 1523, 1524, 1526, 1538, 1543, 1558, 1563, 1564, 1573, 1591, 1592, 1596, 1606, 1612, 1614, 1642, 1643, 1649, 1656, 1658, 1661, 1681, 1687, 1689, 1699, 1702, 1704, 1714, 1718, 1726, 1732, 1737, 1745, 1757, 1759, 1762, 1763, 1764, 1786, 1789, 1810, 1813, 1818, 1828, 1831, 1839, 1848, 1851, 1871, 1878, 1884, 1916, 1919, 1923, 1924, 1943, 1949, 1952, 1954, 1993, 1994, 1997, 2004, 2006, 2017, 2030, 2038, 2039, 2040, 2047, 2051, 2071, 2073, 2079, 2106, 2108, 2121, 2133, 2142, 2173, 2174, 2180, 2181, 2199, 2205, 2206, 2208, 2213, 2220, 2223, 2225, 2228, 2240, 2248, 2266, 2269, 2270, 2274, 2283, 2289, 2307, 2325, 2334, 2349, 2366, 2375, 2376, 2381, 2384, 2388, 2395, 2398, 2404, 2406, 2411, 2420, 2424, 2425, 2432, 2433, 2439, 2442, 2452, 2468, 2469, 2483, 2505, 2529, 2531, 2547, 2566, 2578, 2580, 2590, 2591, 2593, 2597, 2598, 2610, 2616, 2623, 2626, 2636, 2637, 2639, 2648, 2673, 2676, 2701, 2704, 2707, 2709, 2717, 2719, 2746, 2760, 2762, 2764, 2769, 2780, 2787, 2802, 2803, 2819, 2820, 2823, 2825, 2830, 2847, 2859, 2860, 2861, 2863, 2883, 2889, 2900, 2905, 2925, 2930, 2939, 2946, 2956, 2964, 2972, 2979, 2980, 2991, 2996, 3014, 3020, 3029, 3034, 3045, 3048, 3056, 3067, 3068, 3072, 3074, 3075, 3086, 3095, 3101, 3117, 3149, 3158, 3182, 3186, 3200, 3201, 3208, 3217, 3236, 3238, 3256, 3266, 3267, 3291, 3294, 3302, 3305, 3312, 3318, 3320, 3322, 3330, 3335, 3343, 3357, 3359, 3366, 3367, 3395, 3398, 3409, 3414, 3422, 3436, 3440, 3450, 3465, 3468, 3479, 3485, 3491, 3497, 3504, 3508, 3513, 3522, 3527, 3539, 3552, 3553, 3554, 3556, 3560, 3561, 3583, 3584, 3598, 3609, 3610, 3617, 3626, 3656, 3668, 3671, 3680, 3695, 3700, 3701, 3712, 3713, 3718, 3742, 3756, 3759, 3769, 3775, 3805, 3807, 3809, 3822, 3834, 3841, 3849, 3854, 3857, 3858, 3860, 3870, 3881, 3889, 3914, 3920, 3921, 3933, 3936, 3945, 3946, 3948, 3950, 3951, 3953, 3956, 3959, 3980, 3992, 4002, 4016, 4029, 4032, 4037, 4052, 4053, 4055, 4078, 4099, 4105, 4119, 4122, 4129, 4137, 4138, 4139, 4142, 4154, 4157, 4163, 4171, 4176, 4198, 4209, 4215, 4217, 4232, 4235, 4247, 4251, 4263, 4267, 4270, 4284, 4287, 4290, 4295, 4318, 4328, 4330, 4332, 4341, 4342, 4363, 4364, 4407, 4417, 4418, 4422, 4431, 4440, 4442, 4460, 4469, 4472, 4475, 4476, 4478, 4479, 4480, 4492, 4495, 4514, 4516, 4519, 4522, 4530, 4542, 4553, 4556, 4559, 4566, 4571, 4572, 4573, 4577, 4578, 4579, 4580, 4583, 4593, 4595, 4608, 4612, 4617, 4627, 4642, 4644, 4650, 4653, 4667, 4680, 4704, 4715, 4720, 4723, 4741, 4744, 4745, 4768, 4797, 4815, 4827, 4841, 4856, 4860, 4877, 4887, 4898, 4902, 4912, 4918, 4930, 4960, 4975, 4979, 4982, 4986, 4990, 4991, 5001, 5005, 5007, 5017, 5030, 5035, 5044, 5055, 5072, 5078, 5080, 5092, 5097, 5099, 5103, 5106, 5112, 5129, 5140, 5142, 5144, 5147, 5156, 5161, 5177, 5180, 5184, 5198, 5204, 5212, 5241, 5250, 5255, 5259, 5273, 5278, 5287, 5299, 5305, 5306, 5307, 5309, 5315, 5332, 5342, 5346, 5351, 5363, 5375, 5376, 5388, 5410, 5421, 5423, 5427, 5434, 5449, 5462, 5477, 5479, 5480, 5483, 5485, 5504, 5508, 5510, 5514, 5524, 5525, 5528, 5543, 5556, 5564, 5571, 5572, 5573, 5574, 5576, 5584, 5599, 5602, 5604, 5632, 5633, 5643, 5650, 5652, 5653, 5659, 5669, 5682, 5689, 5692, 5700, 5701, 5715, 5719, 5720, 5750, 5766, 5774, 5783, 5790, 5801, 5810, 5812, 5834, 5840, 5843, 5845, 5848, 5874, 5879, 5881, 5882, 5886, 5905, 5906, 5908, 5909, 5913, 5917, 5926, 5927, 5937, 5947, 5956, 5957, 5958, 5959, 5963, 5971, 5974, 5978, 5980, 5981, 5984, 5992, 6006, 6009, 6010, 6012, 6019, 6063, 6069, 6073, 6091, 6092, 6094, 6102, 6105, 6116, 6121, 6126, 6133, 6146, 6157, 6159, 6163, 6173, 6176, 6185, 6195, 6197, 6209, 6213, 6228, 6231, 6233, 6243, 6244, 6270, 6271, 6272, 6284, 6291, 6293, 6306, 6311, 6317, 6329, 6336, 6339, 6343, 6344, 6347, 6357, 6365, 6382, 6392, 6393, 6396, 6408, 6409, 6413, 6423, 6426, 6438, 6444, 6450, 6453, 6464, 6475, 6499, 6505, 6509, 6516, 6538, 6549, 6551, 6556, 6564, 6569, 6571, 6591, 6597, 6612, 6640, 6648, 6652, 6654, 6662, 6665, 6686, 6692, 6713, 6717, 6743, 6745, 6769, 6775, 6777, 6783, 6787, 6789, 6790, 6795, 6799, 6806, 6815, 6822, 6834, 6835, 6836, 6841, 6842, 6846, 6847, 6850, 6869, 6870, 6875, 6893, 6902, 6914, 6932, 6936, 6939, 6940, 6949, 6954, 6982, 6986, 6987, 7017, 7025, 7037, 7044, 7049, 7052, 7057, 7065, 7071, 7076, 7086, 7102, 7107, 7110, 7114, 7123, 7135, 7142, 7145, 7148, 7154, 7156, 7173, 7186, 7194, 7195, 7215, 7218, 7230, 7239, 7248, 7269, 7270, 7304, 7306, 7314, 7329, 7335, 7339, 7350, 7353, 7368, 7379, 7397, 7406, 7407, 7409, 7416, 7425, 7428, 7433, 7436, 7456, 7476, 7477, 7485, 7498, 7520, 7525, 7527, 7539, 7560, 7563, 7570, 7572, 7589, 7592, 7609, 7637, 7641, 7650, 7656, 7662, 7668, 7691, 7693, 7702, 7709, 7730, 7733, 7736, 7749, 7764, 7773, 7786, 7788, 7789, 7792, 7796, 7798, 7800, 7804, 7812, 7816, 7817, 7818, 7832, 7839, 7840, 7843, 7847, 7862, 7865, 7905, 7906, 7910, 7911, 7946, 7954, 7963, 7974, 7981, 7983, 7989, 7996, 8001, 8004, 8005, 8018, 8024, 8030, 8032, 8033, 8035, 8036, 8040, 8041, 8054, 8055, 8066, 8068, 8069, 8071, 8074, 8083, 8087, 8089, 8093, 8094, 8107, 8110, 8113, 8133, 8136, 8147, 8155, 8157, 8160, 8162, 8179, 8209, 8213, 8214, 8224, 8230, 8237, 8247, 8262, 8264, 8269, 8272, 8273, 8281, 8283, 8300, 8302, 8322, 8342, 8343, 8346, 8349, 8364, 8366, 8370, 8376, 8379, 8385, 8389, 8391, 8395, 8438, 8444, 8448, 8454, 8460, 8461, 8462, 8468, 8472, 8485, 8509, 8518, 8545, 8555, 8556, 8560, 8571, 8579, 8587, 8590, 8595, 8596, 8599, 8601, 8605, 8619, 8621, 8627, 8628, 8633, 8636, 8644, 8646, 8657, 8666, 8679, 8687, 8690, 8694, 8696, 8713, 8717, 8724, 8735, 8740, 8744, 8754, 8762, 8767, 8768, 8770, 8774, 8779, 8781, 8822, 8830, 8831, 8844, 8847, 8854, 8861, 8862, 8864, 8868, 8872, 8889, 8929, 8933, 8937, 8938, 8946, 8954, 8956, 8961, 8966, 8990, 8991, 8993, 8996, 9001, 9002, 9006, 9017, 9024, 9030, 9033, 9034, 9038, 9043, 9046, 9048, 9051, 9070, 9074, 9077, 9078, 9092, 9102, 9105, 9115, 9116, 9119, 9127, 9137, 9138, 9140, 9157, 9160, 9165, 9173, 9182, 9183, 9194, 9214, 9217, 9219, 9228, 9237, 9260, 9267, 9269, 9273, 9275, 9276, 9287, 9309, 9313, 9314, 9323, 9325, 9328, 9348, 9361, 9362, 9374, 9379, 9395, 9397, 9399, 9414, 9423, 9425, 9440, 9441, 9446, 9469, 9473, 9474, 9476, 9488, 9491, 9501, 9510, 9520, 9529, 9544, 9548, 9559, 9563, 9565, 9572, 9578, 9582, 9586, 9590, 9592, 9594, 9596, 9604, 9608, 9610, 9615, 9637, 9639, 9644, 9654, 9656, 9661, 9676, 9683, 9685, 9688, 9695, 9708, 9709, 9710, 9732, 9734, 9746, 9749, 9760, 9772, 9773, 9785, 9808, 9809, 9818, 9820, 9824, 9839, 9841, 9842, 9843, 9848, 9855, 9867, 9871, 9877, 9886, 9891, 9895, 9902, 9903, 9905, 9907, 9917, 9927, 9949, 9950, 9959, 9960, 9968, 9991, 9993, 9996, 9997]\n",
      "1306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Masked': 'السؤال كيف توفق بين دخلك والمنصرفات [MASK] الله كريم فخلص إلى أن',\n",
       " 'Options': ['ؤةي',\n",
       "  'ؤةى',\n",
       "  'ؤةئ',\n",
       "  'ؤهي',\n",
       "  'ؤهى',\n",
       "  'ؤهئ',\n",
       "  'وةي',\n",
       "  'وةى',\n",
       "  'وةئ',\n",
       "  'وهي',\n",
       "  'وهى',\n",
       "  'وهئ'],\n",
       " 'Target': 'وهي'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicies = []\n",
    "for i in range(10000):\n",
    "    length = len(dataset['train'][i][\"Options\"])\n",
    "    if length > 10 and length <= 20:\n",
    "        indicies.append(i)\n",
    "\n",
    "print(indicies)\n",
    "print(len(indicies))\n",
    "dataset['train'][indicies[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ammar/Developer/git-repos/dotless/data/arwiki.wordlist\", 'r') as file:\n",
    "    wordlist = set(file.read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-mix\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "[1, 3, 0, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/ARBERTv2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"fill-mask\", model=\"UBC-NLP/ARBERTv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.706984281539917,\n",
       "  'token': 2134,\n",
       "  'token_str': 'العربية',\n",
       "  'sequence': 'اللغة العربية هي لغة العرب'},\n",
       " {'score': 0.05566714331507683,\n",
       "  'token': 5194,\n",
       "  'token_str': 'التركية',\n",
       "  'sequence': 'اللغة التركية هي لغة العرب'},\n",
       " {'score': 0.04787074029445648,\n",
       "  'token': 11755,\n",
       "  'token_str': 'الكردية',\n",
       "  'sequence': 'اللغة الكردية هي لغة العرب'},\n",
       " {'score': 0.01563572697341442,\n",
       "  'token': 4181,\n",
       "  'token_str': 'الفرنسية',\n",
       "  'sequence': 'اللغة الفرنسية هي لغة العرب'},\n",
       " {'score': 0.012686226516962051,\n",
       "  'token': 1,\n",
       "  'token_str': '[UNK]',\n",
       "  'sequence': 'اللغة هي لغة العرب'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"اللغة [MASK] هي لغة العرب\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3219]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wordTT = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"أحمد\"))\n",
    "test_wordTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أحمد']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wordTid = tokenizer.convert_ids_to_tokens(test_wordTT)\n",
    "test_wordTid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_word_probabilities(input_text, candidate_words):\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    masked_word_index = tokenized_text.index('[MASK]')\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze_(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "\n",
    "    predictions = output.logits[0, masked_word_index].softmax(dim=0)\n",
    "\n",
    "    # Tokenize and convert all candidate words to ids at once\n",
    "\n",
    "    tokenized_candidate_words = [tokenizer.tokenize(word)for word in candidate_words]\n",
    "    #print(tokenized_candidate_words)\n",
    "    \n",
    "    tokenized_candidate_words = [word for word in tokenized_candidate_words if len(word) == 1]\n",
    "    #print(tokenized_candidate_words)\n",
    "    \n",
    "    flattened_tokenized_candidate_words = [word for sublist in tokenized_candidate_words for word in sublist]\n",
    "    #print(flattened_tokenized_candidate_words)\n",
    "    \n",
    "    candidate_word_ids = [tokenizer.convert_tokens_to_ids(TCW) for TCW in flattened_tokenized_candidate_words]\n",
    "    #print(candidate_word_ids)\n",
    "    \n",
    "    # Use a list comprehension to populate the dictionary\n",
    "    candidate_probabilities = {word: predictions[word_id].item() for word, word_id in zip(flattened_tokenized_candidate_words, candidate_word_ids)}\n",
    "    #print(candidate_probabilities)\n",
    "\n",
    "    return candidate_probabilities\n",
    "\n",
    "\n",
    "def generate_probabilties_end(example):\n",
    "    input_text = example[\"Masked\"]\n",
    "    candidates = example[\"Options\"]\n",
    "\n",
    "            \n",
    "    word_probabilities = get_candidate_word_probabilities(input_text, candidates)\n",
    "\n",
    "    sorted_words = sorted(word_probabilities, key=word_probabilities.get, reverse=True)\n",
    "    if len(sorted_words ) > 0:\n",
    "        most_probable_word = sorted_words[0]\n",
    "    else:\n",
    "        most_probable_word = None\n",
    "        print(example[\"Target\"])\n",
    "        \n",
    "    return word_probabilities, sorted_words, most_probable_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ammar/Developer/git-repos/dotless/Models/Model v4/testRestrictedFill.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ammar/Developer/git-repos/dotless/Models/Model%20v4/testRestrictedFill.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_eg \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ammar/Developer/git-repos/dotless/Models/Model%20v4/testRestrictedFill.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#example = dataset['train'][indicies[num_eg]]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ammar/Developer/git-repos/dotless/Models/Model%20v4/testRestrictedFill.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m example \u001b[39m=\u001b[39m mask_random_word(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ammar/Developer/git-repos/dotless/Models/Model%20v4/testRestrictedFill.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m word_probabilities, sorted_words, most_probable_word \u001b[39m=\u001b[39m generate_probabilties_end(example)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ammar/Developer/git-repos/dotless/Models/Model%20v4/testRestrictedFill.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLength of words:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(sorted_words))\n",
      "File \u001b[0;32m~/Developer/git-repos/dotless/Models/Model v4/processingDatasetDotless.py:92\u001b[0m, in \u001b[0;36mmask_random_word\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmask_random_word\u001b[39m(text):\n\u001b[1;32m     90\u001b[0m     words \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n\u001b[0;32m---> 92\u001b[0m     index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(words) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)  \u001b[39m# Generate a random index for each sentence\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words):\n\u001b[1;32m     95\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m index:\n",
      "File \u001b[0;32m~/Developer/git-repos/dotless/env/lib/python3.11/random.py:362\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandint\u001b[39m(\u001b[39mself\u001b[39m, a, b):\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandrange(a, b\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Developer/git-repos/dotless/env/lib/python3.11/random.py:345\u001b[0m, in \u001b[0;36mRandom.randrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m width \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    344\u001b[0m         \u001b[39mreturn\u001b[39;00m istart \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(width)\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mempty range for randrange() (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (istart, istop, width))\n\u001b[1;32m    347\u001b[0m \u001b[39m# Non-unit step argument supplied.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m istep \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: empty range for randrange() (0, 0, 0)"
     ]
    }
   ],
   "source": [
    "num_eg = 0\n",
    "#example = dataset['train'][indicies[num_eg]]\n",
    "example = mask_random_word(\"قرأ أحمد كتابه ثم نام\")\n",
    "word_probabilities, sorted_words, most_probable_word = generate_probabilties_end(example)\n",
    "print(\"Length of words:\", len(sorted_words))\n",
    "for word in sorted_words:\n",
    "    probability = word_probabilities[word]\n",
    "    print(f\"Word: '{word}', Probability: {probability:.10f}\")\n",
    "    \n",
    "print()\n",
    "\n",
    "print(\"Most probable word:\", most_probable_word)\n",
    "print(\"Target word:\", example[\"Target\"])\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "found = False\n",
    "for i in range(len(sorted_words)):\n",
    "    if sorted_words[i] == example[\"Target\"]:\n",
    "        print(\"Sucess at probability level:\", i)\n",
    "        found = True\n",
    "        break\n",
    "if not found: print(\"Not found.\")\n",
    "sucess_level = i\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الزملا\n",
      "تزينت\n",
      "تجربتها\n",
      "بنجران\n",
      "بوشناق\n",
      "ودلائل\n",
      "ناروتو\n",
      "الفندقية\n",
      "المئوية\n",
      "التصفية\n",
      "أدخله\n",
      "المتلاحقة\n",
      "بالرباط\n",
      "لمنهج\n",
      "لآثار\n",
      "وشعرا\n",
      "نيئن\n",
      "إخطاره\n",
      "المزيدكشفت\n",
      "والسياح\n",
      "جيتيت\n",
      "ذويه\n",
      "المطرقة\n",
      "75 placments at level 0   \t 75.0 %\n",
      "2 placments at level 1   \t 2.0 %\n",
      "\n",
      "Number of examples tested: 100\n",
      "Number of examples that yielded a result: 77\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = np.zeros(1000000, dtype =int)\n",
    "#longWords = []\n",
    "#veryLongWords = []\n",
    "#veryVeryLongWords = []\n",
    "start = 0\n",
    "end = 101\n",
    "iterationCount = end - start - 1\n",
    "for i in range(start, end):\n",
    "    num_eg = i\n",
    "    #example1 = mask_random_word(\"التحدي الصعب يكمن في فهم التفاصيل الدقيقة لهذه الفكرة المعقدة\")\n",
    "    #example1 = dataset['train'][indicies[num_eg]]\n",
    "    example1 = dataset['train'][num_eg]\n",
    "\n",
    "    word_probabilities, sorted_words, most_probable_word = generate_probabilties_end(example1)\n",
    "    for word in sorted_words:\n",
    "        probability = word_probabilities[word]\n",
    "        \n",
    "    #if len(sorted_words) >= 500 and len(sorted_words) < 1000: longWords.append(example1[\"Target\"])\n",
    "    #elif len(sorted_words) >= 1000 and len(sorted_words) < 100000: veryLongWords.append(example1[\"Target\"])\n",
    "    #elif len(sorted_words) >= 100000 and len(sorted_words) <1000000: veryVeryLongWords.append(example1[\"Target\"])\n",
    "    #elif len(sorted_words) >= 1000000: veryVeryLongWords.append(example1[\"Target\"])\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(len(sorted_words)):\n",
    "        if sorted_words[j] == example1[\"Target\"]:\n",
    "            count[j] += 1\n",
    "            break\n",
    "        \n",
    "\n",
    "for k in range(len(count)):\n",
    "    numberCount = count[k]\n",
    "    if numberCount != 0:\n",
    "        print(numberCount, \"placments at level\", k, \"  \\t\", (numberCount/iterationCount) * 100, \"%\")\n",
    "        \n",
    "print()\n",
    "print(\"Number of examples tested:\", iterationCount)\n",
    "print(\"Number of examples that yielded a result:\", np.sum(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
