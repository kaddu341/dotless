{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T18:42:16.620501Z",
     "start_time": "2024-04-02T18:42:10.741852Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from pprint import pprint as pprint\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "from processingDatasetDotless import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T18:42:16.857289Z",
     "start_time": "2024-04-02T18:42:16.620692Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0ba625008843338870df8b90b611ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/Users/ammar/Developer/git-repos/dotless/Models/Model v3/AR-multi-dotted-Small-arrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T18:42:16.860850Z",
     "start_time": "2024-04-02T18:42:16.858100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Masked', 'Options', 'Target'],\n",
       "        num_rows: 500000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T18:43:31.847973Z",
     "start_time": "2024-04-02T18:42:29.687448Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataset.push_to_hub(\"dot-ammar/AR-multi-dotted-Small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:51.623813Z",
     "start_time": "2023-11-30T00:50:51.506941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Masked': 'يوسف الذين دفنوا السلاح و المهمات العسكرية في [MASK] من جب بالعاصمة',\n",
       " 'Options': ['آكثر',\n",
       "  'آكثز',\n",
       "  'آكتر',\n",
       "  'آكتز',\n",
       "  'آكبر',\n",
       "  'آكبز',\n",
       "  'إكثر',\n",
       "  'إكثز',\n",
       "  'إكتر',\n",
       "  'إكتز',\n",
       "  'إكبر',\n",
       "  'إكبز',\n",
       "  'أكثر',\n",
       "  'أكثز',\n",
       "  'أكتر',\n",
       "  'أكتز',\n",
       "  'أكبر',\n",
       "  'أكبز',\n",
       "  'اكثر',\n",
       "  'اكثز',\n",
       "  'اكتر',\n",
       "  'اكتز',\n",
       "  'اكبر',\n",
       "  'اكبز'],\n",
       " 'Target': 'أكثر'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.572823Z",
     "start_time": "2023-11-30T00:50:51.518738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 28, 41, 57]...[9968, 9991, 9993, 9996, 9997]\n",
      "1306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Masked': 'السؤال كيف توفق بين دخلك والمنصرفات [MASK] الله كريم فخلص إلى أن',\n",
       " 'Options': ['ؤةي',\n",
       "  'ؤةى',\n",
       "  'ؤةئ',\n",
       "  'ؤهي',\n",
       "  'ؤهى',\n",
       "  'ؤهئ',\n",
       "  'وةي',\n",
       "  'وةى',\n",
       "  'وةئ',\n",
       "  'وهي',\n",
       "  'وهى',\n",
       "  'وهئ'],\n",
       " 'Target': 'وهي'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicies = []\n",
    "for i in range(10000):\n",
    "    length = len(dataset['train'][i][\"Options\"])\n",
    "    if length > 10 and length <= 20:\n",
    "        indicies.append(i)\n",
    "\n",
    "print(str(indicies[0:5]) + \"...\" + str(indicies[-5:]))\n",
    "print(len(indicies))\n",
    "dataset['train'][indicies[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.578606Z",
     "start_time": "2023-11-30T00:50:54.571770Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/ammar/Developer/git-repos/dotless/Models/Model v4/roots.txt\", 'r') as file:\n",
    "    rootlist = set(file.read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/ammar/Developer/git-repos/dotless/data/arwiki.wordlist\", 'r') as file:\n",
    "    arDictionary = set(file.read().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.580606Z",
     "start_time": "2023-11-30T00:50:54.577849Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_len_variations(word):\n",
    "    total_variations = 1\n",
    "\n",
    "    for char in word:\n",
    "        total_variations *= len(iLetters[char])\n",
    "\n",
    "    return total_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.581625Z",
     "start_time": "2023-11-30T00:50:54.579728Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iletter_set = set(iLetters.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.612377Z",
     "start_time": "2023-11-30T00:50:54.592825Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a word with desired variations:\n",
      "Word: افافىا\n",
      "Number of variations: 1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_random_word(length):\n",
    "    return ''.join(random.choice(list(iletter_set)) for _ in range(length))\n",
    "\n",
    "while True:\n",
    "    random_word = generate_random_word(6) # You can adjust the length as needed\n",
    "    num_variations = calculate_len_variations(random_word)\n",
    "    if 1_000 < num_variations < 2_000:\n",
    "        print(\"Found a word with desired variations:\")\n",
    "        print(\"Word:\", random_word)\n",
    "        print(\"Number of variations:\", num_variations)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.612956Z",
     "start_time": "2023-11-30T00:50:54.595653Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"افافىا\"\n",
    "print(len(word))\n",
    "calculate_len_variations(parse_dotless_text(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.695745Z",
     "start_time": "2023-11-30T00:50:54.610339Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ءقءقئا', 'ءقءقئأ', 'ءقءقئإ', 'ءقءقئآ', 'ءقءقئء', 'ءقءقىا', 'ءقءقىأ', 'ءقءقىإ', 'ءقءقىآ', 'ءقءقىء', 'ءقءقيا', 'ءقءقيأ', 'ءقءقيإ', 'ءقءقيآ', 'ءقءقيء', 'ءقءفئا', 'ءقءفئأ', 'ءقءفئإ', 'ءقءفئآ', 'ءقءفئء', 'ءقءفىا', 'ءقءفىأ', 'ءقءفىإ', 'ءقءفىآ', 'ءقءفىء', 'ءقءفيا', 'ءقءفيأ', 'ءقءفيإ', 'ءقءفيآ', 'ءقءفيء', 'ءقآقئا', 'ءقآقئأ', 'ءقآقئإ', 'ءقآقئآ', 'ءقآقئء', 'ءقآقىا', 'ءقآقىأ', 'ءقآقىإ', 'ءقآقىآ', 'ءقآقىء', 'ءقآقيا', 'ءقآقيأ', 'ءقآقيإ', 'ءقآقيآ', 'ءقآقيء', 'ءقآفئا', 'ءقآفئأ', 'ءقآفئإ', 'ءقآفئآ', 'ءقآفئء', 'ءقآفىا', 'ءقآفىأ', 'ءقآفىإ', 'ءقآفىآ', 'ءقآفىء', 'ءقآفيا', 'ءقآفيأ', 'ءقآفيإ', 'ءقآفيآ', 'ءقآفيء', 'ءقإقئا', 'ءقإقئأ', 'ءقإقئإ', 'ءقإقئآ', 'ءقإقئء', 'ءقإقىا', 'ءقإقىأ', 'ءقإقىإ', 'ءقإقىآ', 'ءقإقىء', 'ءقإقيا', 'ءقإقيأ', 'ءقإقيإ', 'ءقإقيآ', 'ءقإقيء', 'ءقإفئا', 'ءقإفئأ', 'ءقإفئإ', 'ءقإفئآ', 'ءقإفئء', 'ءقإفىا', 'ءقإفىأ', 'ءقإفىإ', 'ءقإفىآ', 'ءقإفىء', 'ءقإفيا', 'ءقإفيأ', 'ءقإفيإ', 'ءقإفيآ', 'ءقإفيء', 'ءقأقئا', 'ءقأقئأ', 'ءقأقئإ', 'ءقأقئآ', 'ءقأقئء', 'ءقأقىا', 'ءقأقىأ', 'ءقأقىإ', 'ءقأقىآ', 'ءقأقىء', 'ءقأقيا', 'ءقأقيأ', 'ءقأقيإ', 'ءقأقيآ', 'ءقأقيء', 'ءقأفئا', 'ءقأفئأ', 'ءقأفئإ', 'ءقأفئآ', 'ءقأفئء', 'ءقأفىا', 'ءقأفىأ', 'ءقأفىإ', 'ءقأفىآ', 'ءقأفىء', 'ءقأفيا', 'ءقأفيأ', 'ءقأفيإ', 'ءقأفيآ', 'ءقأفيء', 'ءقاقئا', 'ءقاقئأ', 'ءقاقئإ', 'ءقاقئآ', 'ءقاقئء', 'ءقاقىا', 'ءقاقىأ', 'ءقاقىإ', 'ءقاقىآ', 'ءقاقىء', 'ءقاقيا', 'ءقاقيأ', 'ءقاقيإ', 'ءقاقيآ', 'ءقاقيء', 'ءقافئا', 'ءقافئأ', 'ءقافئإ', 'ءقافئآ', 'ءقافئء', 'ءقافىا', 'ءقافىأ', 'ءقافىإ', 'ءقافىآ', 'ءقافىء', 'ءقافيا', 'ءقافيأ', 'ءقافيإ', 'ءقافيآ', 'ءقافيء', 'ءفءقئا', 'ءفءقئأ', 'ءفءقئإ', 'ءفءقئآ', 'ءفءقئء', 'ءفءقىا', 'ءفءقىأ', 'ءفءقىإ', 'ءفءقىآ', 'ءفءقىء', 'ءفءقيا', 'ءفءقيأ', 'ءفءقيإ', 'ءفءقيآ', 'ءفءقيء', 'ءفءفئا', 'ءفءفئأ', 'ءفءفئإ', 'ءفءفئآ', 'ءفءفئء', 'ءفءفىا', 'ءفءفىأ', 'ءفءفىإ', 'ءفءفىآ', 'ءفءفىء', 'ءفءفيا', 'ءفءفيأ', 'ءفءفيإ', 'ءفءفيآ', 'ءفءفيء', 'ءفآقئا', 'ءفآقئأ', 'ءفآقئإ', 'ءفآقئآ', 'ءفآقئء', 'ءفآقىا', 'ءفآقىأ', 'ءفآقىإ', 'ءفآقىآ', 'ءفآقىء', 'ءفآقيا', 'ءفآقيأ', 'ءفآقيإ', 'ءفآقيآ', 'ءفآقيء', 'ءفآفئا', 'ءفآفئأ', 'ءفآفئإ', 'ءفآفئآ', 'ءفآفئء', 'ءفآفىا', 'ءفآفىأ', 'ءفآفىإ', 'ءفآفىآ', 'ءفآفىء', 'ءفآفيا', 'ءفآفيأ', 'ءفآفيإ', 'ءفآفيآ', 'ءفآفيء', 'ءفإقئا', 'ءفإقئأ', 'ءفإقئإ', 'ءفإقئآ', 'ءفإقئء', 'ءفإقىا', 'ءفإقىأ', 'ءفإقىإ', 'ءفإقىآ', 'ءفإقىء', 'ءفإقيا', 'ءفإقيأ', 'ءفإقيإ', 'ءفإقيآ', 'ءفإقيء', 'ءفإفئا', 'ءفإفئأ', 'ءفإفئإ', 'ءفإفئآ', 'ءفإفئء', 'ءفإفىا', 'ءفإفىأ', 'ءفإفىإ', 'ءفإفىآ', 'ءفإفىء', 'ءفإفيا', 'ءفإفيأ', 'ءفإفيإ', 'ءفإفيآ', 'ءفإفيء', 'ءفأقئا', 'ءفأقئأ', 'ءفأقئإ', 'ءفأقئآ', 'ءفأقئء', 'ءفأقىا', 'ءفأقىأ', 'ءفأقىإ', 'ءفأقىآ', 'ءفأقىء', 'ءفأقيا', 'ءفأقيأ', 'ءفأقيإ', 'ءفأقيآ', 'ءفأقيء', 'ءفأفئا', 'ءفأفئأ', 'ءفأفئإ', 'ءفأفئآ', 'ءفأفئء', 'ءفأفىا', 'ءفأفىأ', 'ءفأفىإ', 'ءفأفىآ', 'ءفأفىء', 'ءفأفيا', 'ءفأفيأ', 'ءفأفيإ', 'ءفأفيآ', 'ءفأفيء', 'ءفاقئا', 'ءفاقئأ', 'ءفاقئإ', 'ءفاقئآ', 'ءفاقئء', 'ءفاقىا', 'ءفاقىأ', 'ءفاقىإ', 'ءفاقىآ', 'ءفاقىء', 'ءفاقيا', 'ءفاقيأ', 'ءفاقيإ', 'ءفاقيآ', 'ءفاقيء', 'ءفافئا', 'ءفافئأ', 'ءفافئإ', 'ءفافئآ', 'ءفافئء', 'ءفافىا', 'ءفافىأ', 'ءفافىإ', 'ءفافىآ', 'ءفافىء', 'ءفافيا', 'ءفافيأ', 'ءفافيإ', 'ءفافيآ', 'ءفافيء', 'آقءقئا', 'آقءقئأ', 'آقءقئإ', 'آقءقئآ', 'آقءقئء', 'آقءقىا', 'آقءقىأ', 'آقءقىإ', 'آقءقىآ', 'آقءقىء', 'آقءقيا', 'آقءقيأ', 'آقءقيإ', 'آقءقيآ', 'آقءقيء', 'آقءفئا', 'آقءفئأ', 'آقءفئإ', 'آقءفئآ', 'آقءفئء', 'آقءفىا', 'آقءفىأ', 'آقءفىإ', 'آقءفىآ', 'آقءفىء', 'آقءفيا', 'آقءفيأ', 'آقءفيإ', 'آقءفيآ', 'آقءفيء', 'آقآقئا', 'آقآقئأ', 'آقآقئإ', 'آقآقئآ', 'آقآقئء', 'آقآقىا', 'آقآقىأ', 'آقآقىإ', 'آقآقىآ', 'آقآقىء', 'آقآقيا', 'آقآقيأ', 'آقآقيإ', 'آقآقيآ', 'آقآقيء', 'آقآفئا', 'آقآفئأ', 'آقآفئإ', 'آقآفئآ', 'آقآفئء', 'آقآفىا', 'آقآفىأ', 'آقآفىإ', 'آقآفىآ', 'آقآفىء', 'آقآفيا', 'آقآفيأ', 'آقآفيإ', 'آقآفيآ', 'آقآفيء', 'آقإقئا', 'آقإقئأ', 'آقإقئإ', 'آقإقئآ', 'آقإقئء', 'آقإقىا', 'آقإقىأ', 'آقإقىإ', 'آقإقىآ', 'آقإقىء', 'آقإقيا', 'آقإقيأ', 'آقإقيإ', 'آقإقيآ', 'آقإقيء', 'آقإفئا', 'آقإفئأ', 'آقإفئإ', 'آقإفئآ', 'آقإفئء', 'آقإفىا', 'آقإفىأ', 'آقإفىإ', 'آقإفىآ', 'آقإفىء', 'آقإفيا', 'آقإفيأ', 'آقإفيإ', 'آقإفيآ', 'آقإفيء', 'آقأقئا', 'آقأقئأ', 'آقأقئإ', 'آقأقئآ', 'آقأقئء', 'آقأقىا', 'آقأقىأ', 'آقأقىإ', 'آقأقىآ', 'آقأقىء', 'آقأقيا', 'آقأقيأ', 'آقأقيإ', 'آقأقيآ', 'آقأقيء', 'آقأفئا', 'آقأفئأ', 'آقأفئإ', 'آقأفئآ', 'آقأفئء', 'آقأفىا', 'آقأفىأ', 'آقأفىإ', 'آقأفىآ', 'آقأفىء', 'آقأفيا', 'آقأفيأ', 'آقأفيإ', 'آقأفيآ', 'آقأفيء', 'آقاقئا', 'آقاقئأ', 'آقاقئإ', 'آقاقئآ', 'آقاقئء', 'آقاقىا', 'آقاقىأ', 'آقاقىإ', 'آقاقىآ', 'آقاقىء', 'آقاقيا', 'آقاقيأ', 'آقاقيإ', 'آقاقيآ', 'آقاقيء', 'آقافئا', 'آقافئأ', 'آقافئإ', 'آقافئآ', 'آقافئء', 'آقافىا', 'آقافىأ', 'آقافىإ', 'آقافىآ', 'آقافىء', 'آقافيا', 'آقافيأ', 'آقافيإ', 'آقافيآ', 'آقافيء', 'آفءقئا', 'آفءقئأ', 'آفءقئإ', 'آفءقئآ', 'آفءقئء', 'آفءقىا', 'آفءقىأ', 'آفءقىإ', 'آفءقىآ', 'آفءقىء', 'آفءقيا', 'آفءقيأ', 'آفءقيإ', 'آفءقيآ', 'آفءقيء', 'آفءفئا', 'آفءفئأ', 'آفءفئإ', 'آفءفئآ', 'آفءفئء', 'آفءفىا', 'آفءفىأ', 'آفءفىإ', 'آفءفىآ', 'آفءفىء', 'آفءفيا', 'آفءفيأ', 'آفءفيإ', 'آفءفيآ', 'آفءفيء', 'آفآقئا', 'آفآقئأ', 'آفآقئإ', 'آفآقئآ', 'آفآقئء', 'آفآقىا', 'آفآقىأ', 'آفآقىإ', 'آفآقىآ', 'آفآقىء', 'آفآقيا', 'آفآقيأ', 'آفآقيإ', 'آفآقيآ', 'آفآقيء', 'آفآفئا', 'آفآفئأ', 'آفآفئإ', 'آفآفئآ', 'آفآفئء', 'آفآفىا', 'آفآفىأ', 'آفآفىإ', 'آفآفىآ', 'آفآفىء', 'آفآفيا', 'آفآفيأ', 'آفآفيإ', 'آفآفيآ', 'آفآفيء', 'آفإقئا', 'آفإقئأ', 'آفإقئإ', 'آفإقئآ', 'آفإقئء', 'آفإقىا', 'آفإقىأ', 'آفإقىإ', 'آفإقىآ', 'آفإقىء', 'آفإقيا', 'آفإقيأ', 'آفإقيإ', 'آفإقيآ', 'آفإقيء', 'آفإفئا', 'آفإفئأ', 'آفإفئإ', 'آفإفئآ', 'آفإفئء', 'آفإفىا', 'آفإفىأ', 'آفإفىإ', 'آفإفىآ', 'آفإفىء', 'آفإفيا', 'آفإفيأ', 'آفإفيإ', 'آفإفيآ', 'آفإفيء', 'آفأقئا', 'آفأقئأ', 'آفأقئإ', 'آفأقئآ', 'آفأقئء', 'آفأقىا', 'آفأقىأ', 'آفأقىإ', 'آفأقىآ', 'آفأقىء', 'آفأقيا', 'آفأقيأ', 'آفأقيإ', 'آفأقيآ', 'آفأقيء', 'آفأفئا', 'آفأفئأ', 'آفأفئإ', 'آفأفئآ', 'آفأفئء', 'آفأفىا', 'آفأفىأ', 'آفأفىإ', 'آفأفىآ', 'آفأفىء', 'آفأفيا', 'آفأفيأ', 'آفأفيإ', 'آفأفيآ', 'آفأفيء', 'آفاقئا', 'آفاقئأ', 'آفاقئإ', 'آفاقئآ', 'آفاقئء', 'آفاقىا', 'آفاقىأ', 'آفاقىإ', 'آفاقىآ', 'آفاقىء', 'آفاقيا', 'آفاقيأ', 'آفاقيإ', 'آفاقيآ', 'آفاقيء', 'آفافئا', 'آفافئأ', 'آفافئإ', 'آفافئآ', 'آفافئء', 'آفافىا', 'آفافىأ', 'آفافىإ', 'آفافىآ', 'آفافىء', 'آفافيا', 'آفافيأ', 'آفافيإ', 'آفافيآ', 'آفافيء', 'إقءقئا', 'إقءقئأ', 'إقءقئإ', 'إقءقئآ', 'إقءقئء', 'إقءقىا', 'إقءقىأ', 'إقءقىإ', 'إقءقىآ', 'إقءقىء', 'إقءقيا', 'إقءقيأ', 'إقءقيإ', 'إقءقيآ', 'إقءقيء', 'إقءفئا', 'إقءفئأ', 'إقءفئإ', 'إقءفئآ', 'إقءفئء', 'إقءفىا', 'إقءفىأ', 'إقءفىإ', 'إقءفىآ', 'إقءفىء', 'إقءفيا', 'إقءفيأ', 'إقءفيإ', 'إقءفيآ', 'إقءفيء', 'إقآقئا', 'إقآقئأ', 'إقآقئإ', 'إقآقئآ', 'إقآقئء', 'إقآقىا', 'إقآقىأ', 'إقآقىإ', 'إقآقىآ', 'إقآقىء', 'إقآقيا', 'إقآقيأ', 'إقآقيإ', 'إقآقيآ', 'إقآقيء', 'إقآفئا', 'إقآفئأ', 'إقآفئإ', 'إقآفئآ', 'إقآفئء', 'إقآفىا', 'إقآفىأ', 'إقآفىإ', 'إقآفىآ', 'إقآفىء', 'إقآفيا', 'إقآفيأ', 'إقآفيإ', 'إقآفيآ', 'إقآفيء', 'إقإقئا', 'إقإقئأ', 'إقإقئإ', 'إقإقئآ', 'إقإقئء', 'إقإقىا', 'إقإقىأ', 'إقإقىإ', 'إقإقىآ', 'إقإقىء', 'إقإقيا', 'إقإقيأ', 'إقإقيإ', 'إقإقيآ', 'إقإقيء', 'إقإفئا', 'إقإفئأ', 'إقإفئإ', 'إقإفئآ', 'إقإفئء', 'إقإفىا', 'إقإفىأ', 'إقإفىإ', 'إقإفىآ', 'إقإفىء', 'إقإفيا', 'إقإفيأ', 'إقإفيإ', 'إقإفيآ', 'إقإفيء', 'إقأقئا', 'إقأقئأ', 'إقأقئإ', 'إقأقئآ', 'إقأقئء', 'إقأقىا', 'إقأقىأ', 'إقأقىإ', 'إقأقىآ', 'إقأقىء', 'إقأقيا', 'إقأقيأ', 'إقأقيإ', 'إقأقيآ', 'إقأقيء', 'إقأفئا', 'إقأفئأ', 'إقأفئإ', 'إقأفئآ', 'إقأفئء', 'إقأفىا', 'إقأفىأ', 'إقأفىإ', 'إقأفىآ', 'إقأفىء', 'إقأفيا', 'إقأفيأ', 'إقأفيإ', 'إقأفيآ', 'إقأفيء', 'إقاقئا', 'إقاقئأ', 'إقاقئإ', 'إقاقئآ', 'إقاقئء', 'إقاقىا', 'إقاقىأ', 'إقاقىإ', 'إقاقىآ', 'إقاقىء', 'إقاقيا', 'إقاقيأ', 'إقاقيإ', 'إقاقيآ', 'إقاقيء', 'إقافئا', 'إقافئأ', 'إقافئإ', 'إقافئآ', 'إقافئء', 'إقافىا', 'إقافىأ', 'إقافىإ', 'إقافىآ', 'إقافىء', 'إقافيا', 'إقافيأ', 'إقافيإ', 'إقافيآ', 'إقافيء', 'إفءقئا', 'إفءقئأ', 'إفءقئإ', 'إفءقئآ', 'إفءقئء', 'إفءقىا', 'إفءقىأ', 'إفءقىإ', 'إفءقىآ', 'إفءقىء', 'إفءقيا', 'إفءقيأ', 'إفءقيإ', 'إفءقيآ', 'إفءقيء', 'إفءفئا', 'إفءفئأ', 'إفءفئإ', 'إفءفئآ', 'إفءفئء', 'إفءفىا', 'إفءفىأ', 'إفءفىإ', 'إفءفىآ', 'إفءفىء', 'إفءفيا', 'إفءفيأ', 'إفءفيإ', 'إفءفيآ', 'إفءفيء', 'إفآقئا', 'إفآقئأ', 'إفآقئإ', 'إفآقئآ', 'إفآقئء', 'إفآقىا', 'إفآقىأ', 'إفآقىإ', 'إفآقىآ', 'إفآقىء', 'إفآقيا', 'إفآقيأ', 'إفآقيإ', 'إفآقيآ', 'إفآقيء', 'إفآفئا', 'إفآفئأ', 'إفآفئإ', 'إفآفئآ', 'إفآفئء', 'إفآفىا', 'إفآفىأ', 'إفآفىإ', 'إفآفىآ', 'إفآفىء', 'إفآفيا', 'إفآفيأ', 'إفآفيإ', 'إفآفيآ', 'إفآفيء', 'إفإقئا', 'إفإقئأ', 'إفإقئإ', 'إفإقئآ', 'إفإقئء', 'إفإقىا', 'إفإقىأ', 'إفإقىإ', 'إفإقىآ', 'إفإقىء', 'إفإقيا', 'إفإقيأ', 'إفإقيإ', 'إفإقيآ', 'إفإقيء', 'إفإفئا', 'إفإفئأ', 'إفإفئإ', 'إفإفئآ', 'إفإفئء', 'إفإفىا', 'إفإفىأ', 'إفإفىإ', 'إفإفىآ', 'إفإفىء', 'إفإفيا', 'إفإفيأ', 'إفإفيإ', 'إفإفيآ', 'إفإفيء', 'إفأقئا', 'إفأقئأ', 'إفأقئإ', 'إفأقئآ', 'إفأقئء', 'إفأقىا', 'إفأقىأ', 'إفأقىإ', 'إفأقىآ', 'إفأقىء', 'إفأقيا', 'إفأقيأ', 'إفأقيإ', 'إفأقيآ', 'إفأقيء', 'إفأفئا', 'إفأفئأ', 'إفأفئإ', 'إفأفئآ', 'إفأفئء', 'إفأفىا', 'إفأفىأ', 'إفأفىإ', 'إفأفىآ', 'إفأفىء', 'إفأفيا', 'إفأفيأ', 'إفأفيإ', 'إفأفيآ', 'إفأفيء', 'إفاقئا', 'إفاقئأ', 'إفاقئإ', 'إفاقئآ', 'إفاقئء', 'إفاقىا', 'إفاقىأ', 'إفاقىإ', 'إفاقىآ', 'إفاقىء', 'إفاقيا', 'إفاقيأ', 'إفاقيإ', 'إفاقيآ', 'إفاقيء', 'إفافئا', 'إفافئأ', 'إفافئإ', 'إفافئآ', 'إفافئء', 'إفافىا', 'إفافىأ', 'إفافىإ', 'إفافىآ', 'إفافىء', 'إفافيا', 'إفافيأ', 'إفافيإ', 'إفافيآ', 'إفافيء', 'أقءقئا', 'أقءقئأ', 'أقءقئإ', 'أقءقئآ', 'أقءقئء', 'أقءقىا', 'أقءقىأ', 'أقءقىإ', 'أقءقىآ', 'أقءقىء', 'أقءقيا', 'أقءقيأ', 'أقءقيإ', 'أقءقيآ', 'أقءقيء', 'أقءفئا', 'أقءفئأ', 'أقءفئإ', 'أقءفئآ', 'أقءفئء', 'أقءفىا', 'أقءفىأ', 'أقءفىإ', 'أقءفىآ', 'أقءفىء', 'أقءفيا', 'أقءفيأ', 'أقءفيإ', 'أقءفيآ', 'أقءفيء', 'أقآقئا', 'أقآقئأ', 'أقآقئإ', 'أقآقئآ', 'أقآقئء', 'أقآقىا', 'أقآقىأ', 'أقآقىإ', 'أقآقىآ', 'أقآقىء', 'أقآقيا', 'أقآقيأ', 'أقآقيإ', 'أقآقيآ', 'أقآقيء', 'أقآفئا', 'أقآفئأ', 'أقآفئإ', 'أقآفئآ', 'أقآفئء', 'أقآفىا', 'أقآفىأ', 'أقآفىإ', 'أقآفىآ', 'أقآفىء', 'أقآفيا', 'أقآفيأ', 'أقآفيإ', 'أقآفيآ', 'أقآفيء', 'أقإقئا', 'أقإقئأ', 'أقإقئإ', 'أقإقئآ', 'أقإقئء', 'أقإقىا', 'أقإقىأ', 'أقإقىإ', 'أقإقىآ', 'أقإقىء', 'أقإقيا', 'أقإقيأ', 'أقإقيإ', 'أقإقيآ', 'أقإقيء', 'أقإفئا', 'أقإفئأ', 'أقإفئإ', 'أقإفئآ', 'أقإفئء', 'أقإفىا', 'أقإفىأ', 'أقإفىإ', 'أقإفىآ', 'أقإفىء', 'أقإفيا', 'أقإفيأ', 'أقإفيإ', 'أقإفيآ', 'أقإفيء', 'أقأقئا', 'أقأقئأ', 'أقأقئإ', 'أقأقئآ', 'أقأقئء', 'أقأقىا', 'أقأقىأ', 'أقأقىإ', 'أقأقىآ', 'أقأقىء', 'أقأقيا', 'أقأقيأ', 'أقأقيإ', 'أقأقيآ', 'أقأقيء', 'أقأفئا', 'أقأفئأ', 'أقأفئإ', 'أقأفئآ', 'أقأفئء', 'أقأفىا', 'أقأفىأ', 'أقأفىإ', 'أقأفىآ', 'أقأفىء', 'أقأفيا', 'أقأفيأ', 'أقأفيإ', 'أقأفيآ', 'أقأفيء', 'أقاقئا', 'أقاقئأ', 'أقاقئإ', 'أقاقئآ', 'أقاقئء', 'أقاقىا', 'أقاقىأ', 'أقاقىإ', 'أقاقىآ', 'أقاقىء', 'أقاقيا', 'أقاقيأ', 'أقاقيإ', 'أقاقيآ', 'أقاقيء', 'أقافئا', 'أقافئأ', 'أقافئإ', 'أقافئآ', 'أقافئء', 'أقافىا', 'أقافىأ', 'أقافىإ', 'أقافىآ', 'أقافىء', 'أقافيا', 'أقافيأ', 'أقافيإ', 'أقافيآ', 'أقافيء', 'أفءقئا', 'أفءقئأ', 'أفءقئإ', 'أفءقئآ', 'أفءقئء', 'أفءقىا', 'أفءقىأ', 'أفءقىإ', 'أفءقىآ', 'أفءقىء', 'أفءقيا', 'أفءقيأ', 'أفءقيإ', 'أفءقيآ', 'أفءقيء', 'أفءفئا', 'أفءفئأ', 'أفءفئإ', 'أفءفئآ', 'أفءفئء', 'أفءفىا', 'أفءفىأ', 'أفءفىإ', 'أفءفىآ', 'أفءفىء', 'أفءفيا', 'أفءفيأ', 'أفءفيإ', 'أفءفيآ', 'أفءفيء', 'أفآقئا', 'أفآقئأ', 'أفآقئإ', 'أفآقئآ', 'أفآقئء', 'أفآقىا', 'أفآقىأ', 'أفآقىإ', 'أفآقىآ', 'أفآقىء', 'أفآقيا', 'أفآقيأ', 'أفآقيإ', 'أفآقيآ', 'أفآقيء', 'أفآفئا', 'أفآفئأ', 'أفآفئإ', 'أفآفئآ', 'أفآفئء', 'أفآفىا', 'أفآفىأ', 'أفآفىإ', 'أفآفىآ', 'أفآفىء', 'أفآفيا', 'أفآفيأ', 'أفآفيإ', 'أفآفيآ', 'أفآفيء', 'أفإقئا', 'أفإقئأ', 'أفإقئإ', 'أفإقئآ', 'أفإقئء', 'أفإقىا', 'أفإقىأ', 'أفإقىإ', 'أفإقىآ', 'أفإقىء', 'أفإقيا', 'أفإقيأ', 'أفإقيإ', 'أفإقيآ', 'أفإقيء', 'أفإفئا', 'أفإفئأ', 'أفإفئإ', 'أفإفئآ', 'أفإفئء', 'أفإفىا', 'أفإفىأ', 'أفإفىإ', 'أفإفىآ', 'أفإفىء', 'أفإفيا', 'أفإفيأ', 'أفإفيإ', 'أفإفيآ', 'أفإفيء', 'أفأقئا', 'أفأقئأ', 'أفأقئإ', 'أفأقئآ', 'أفأقئء', 'أفأقىا', 'أفأقىأ', 'أفأقىإ', 'أفأقىآ', 'أفأقىء', 'أفأقيا', 'أفأقيأ', 'أفأقيإ', 'أفأقيآ', 'أفأقيء', 'أفأفئا', 'أفأفئأ', 'أفأفئإ', 'أفأفئآ', 'أفأفئء', 'أفأفىا', 'أفأفىأ', 'أفأفىإ', 'أفأفىآ', 'أفأفىء', 'أفأفيا', 'أفأفيأ', 'أفأفيإ', 'أفأفيآ', 'أفأفيء', 'أفاقئا', 'أفاقئأ', 'أفاقئإ', 'أفاقئآ', 'أفاقئء', 'أفاقىا', 'أفاقىأ', 'أفاقىإ', 'أفاقىآ', 'أفاقىء', 'أفاقيا', 'أفاقيأ', 'أفاقيإ', 'أفاقيآ', 'أفاقيء', 'أفافئا', 'أفافئأ', 'أفافئإ', 'أفافئآ', 'أفافئء', 'أفافىا', 'أفافىأ', 'أفافىإ', 'أفافىآ', 'أفافىء', 'أفافيا', 'أفافيأ', 'أفافيإ', 'أفافيآ', 'أفافيء', 'اقءقئا', 'اقءقئأ', 'اقءقئإ', 'اقءقئآ', 'اقءقئء', 'اقءقىا', 'اقءقىأ', 'اقءقىإ', 'اقءقىآ', 'اقءقىء', 'اقءقيا', 'اقءقيأ', 'اقءقيإ', 'اقءقيآ', 'اقءقيء', 'اقءفئا', 'اقءفئأ', 'اقءفئإ', 'اقءفئآ', 'اقءفئء', 'اقءفىا', 'اقءفىأ', 'اقءفىإ', 'اقءفىآ', 'اقءفىء', 'اقءفيا', 'اقءفيأ', 'اقءفيإ', 'اقءفيآ', 'اقءفيء', 'اقآقئا', 'اقآقئأ', 'اقآقئإ', 'اقآقئآ', 'اقآقئء', 'اقآقىا', 'اقآقىأ', 'اقآقىإ', 'اقآقىآ', 'اقآقىء', 'اقآقيا', 'اقآقيأ', 'اقآقيإ', 'اقآقيآ', 'اقآقيء', 'اقآفئا', 'اقآفئأ', 'اقآفئإ', 'اقآفئآ', 'اقآفئء', 'اقآفىا', 'اقآفىأ', 'اقآفىإ', 'اقآفىآ', 'اقآفىء', 'اقآفيا', 'اقآفيأ', 'اقآفيإ', 'اقآفيآ', 'اقآفيء', 'اقإقئا', 'اقإقئأ', 'اقإقئإ', 'اقإقئآ', 'اقإقئء', 'اقإقىا', 'اقإقىأ', 'اقإقىإ', 'اقإقىآ', 'اقإقىء', 'اقإقيا', 'اقإقيأ', 'اقإقيإ', 'اقإقيآ', 'اقإقيء', 'اقإفئا', 'اقإفئأ', 'اقإفئإ', 'اقإفئآ', 'اقإفئء', 'اقإفىا', 'اقإفىأ', 'اقإفىإ', 'اقإفىآ', 'اقإفىء', 'اقإفيا', 'اقإفيأ', 'اقإفيإ', 'اقإفيآ', 'اقإفيء', 'اقأقئا', 'اقأقئأ', 'اقأقئإ', 'اقأقئآ', 'اقأقئء', 'اقأقىا', 'اقأقىأ', 'اقأقىإ', 'اقأقىآ', 'اقأقىء', 'اقأقيا', 'اقأقيأ', 'اقأقيإ', 'اقأقيآ', 'اقأقيء', 'اقأفئا', 'اقأفئأ', 'اقأفئإ', 'اقأفئآ', 'اقأفئء', 'اقأفىا', 'اقأفىأ', 'اقأفىإ', 'اقأفىآ', 'اقأفىء', 'اقأفيا', 'اقأفيأ', 'اقأفيإ', 'اقأفيآ', 'اقأفيء', 'اقاقئا', 'اقاقئأ', 'اقاقئإ', 'اقاقئآ', 'اقاقئء', 'اقاقىا', 'اقاقىأ', 'اقاقىإ', 'اقاقىآ', 'اقاقىء', 'اقاقيا', 'اقاقيأ', 'اقاقيإ', 'اقاقيآ', 'اقاقيء', 'اقافئا', 'اقافئأ', 'اقافئإ', 'اقافئآ', 'اقافئء', 'اقافىا', 'اقافىأ', 'اقافىإ', 'اقافىآ', 'اقافىء', 'اقافيا', 'اقافيأ', 'اقافيإ', 'اقافيآ', 'اقافيء', 'افءقئا', 'افءقئأ', 'افءقئإ', 'افءقئآ', 'افءقئء', 'افءقىا', 'افءقىأ', 'افءقىإ', 'افءقىآ', 'افءقىء', 'افءقيا', 'افءقيأ', 'افءقيإ', 'افءقيآ', 'افءقيء', 'افءفئا', 'افءفئأ', 'افءفئإ', 'افءفئآ', 'افءفئء', 'افءفىا', 'افءفىأ', 'افءفىإ', 'افءفىآ', 'افءفىء', 'افءفيا', 'افءفيأ', 'افءفيإ', 'افءفيآ', 'افءفيء', 'افآقئا', 'افآقئأ', 'افآقئإ', 'افآقئآ', 'افآقئء', 'افآقىا', 'افآقىأ', 'افآقىإ', 'افآقىآ', 'افآقىء', 'افآقيا', 'افآقيأ', 'افآقيإ', 'افآقيآ', 'افآقيء', 'افآفئا', 'افآفئأ', 'افآفئإ', 'افآفئآ', 'افآفئء', 'افآفىا', 'افآفىأ', 'افآفىإ', 'افآفىآ', 'افآفىء', 'افآفيا', 'افآفيأ', 'افآفيإ', 'افآفيآ', 'افآفيء', 'افإقئا', 'افإقئأ', 'افإقئإ', 'افإقئآ', 'افإقئء', 'افإقىا', 'افإقىأ', 'افإقىإ', 'افإقىآ', 'افإقىء', 'افإقيا', 'افإقيأ', 'افإقيإ', 'افإقيآ', 'افإقيء', 'افإفئا', 'افإفئأ', 'افإفئإ', 'افإفئآ', 'افإفئء', 'افإفىا', 'افإفىأ', 'افإفىإ', 'افإفىآ', 'افإفىء', 'افإفيا', 'افإفيأ', 'افإفيإ', 'افإفيآ', 'افإفيء', 'افأقئا', 'افأقئأ', 'افأقئإ', 'افأقئآ', 'افأقئء', 'افأقىا', 'افأقىأ', 'افأقىإ', 'افأقىآ', 'افأقىء', 'افأقيا', 'افأقيأ', 'افأقيإ', 'افأقيآ', 'افأقيء', 'افأفئا', 'افأفئأ', 'افأفئإ', 'افأفئآ', 'افأفئء', 'افأفىا', 'افأفىأ', 'افأفىإ', 'افأفىآ', 'افأفىء', 'افأفيا', 'افأفيأ', 'افأفيإ', 'افأفيآ', 'افأفيء', 'افاقئا', 'افاقئأ', 'افاقئإ', 'افاقئآ', 'افاقئء', 'افاقىا', 'افاقىأ', 'افاقىإ', 'افاقىآ', 'افاقىء', 'افاقيا', 'افاقيأ', 'افاقيإ', 'افاقيآ', 'افاقيء', 'افافئا', 'افافئأ', 'افافئإ', 'افافئآ', 'افافئء', 'افافىا', 'افافىأ', 'افافىإ', 'افافىآ', 'افافىء', 'افافيا', 'افافيأ', 'افافيإ', 'افافيآ', 'افافيء']\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "variations = allVariation(parse_dotless_text(word))\n",
    "print(variations)\n",
    "print(len(variations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:54.696059Z",
     "start_time": "2023-11-30T00:50:54.651062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"#\" in list(rootlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-mix\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.192186Z",
     "start_time": "2023-11-30T00:50:54.651269Z"
    },
    "tags": [
     "model load"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "Some layers of TFBertForMaskedLM were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth and are newly initialized: ['mlm___cls']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.197583Z",
     "start_time": "2023-11-30T00:50:56.192878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "[1, 3, 0, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.204218Z",
     "start_time": "2023-11-30T00:50:56.200291Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ArListem = ArabicLightStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.205472Z",
     "start_time": "2023-11-30T00:50:56.202590Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = 'السعادة'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.211142Z",
     "start_time": "2023-11-30T00:50:56.205186Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13876]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.212741Z",
     "start_time": "2023-11-30T00:50:56.211246Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السعادة']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenID = tokenizer.convert_ids_to_tokens(tokenized_word)\n",
    "word_tokenID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(word_tokenID[0] + word_tokenID[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.256684Z",
     "start_time": "2023-11-30T00:50:56.214867Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: سعاد\n",
      "root: سعد\n"
     ]
    }
   ],
   "source": [
    "stem = ArListem.light_stem(word_tokenID[0])\n",
    "root = ArListem.get_root()\n",
    "print(\"stem:\", stem)\n",
    "print(\"root:\", root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.257608Z",
     "start_time": "2023-11-30T00:50:56.217518Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: سعاد\n",
      "root: سعد\n"
     ]
    }
   ],
   "source": [
    "stem = ArListem.light_stem(word)\n",
    "root = ArListem.get_root()\n",
    "print(\"stem:\", stem)\n",
    "print(\"root:\", root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.258013Z",
     "start_time": "2023-11-30T00:50:56.219986Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root in rootlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem in rootlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.475744Z",
     "start_time": "2023-11-30T00:50:56.227814Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['جمع', '[MASK]', 'سكين']\n",
      "tensor([[ 3341,     4, 29555]])\n",
      "السعادة 5.7757080895726176e-08\n",
      "##ة 2.41798346678479e-07\n"
     ]
    }
   ],
   "source": [
    "sentance = \"جمع [MASK] سكين\"\n",
    "word1 =\"السعادة\" \n",
    "word2 = \"##ة\"\n",
    "\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(sentance)\n",
    "masked_word_index = tokenized_text.index('[MASK]')\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze_(0)\n",
    "\n",
    "print(tokenized_text)\n",
    "print(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "predictions = output.logits[0, masked_word_index].softmax(dim=0)\n",
    "\n",
    "print(word1, predictions[tokenizer.convert_tokens_to_ids(word1)].item())\n",
    "print(word2, predictions[tokenizer.convert_tokens_to_ids(word2)].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.478947Z",
     "start_time": "2023-11-30T00:50:56.476408Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['السعادة']\n",
      "['سعد']\n",
      "[True]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "word_tokenID = [w.replace('#', '') for w in word_tokenID]\n",
    "print(word_tokenID)\n",
    "\n",
    "roots = []\n",
    "for w in word_tokenID:\n",
    "    stem = ArListem.light_stem(w)\n",
    "    root = ArListem.get_root()\n",
    "    roots.append(root)\n",
    "\n",
    "print(roots)\n",
    "\n",
    "is_root = [w in rootlist for w in roots]\n",
    "print(is_root)\n",
    "\n",
    "#check if there is at least one true\n",
    "true_root = any(is_root)\n",
    "print(true_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'احتاج'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qalsadi.lemmatizer  as lemmatizer\n",
    "import qalsadi.analex as analax\n",
    "\n",
    "lemmer = lemmatizer.Lemmatizer()\n",
    "lemmer.lemmatize(\"يحتاج\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{\n",
       "  \t\tu'word' = u'يحتاج', \n",
       "  \t\tu'vocalized' = u'يَحْتَاجَ', \n",
       "  \t\tu'unvocalized' = u'', \n",
       "  \t\tu'semivocalized' = u'يَحْتَاج', \n",
       "  \t\tu'tags' = u'المضارع المنصوب:هو:n:', \n",
       "  \t\tu'affix_key' = u'-ي--|المضارع المنصوب:هو:n', \n",
       "  \t\tu'stem' = u'حتاج', \n",
       "  \t\tu'original_tags' = u'('',)', \n",
       "  \t\tu'freq' = u'207567', \n",
       "  \t\tu'type' = u'Verb', \n",
       "  \t\tu'original' = u'اِحْتَاجَ', \n",
       "  \t\tu'tag_regular' = u'True', \n",
       "  \t\tu'root' = u'حوج', \n",
       "  \t\tu'affix' = u'Taha', \n",
       "  \t\tu'action' = u'', \n",
       "  \t\tu'object_type' = u'', \n",
       "  \t\tu'need' = u'', \n",
       "  \t\tu'tag_type' = u'2', \n",
       "  \t\tu'tag_added' = u'False', \n",
       "  \t\tu'tag_initial' = u'False', \n",
       "  \t\tu'tag_transparent' = u'False', \n",
       "  \t\tu'tag_mamnou3' = u'False', \n",
       "  \t\tu'tag_break' = u'False', \n",
       "  \t\tu'tag_voice' = u'معلوم', \n",
       "  \t\tu'tag_mood' = u'منصوب', \n",
       "  \t\tu'tag_confirmed' = u'', \n",
       "  \t\tu'tag_pronoun' = u'هو', \n",
       "  \t\tu'tag_transitive' = u'False', \n",
       "  \t\tu'tag_person' = u'4', \n",
       "  \t\tu'tag_original_number' = u'مفرد', \n",
       "  \t\tu'tag_original_gender' = u'مذكر', \n",
       "  \t\tu'tag_number' = u'1', \n",
       "  \t\tu'tag_gender' = u'1', \n",
       "  \t\tu'tag_tense' = u'المضارع المنصوب', \n",
       "  \t\t},\n",
       "  {\n",
       "  \t\tu'word' = u'يحتاج', \n",
       "  \t\tu'vocalized' = u'يَحْتَاجُ', \n",
       "  \t\tu'unvocalized' = u'', \n",
       "  \t\tu'semivocalized' = u'يَحْتَاج', \n",
       "  \t\tu'tags' = u'المضارع المعلوم:هو:n:', \n",
       "  \t\tu'affix_key' = u'-ي--|المضارع المعلوم:هو:n', \n",
       "  \t\tu'stem' = u'حتاج', \n",
       "  \t\tu'original_tags' = u'('',)', \n",
       "  \t\tu'freq' = u'207567', \n",
       "  \t\tu'type' = u'Verb', \n",
       "  \t\tu'original' = u'اِحْتَاجَ', \n",
       "  \t\tu'tag_regular' = u'True', \n",
       "  \t\tu'root' = u'حوج', \n",
       "  \t\tu'affix' = u'Taha', \n",
       "  \t\tu'action' = u'', \n",
       "  \t\tu'object_type' = u'', \n",
       "  \t\tu'need' = u'', \n",
       "  \t\tu'tag_type' = u'2', \n",
       "  \t\tu'tag_added' = u'False', \n",
       "  \t\tu'tag_initial' = u'False', \n",
       "  \t\tu'tag_transparent' = u'False', \n",
       "  \t\tu'tag_mamnou3' = u'False', \n",
       "  \t\tu'tag_break' = u'False', \n",
       "  \t\tu'tag_voice' = u'معلوم', \n",
       "  \t\tu'tag_mood' = u'مرفوع', \n",
       "  \t\tu'tag_confirmed' = u'', \n",
       "  \t\tu'tag_pronoun' = u'هو', \n",
       "  \t\tu'tag_transitive' = u'False', \n",
       "  \t\tu'tag_person' = u'4', \n",
       "  \t\tu'tag_original_number' = u'مفرد', \n",
       "  \t\tu'tag_original_gender' = u'مذكر', \n",
       "  \t\tu'tag_number' = u'1', \n",
       "  \t\tu'tag_gender' = u'1', \n",
       "  \t\tu'tag_tense' = u'المضارع المعلوم', \n",
       "  \t\t}]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qalsadi.analex as analax\n",
    "analyzer = analax.Analex()\n",
    "\n",
    "analyzer.set_debug(False)\n",
    "text = \"يحتاج\"\n",
    "result = analyzer.check_text(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حتاج\n",
      "حوج\n"
     ]
    }
   ],
   "source": [
    "stem = ArListem.light_stem(\"يحتاج\")\n",
    "root = ArListem.get_root()\n",
    "print(stem)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check For Root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T01:25:30.226538Z",
     "start_time": "2023-11-30T01:25:30.216863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_for_root(tokens):\n",
    "    tokens = [w.replace('#', '') for w in tokens]\n",
    "    roots = []\n",
    "    for w in tokens:\n",
    "        stem = ArListem.light_stem(w)\n",
    "        root = ArListem.get_root()\n",
    "        roots.append(root)\n",
    "\n",
    "    is_roots = [w in rootlist for w in roots]\n",
    "    any_true_root = any(is_roots)\n",
    "    all_true_root = all(is_roots)\n",
    "    real_roots = [roots[i] for i in range(len(roots)) if is_roots[i]]\n",
    "    return any_true_root, all_true_root, is_roots, roots, real_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.487997Z",
     "start_time": "2023-11-30T00:50:56.484868Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السعادة']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السعادة\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.492860Z",
     "start_time": "2023-11-30T00:50:56.488466Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, [True], ['سعد'], ['سعد'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_for_root([\"السعادة\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "text = \"The Milky Way is a [MASK] galaxy.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "logits = model(**inputs).logits\n",
    "mask_token_logits = logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "top_tokens = torch.topk(mask_token_logits, 10000, dim=1).indices[0].tolist()[-10:-1]\n",
    "\n",
    "for token in top_tokens:\n",
    "    token_text = tokenizer.convert_ids_to_tokens(token)\n",
    "    print(token_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cand Prob on root mixed prob\n",
    "## Status: failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.548479Z",
     "start_time": "2023-11-30T00:50:56.495845Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_candidate_word_probabilities1(input_text, candidate_words):\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    masked_word_index = tokenized_text.index('[MASK]')\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze_(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "\n",
    "    predictions = output.logits[0, masked_word_index].softmax(dim=0)\n",
    "    initial_length = len(candidate_words)\n",
    "    \n",
    "    filtered_candidate_probabilities = []\n",
    "    filtered_candidate_words = []\n",
    "    \n",
    "    for word in candidate_words:\n",
    "        \n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        any_true_root, all_true_root, is_roots, roots, real_roots = check_for_root(word_tokens)\n",
    "            \n",
    "        # print(\"word:\", word)\n",
    "        # print(\"true_root:\", true_root)\n",
    "        # print(\"is_roots:\", is_roots)\n",
    "        # print(\"roots:\", roots)\n",
    "        # print(\"word_tokens:\", word_tokens)\n",
    "        # print(\"-------------------------------------\")\n",
    "        \n",
    "        if not true_root:\n",
    "            # print(\"Rejected\")\n",
    "            # print(\"--------------------------------------------------------------------------\")\n",
    "            continue\n",
    "            \n",
    "        word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "        \n",
    "        total_predictions = 0\n",
    "        predictionList = []\n",
    "        for wid in word_ids:\n",
    "            prediction = predictions[wid].item()\n",
    "            predictionList.append(prediction)\n",
    "            total_predictions += prediction     \n",
    "        average = sum(predictionList) / len(predictionList)\n",
    "\n",
    "        filtered_candidate_probabilities.append(average)\n",
    "        filtered_candidate_words.append(word)\n",
    "        # print(f\"{word} : {average:.20f}\")\n",
    "        # print({token: probability for token, probability in zip(word_tokens, predictionList)})\n",
    "        # print(\"-------------------------------------\")\n",
    "    \n",
    "    candidate_probabilities = {word: probability for word, probability in zip(filtered_candidate_words, filtered_candidate_probabilities)}\n",
    "    \n",
    "\n",
    "    return candidate_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cand Prob on single token presence \n",
    "## Status: MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T01:27:52.608818Z",
     "start_time": "2023-11-30T01:27:52.596593Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_candidate_word_probabilities(input_text, candidate_words):\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    masked_word_index = tokenized_text.index('[MASK]')\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze_(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "\n",
    "    predictions = output.logits[0, masked_word_index].softmax(dim=0)\n",
    "\n",
    "    \n",
    "    # Tokenize and convert all candidate words to ids at once\n",
    "     \n",
    "    pre_tokenized_candidate_words = [tokenizer.tokenize(word) for word in candidate_words]\n",
    "    \n",
    "            \n",
    "    \n",
    "    #print(tokenized_candidate_words)\n",
    "    \n",
    "    tokenized_candidate_words = []\n",
    "    verified_words = []\n",
    "    for word in pre_tokenized_candidate_words:\n",
    "        rep_word = candidate_words[pre_tokenized_candidate_words.index(word)]\n",
    "        \n",
    "        if len(word) == 1:\n",
    "            tokenized_candidate_words.append(word[0])\n",
    "            verified_words.append(rep_word)\n",
    "            \n",
    "        else:\n",
    "            #lemma = lemmer.lemmatize(rep_word)\n",
    "            stem = ArListem.light_stem(rep_word)\n",
    "            root = ArListem.get_root()\n",
    "            \n",
    "            if len(tokenizer.tokenize(root)) == 1:\n",
    "                if rep_word not in arDictionary:\n",
    "                    continue\n",
    "                tokenized_candidate_words.append(tokenizer.tokenize(root)[0])\n",
    "                verified_words.append(rep_word)\n",
    "                # print(\"root:\", root)\n",
    "                # print(\"rep_word:\", rep_word)\n",
    "            \n",
    "            \n",
    "                \n",
    "\n",
    "            \n",
    "    #print(tokenized_candidate_words)\n",
    "    #print(verified_words)\n",
    "    flattened_tokenized_candidate_words = [word for sublist in tokenized_candidate_words for word in sublist]\n",
    "    #print(flattened_tokenized_candidate_words)\n",
    "\n",
    "    candidate_word_ids = [tokenizer.convert_tokens_to_ids(TCW) for TCW in flattened_tokenized_candidate_words]\n",
    "    #print(candidate_word_ids)\n",
    "\n",
    "    # Use a list comprehension to populate the dictionary\n",
    "    candidate_probabilities = {word: predictions[word_id].item() for word, word_id in\n",
    "                               zip(verified_words, candidate_word_ids)}\n",
    "    #print(candidate_probabilities)     \n",
    "\n",
    "    return candidate_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_word_probabilities(input_text, candidate_words):\n",
    "    tokenized_text = tokenizer.tokenize(input_text)\n",
    "    masked_word_index = tokenized_text.index('[MASK]')\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    input_ids = tf.constant([input_ids], dtype=tf.int32)  # tf.constant automatically adds batch dimension\n",
    "    \n",
    "    # Perform prediction\n",
    "    output = model(input_ids)\n",
    "    \n",
    "    # Extract logits for the masked word and apply softmax\n",
    "    predictions = tf.nn.softmax(output.logits[0, masked_word_index])\n",
    "\n",
    "    # Tokenize and verify candidate words\n",
    "    pre_tokenized_candidate_words = [tokenizer.tokenize(word) for word in candidate_words]\n",
    "    tokenized_candidate_words = []\n",
    "    verified_words = []\n",
    "    for word in pre_tokenized_candidate_words:\n",
    "        rep_word = candidate_words[pre_tokenized_candidate_words.index(word)]\n",
    "        if len(word) == 1:\n",
    "            tokenized_candidate_words.append(word[0])\n",
    "            verified_words.append(rep_word)\n",
    "        else:\n",
    "            #if rep_word not in arDictionary: continue\n",
    "            stem = ArListem.light_stem(rep_word)\n",
    "            root = ArListem.get_root()\n",
    "            \n",
    "            if len(tokenizer.tokenize(stem)) == 1:\n",
    "                tokenized_candidate_words.append(tokenizer.tokenize(stem)[0])\n",
    "                verified_words.append(rep_word)\n",
    "                \n",
    "            elif len(tokenizer.tokenize(root)) == 1:\n",
    "                tokenized_candidate_words.append(tokenizer.tokenize(root)[0])\n",
    "                verified_words.append(rep_word)\n",
    "   \n",
    "    # Convert tokens to IDs\n",
    "    candidate_word_ids = [tokenizer.convert_tokens_to_ids([word]) for word in tokenized_candidate_words]\n",
    "\n",
    "    # Calculate probabilities for each candidate word\n",
    "    candidate_probabilities = {word: predictions[word_id].numpy() for word, word_id in zip(verified_words, candidate_word_ids)}\n",
    "\n",
    "    return candidate_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T00:50:56.539521Z",
     "start_time": "2023-11-30T00:50:56.493875Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_probabilties(example, gen_prob_func=get_candidate_word_probabilities):\n",
    "    input_text = example[\"Masked\"]\n",
    "    candidates = example[\"Options\"]\n",
    "\n",
    "    word_probabilities = gen_prob_func(input_text, candidates)\n",
    "\n",
    "    sorted_words = sorted(word_probabilities, key=word_probabilities.get, reverse=True)\n",
    "    if len(sorted_words) > 0:\n",
    "        most_probable_word = sorted_words[0]\n",
    "    else:\n",
    "        most_probable_word = None\n",
    "        #print(example[\"Target\"])\n",
    "\n",
    "    return word_probabilities, sorted_words, most_probable_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_test(specific_string=None, specific_index=None, num_eg=None, gen_prob_func=get_candidate_word_probabilities, example = None):\n",
    "    if example != None: example = example\n",
    "    if num_eg != None: example = dataset['train'][indicies[num_eg]]\n",
    "    if specific_string != None: example = mask_word(specific_string, specific_index)\n",
    "    word_probabilities, sorted_words, most_probable_word = generate_probabilties(example, gen_prob_func)\n",
    "    print(\"Length of words:\", len(sorted_words))\n",
    "    for word in sorted_words:\n",
    "        probability = word_probabilities[word]\n",
    "        print(f\"Word: '{word}', Probability: {probability:.10f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Most probable word:\", most_probable_word)\n",
    "    print(\"Target word:\", example[\"Target\"])\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    found = False\n",
    "    for i in range(len(sorted_words)):\n",
    "        if sorted_words[i] == example[\"Target\"]:\n",
    "            print(\"Sucess at probability level:\", i)\n",
    "            found = True\n",
    "            sucess_level = i\n",
    "            break\n",
    "    if not found: print(\"Not found.\")\n",
    "    \n",
    "    pprint(example)\n",
    "    # print(\"Masked:\", example[\"Masked\"])\n",
    "    # print(\"Options:\", example[\"Options\"])\n",
    "    # print(\"Target:\", example[\"Target\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words: 2\n",
      "Word: 'تنجزءن', Probability: 0.0000525901\n",
      "Word: 'بنجران', Probability: 0.0000053743\n",
      "\n",
      "Most probable word: تنجزءن\n",
      "Target word: بنجران\n",
      "------------------------------------------\n",
      "Sucess at probability level: 1\n",
      "{'Masked': 'كبيره للعملا ارخص اسعار سيارات نقل الاثاث [MASK]',\n",
      " 'Options': ['ثنخزءن',\n",
      "             'ثنخزآن',\n",
      "             'ثنخزإن',\n",
      "             'ثنخزأن',\n",
      "             'ثنخزان',\n",
      "             'ثنخرءن',\n",
      "             'ثنخرآن',\n",
      "             'ثنخرإن',\n",
      "             'ثنخرأن',\n",
      "             'ثنخران',\n",
      "             'ثنحزءن',\n",
      "             'ثنحزآن',\n",
      "             'ثنحزإن',\n",
      "             'ثنحزأن',\n",
      "             'ثنحزان',\n",
      "             'ثنحرءن',\n",
      "             'ثنحرآن',\n",
      "             'ثنحرإن',\n",
      "             'ثنحرأن',\n",
      "             'ثنحران',\n",
      "             'ثنجزءن',\n",
      "             'ثنجزآن',\n",
      "             'ثنجزإن',\n",
      "             'ثنجزأن',\n",
      "             'ثنجزان',\n",
      "             'ثنجرءن',\n",
      "             'ثنجرآن',\n",
      "             'ثنجرإن',\n",
      "             'ثنجرأن',\n",
      "             'ثنجران',\n",
      "             'تنخزءن',\n",
      "             'تنخزآن',\n",
      "             'تنخزإن',\n",
      "             'تنخزأن',\n",
      "             'تنخزان',\n",
      "             'تنخرءن',\n",
      "             'تنخرآن',\n",
      "             'تنخرإن',\n",
      "             'تنخرأن',\n",
      "             'تنخران',\n",
      "             'تنحزءن',\n",
      "             'تنحزآن',\n",
      "             'تنحزإن',\n",
      "             'تنحزأن',\n",
      "             'تنحزان',\n",
      "             'تنحرءن',\n",
      "             'تنحرآن',\n",
      "             'تنحرإن',\n",
      "             'تنحرأن',\n",
      "             'تنحران',\n",
      "             'تنجزءن',\n",
      "             'تنجزآن',\n",
      "             'تنجزإن',\n",
      "             'تنجزأن',\n",
      "             'تنجزان',\n",
      "             'تنجرءن',\n",
      "             'تنجرآن',\n",
      "             'تنجرإن',\n",
      "             'تنجرأن',\n",
      "             'تنجران',\n",
      "             'بنخزءن',\n",
      "             'بنخزآن',\n",
      "             'بنخزإن',\n",
      "             'بنخزأن',\n",
      "             'بنخزان',\n",
      "             'بنخرءن',\n",
      "             'بنخرآن',\n",
      "             'بنخرإن',\n",
      "             'بنخرأن',\n",
      "             'بنخران',\n",
      "             'بنحزءن',\n",
      "             'بنحزآن',\n",
      "             'بنحزإن',\n",
      "             'بنحزأن',\n",
      "             'بنحزان',\n",
      "             'بنحرءن',\n",
      "             'بنحرآن',\n",
      "             'بنحرإن',\n",
      "             'بنحرأن',\n",
      "             'بنحران',\n",
      "             'بنجزءن',\n",
      "             'بنجزآن',\n",
      "             'بنجزإن',\n",
      "             'بنجزأن',\n",
      "             'بنجزان',\n",
      "             'بنجرءن',\n",
      "             'بنجرآن',\n",
      "             'بنجرإن',\n",
      "             'بنجرأن',\n",
      "             'بنجران'],\n",
      " 'Target': 'بنجران'}\n",
      "CPU times: user 203 ms, sys: 742 ms, total: 945 ms\n",
      "Wall time: 434 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "single_test('كبيره للعملا ارخص اسعار سيارات نقل الاثاث بنجران', 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hgiuyitcitcicoouyfou\n",
      "40 placments at level 0   \t 40.0 %\n",
      "21 placments at level 1   \t 21.0 %\n",
      "9 placments at level 2   \t 9.0 %\n",
      "5 placments at level 3   \t 5.0 %\n",
      "3 placments at level 4   \t 3.0 %\n",
      "5 placments at level 5   \t 5.0 %\n",
      "3 placments at level 6   \t 3.0 %\n",
      "2 placments at level 7   \t 2.0 %\n",
      "2 placments at level 8   \t 2.0 %\n",
      "1 placments at level 14   \t 1.0 %\n",
      "\n",
      "Number of examples tested: 100\n",
      "Number of examples that yielded a result: 91\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(1000, dtype=int)\n",
    "unknownWords = []\n",
    "example_index = []\n",
    "start = 0\n",
    "end = 101\n",
    "iterationCount = end - start - 1\n",
    "for i in range(start, end):\n",
    "    num_eg = i\n",
    "    #example1 = mask_random_word(\"التحدي الصعب يكمن في فهم التفاصيل الدقيقة لهذه الفكرة المعقدة\")\n",
    "    #example1 = dataset['train'][indicies[num_eg]]\n",
    "    example1 = dataset['train'][num_eg]\n",
    "\n",
    "    word_probabilities, sorted_words, most_probable_word = generate_probabilties(example1)\n",
    "    if most_probable_word == None:\n",
    "        example_index.append(num_eg)\n",
    "        unknownWords.append(example1[\"Target\"])\n",
    "        continue\n",
    "    \n",
    "    for word in sorted_words:\n",
    "        probability = word_probabilities[word]\n",
    "\n",
    "    for j in range(len(sorted_words)):\n",
    "        if sorted_words[j] == example1[\"Target\"]:\n",
    "            count[j] += 1\n",
    "            break\n",
    "for k in range(len(count)):\n",
    "    numberCount = count[k]\n",
    "    if numberCount != 0:\n",
    "        print(numberCount, \"placments at level\", k, \"  \\t\", (numberCount / iterationCount) * 100, \"%\")\n",
    "\n",
    "print()\n",
    "print(\"Number of examples tested:\", iterationCount)\n",
    "print(\"Number of examples that yielded a result:\", np.sum(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9520\n",
      "1698379\n"
     ]
    }
   ],
   "source": [
    "print(len(rootlist))\n",
    "print(len(arDictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown Word, Batch result analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knownUnknownWords: ['الزملا', 'بوشناق']\n",
      "Indecies of knownUnknownWords: [3, 17]\n",
      "stillUnknownWords: ['المزيدكشفت']\n",
      "Indecies of stillUnknownWords: [70]\n",
      "\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "knownUnknownWords = []\n",
    "knownUnknownWordsIndex = []\n",
    "stillUnknownWords = []\n",
    "stillUnknownWordsIndex = []\n",
    "\n",
    "for i in range(len(unknownWords)):\n",
    "    word = unknownWords[i]\n",
    "    if word in arDictionary:\n",
    "        knownUnknownWords.append(word)\n",
    "        knownUnknownWordsIndex.append(example_index[i])\n",
    "    else:\n",
    "        stillUnknownWords.append(word)\n",
    "        stillUnknownWordsIndex.append(example_index[i])\n",
    "\n",
    "print(\"knownUnknownWords:\", knownUnknownWords)\n",
    "print(\"Indecies of knownUnknownWords:\", knownUnknownWordsIndex)\n",
    "print(\"stillUnknownWords:\", stillUnknownWords)\n",
    "print(\"Indecies of stillUnknownWords:\", stillUnknownWordsIndex)\n",
    "\n",
    "print()\n",
    "print(len(knownUnknownWords))\n",
    "print(len(stillUnknownWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بوشناق\n",
      "['بوش', '##ناق']\n",
      "وشناق\n",
      "['وش', '##ناق']\n",
      "شنق\n",
      "['شن', '##ق']\n"
     ]
    }
   ],
   "source": [
    "testunk1 = knownUnknownWords[1]\n",
    "print(testunk1)\n",
    "print(tokenizer.tokenize(testunk1))\n",
    "\n",
    "stem = ArListem.light_stem(testunk1)\n",
    "root = ArListem.get_root()\n",
    "print(stem)\n",
    "print(tokenizer.tokenize(stem))\n",
    "\n",
    "print(root)\n",
    "print(tokenizer.tokenize(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'كبيره للعملا ارخص اسعار سيارات نقل الاثاث بنجران'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a123 = dataset['train'][8]\n",
    "a123c = a123[\"Masked\"].replace(\"[MASK]\", a123[\"Target\"])\n",
    "a123c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بنجران\n",
      "['بنج', '##ران']\n",
      "\n",
      "Sentence: كبيره للعملا ارخص اسعار سيارات نقل الاثاث [بنجران]\n",
      "\n",
      "Sentence: كبيره للعملا ارخص اسعار سيارات نقل الاثاث بنج \n",
      "Word Token: بنج \n",
      "Root word: نجج \n",
      "In rootlist: True \n",
      "-------------------------------------\n",
      "Sentence: كبيره للعملا ارخص اسعار سيارات نقل الاثاث ران \n",
      "Word Token: ##ران \n",
      "Root word: رون \n",
      "In rootlist: True \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "example = dataset[\"train\"][knownUnknownWordsIndex[index]]\n",
    "sentance = example[\"Masked\"].replace(\"MASK\", example[\"Target\"])\n",
    "wordKU = knownUnknownWords[index]\n",
    "wordKUtokens = tokenizer.tokenize(wordKU)\n",
    "\n",
    "\n",
    "print(wordKU)\n",
    "print(wordKUtokens)\n",
    "print()\n",
    "\n",
    "any_true_root, all_true_root, is_roots, roots, real_roots = check_for_root(wordKUtokens)\n",
    "\n",
    "print(\"Sentence:\", sentance)\n",
    "print()\n",
    "\n",
    "collection_of_dicts = []\n",
    "\n",
    "for i in range(len(is_roots)):\n",
    "    is_root = is_roots[i]\n",
    "    \n",
    "    \n",
    "    wordToken = wordKUtokens[i]\n",
    "    root = roots[i]\n",
    "    is_root = is_roots[i]\n",
    "    replace_rootW_wordToken = example[\"Masked\"].replace(\"[MASK]\", wordToken.replace('#', ''))\n",
    "    replace_rootW_mask = example[\"Masked\"].replace(\"[MASK]\", root)\n",
    "    \n",
    "    if root == \"\":\n",
    "        collection_of_dicts.append({\"Masked\": example[\"Masked\"], \"Options\": allVariation(parse_dotless_text(wordToken.replace('#', ''))), \"Target\": wordToken.replace('#', '')})\n",
    "    else: \n",
    "        collection_of_dicts.append({\"Masked\": example[\"Masked\"], \"Options\": allVariation(parse_dotless_text(root)), \"Target\": root})\n",
    "\n",
    "    \n",
    "    print(\"Sentence:\", replace_rootW_wordToken, \"\\nWord Token:\", wordToken, \"\\nRoot word:\", root, \"\\nIn rootlist:\", is_root,\n",
    "          \"\\n-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 327 µs, sys: 31 µs, total: 358 µs\n",
      "Wall time: 360 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'مطرق'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lemmer.lemmatize(\"المطرقة\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 µs, sys: 1 µs, total: 53 µs\n",
      "Wall time: 55.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'مطرق'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ArListem.light_stem(\"المطرقة\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(collection_of_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test(example = collection_of_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'بعد_المطر_بابى_السمس_وبرهر_الرهور'"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_dotless_text(\"بعد المطر تأتي الشمس وتزهر الزهور\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tkinter Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T01:01:03.125518Z",
     "start_time": "2023-11-30T00:59:36.711411Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "second_probable_word = \"\"\n",
    "cursor_position = 0\n",
    "\n",
    "def function123(input_text, cursor_position):\n",
    "    # Get the text up to the cursor position\n",
    "    text_up_to_cursor = input_text[:cursor_position]\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text_up_to_cursor.split()\n",
    "\n",
    "    # The word number that the user is currently editing is the number of words\n",
    "    word_number = len(words) - 1\n",
    "    current_word = words[word_number]\n",
    "\n",
    "    print(\"current_word:\", current_word)\n",
    "    print(\"Cursor position:\", cursor_position)\n",
    "    print(\"Word Number:\", word_number)\n",
    "    print(\"input_text:\", input_text)\n",
    "\n",
    "    Fexample = mask_word(input_text, word_number)\n",
    "    Fword_probabilities, Fsorted_words, Fmost_probable_word = generate_probabilties(Fexample)\n",
    "    Msentence = Fexample[\"Masked\"]\n",
    "\n",
    "    if Fmost_probable_word == None:\n",
    "        fill = words[word_number]\n",
    "    else:\n",
    "        fill = Fmost_probable_word\n",
    "        if len(Fsorted_words) > 1:\n",
    "            firstWord = Fsorted_words[1]\n",
    "        else:\n",
    "            firstWord = \"\"\n",
    "        if len(Fsorted_words) > 2:\n",
    "            secondWord = Fsorted_words[2]\n",
    "        else:\n",
    "            secondWord = \"\"\n",
    "\n",
    "    \n",
    "    sentence = Msentence.replace(\"[MASK]\", fill)\n",
    "\n",
    "    print(\"example:\", Fexample)\n",
    "    print(\"most_probable_word:\", Fmost_probable_word)\n",
    "    print(\"sorted_words:\", Fsorted_words)\n",
    "    print(\"fill:\", fill)\n",
    "    print(\"sentence:\", sentence)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return sentence, firstWord, secondWord\n",
    "\n",
    "def insert_text():\n",
    "    text_to_insert = insert_button.cget(\"text\")  \n",
    "    print(text_to_insert)\n",
    "    current_text = entry.get()\n",
    "    words = current_text.split()\n",
    "    words = words[:-1]\n",
    "    new_text = ' '.join(words) + \" \" + text_to_insert\n",
    "    print(new_text)\n",
    "    entry.delete(0, tk.END)\n",
    "    entry.insert(0, new_text)\n",
    "\n",
    "def insert_text2():\n",
    "    text_to_insert = insert_button2.cget(\"text\")  \n",
    "    print(text_to_insert)\n",
    "    current_text = entry.get()\n",
    "    words = current_text.split()\n",
    "    words = words[:-1]\n",
    "    new_text = ' '.join(words) + \" \" + text_to_insert\n",
    "    print(new_text)\n",
    "    entry.delete(0, tk.END)\n",
    "    entry.insert(0, new_text)\n",
    "    \n",
    "def update_input(event):\n",
    "    # Ignore non-character key presses\n",
    "    if event.keysym == \"BackSpace\":\n",
    "        pass  # Process backspace here if needed\n",
    "    elif event.char == \"\" or event.keysym == \"space\":\n",
    "        return\n",
    "\n",
    "    # Get the current text input from the Entry widget\n",
    "    input_text = entry.get()\n",
    "\n",
    "    # Call the function123() with the current input and get the processed string\n",
    "    cursor_position = entry.index(tk.INSERT)\n",
    "\n",
    "    processed_text, wordOne, wordTwo = function123(input_text, cursor_position)\n",
    "    print(wordOne, wordTwo)\n",
    "    insert_button.config(text=wordOne)\n",
    "    insert_button2.config(text=wordTwo)\n",
    "\n",
    "    # Replace the text in the Entry widget with the processed string\n",
    "    entry.delete(0, tk.END)\n",
    "    entry.insert(0, processed_text)\n",
    "    #entry.icursor(cursor_position)\n",
    "\n",
    "\n",
    "\n",
    "# Create the main window\n",
    "main = tk.Tk()\n",
    "main.geometry(\"800x600\")\n",
    "# Create the Entry widget\n",
    "entry_font = (\"Helvetica\", 50)  # Adjust the font family and size as needed\n",
    "entry = tk.Entry(main, font=entry_font)\n",
    "\n",
    "# Adjust entry box size with the width option\n",
    "entry_width = 60  # Adjust the width as needed\n",
    "entry.config(width=entry_width)\n",
    "\n",
    "# Bind the <Key> event to the update_input function\n",
    "\n",
    "entry.bind(\"<KeyRelease>\", update_input)\n",
    "\n",
    "# Pack the Entry widget to display it in the window\n",
    "entry.pack()\n",
    "\n",
    "button_font = (\"Helvetica\", 20)  # Adjust the font family and size as needed\n",
    "\n",
    "button_label = \"\"\n",
    "\n",
    "insert_button = tk.Button(main, text=button_label, command=insert_text, font=button_font)\n",
    "insert_button.config(width=25)\n",
    "insert_button.pack()\n",
    "\n",
    "button_label = \"\"\n",
    "insert_button2 = tk.Button(main, text=button_label, command=insert_text2, font=button_font)\n",
    "insert_button2.config(width=25)\n",
    "insert_button2.pack()\n",
    "\n",
    "# Start the main event loop\n",
    "main.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
