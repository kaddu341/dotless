{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of arabic letters from dotted to dotless. (credit: @kaddu341)\n",
    "letter_dict = {\n",
    "    \"ا\": \"ا\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ب\": \"ب\",\n",
    "    \"ت\": \"ب\",\n",
    "    \"ث\": \"ب\",\n",
    "    \"ج\": \"ح\",\n",
    "    \"ح\": \"ح\",\n",
    "    \"خ\": \"ح\",\n",
    "    \"د\": \"د\",\n",
    "    \"ذ\": \"د\",\n",
    "    \"ر\": \"ر\",\n",
    "    \"ز\": \"ر\",\n",
    "    \"س\": \"س\",\n",
    "    \"ش\": \"س\",\n",
    "    \"ص\": \"ص\",\n",
    "    \"ض\": \"ص\",\n",
    "    \"ط\": \"ط\",\n",
    "    \"ظ\": \"ط\",\n",
    "    \"ع\": \"ع\",\n",
    "    \"غ\": \"ع\",\n",
    "    \"ف\": \"ف\",\n",
    "    \"ق\": \"ف\",\n",
    "    \"ك\": \"ك\",\n",
    "    \"ل\": \"ل\",\n",
    "    \"م\": \"م\",\n",
    "    \"ن\": \"ن\",\n",
    "    \"و\": \"و\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ه\": \"ه\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ي\": \"ى\",\n",
    "    \"ى\": \"ى\",\n",
    "    \"ئ\": \"ى\",\n",
    "    \" \": \" \",\n",
    "}\n",
    "letter_set = set(letter_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces multiple spaces, or whatever char designated by the perameter into one char. (helper function)\n",
    "def replace(string, char):\n",
    "    while char+char in string:\n",
    "        string = string.replace(char+char, char)\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential function, parses arabic dotted text into dotless text. \n",
    "# Returns a dict with keys \"clean\" and \"dotless\". \n",
    "# This format is required for the datasets.map() function.\n",
    "\n",
    "#unused, see below functions, also this is not batched\n",
    "def parse_text(text):\n",
    "    \n",
    "  text = text[\"text\"].strip(\" \")\n",
    "  \n",
    "  clean_chars = [char for char in text if char in letter_set]\n",
    "  clean_string = ''.join(clean_chars)\n",
    "\n",
    "  dotless_chars = [letter_dict[char] for char in clean_chars]\n",
    "  dotless_string = ''.join(dotless_chars)\n",
    "      \n",
    "  clean_string = replace(clean_string, ' ')\n",
    "  dotless_string = replace(dotless_string, '_')\n",
    "  \n",
    "  return {\"clean\": clean_string, \"dotless\": dotless_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_text(example):\n",
    "  \n",
    "  clean = []\n",
    "  \n",
    "  for text in example[\"text\"]:\n",
    "    text = text.strip(\" \")\n",
    "    \n",
    "    clean_chars = [char for char in text if char in letter_set]\n",
    "    clean_string = ''.join(clean_chars)\n",
    "    \n",
    "    clean_string = replace(clean_string, ' ')\n",
    "    clean.append(clean_string)\n",
    "  \n",
    "  return {\"clean\": clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dotless_text(example):\n",
    "  dotless = []\n",
    "  for text in example[\"clean\"]:\n",
    "    dotless_chars = [letter_dict[char] for char in text]\n",
    "    dotless_string = ''.join(dotless_chars)\n",
    "        \n",
    "    dotless.append(dotless_string)\n",
    "  \n",
    "  return {\"dotless\": dotless}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(text):\n",
    "    dotless_chars = [letter_dict[char] for char in text]\n",
    "    dotless_string = ''.join(dotless_chars)\n",
    "        \n",
    "    return(dotless_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dotless_text_mixed(example):\n",
    "    dotless = []\n",
    "    for text in example[\"clean\"]:\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            # Randomly decide whether to process the word or not\n",
    "            if random.random() < 0.3:  # chance\n",
    "                # Process the word (Replace this with your own processing logic)\n",
    "                processed_word = process_function(word)  # Use your processing function here\n",
    "                processed_words.append(processed_word)\n",
    "            else:\n",
    "                # Keep the word as it is\n",
    "                processed_words.append(word)\n",
    "\n",
    "        processed_text = ' '.join(processed_words)\n",
    "        dotless.append(processed_text)\n",
    "  \n",
    "    return {\"dotless\": dotless}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['ولا تنسى التسجيل معنا لتستفيد      بكل جديد']} \n",
      "\n",
      "{'clean': ['ولا تنسى التسجيل معنا لتستفيد بكل جديد']} \n",
      "\n",
      "{'dotless': ['ولا تنسى البسحىل معنا لبسبفىد بكل جديد']} \n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "text = {\"text\": [\"ولا تنسى التسجيل معنا لتستفيد      بكل جديد\"]}\n",
    "print(text, \"\\n\")\n",
    "\n",
    "text = parse_clean_text(text)\n",
    "print(text, \"\\n\")\n",
    "\n",
    "text = parse_dotless_text_mixed(text)\n",
    "print(text, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the datasets.filter() function to filter out examples/rows that have examples of less than 20 chars.\n",
    "#def filterEmpty(x):\n",
    "#    return not len(x['clean']) < 20\n",
    "\n",
    "#Not used, lambda function used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, saving, and trimming Oscar (unshuffled_deduplicated_ar) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the oscar unshuffled_deduplicated_ar dataset from huggingface \n",
    "datasetLoad = load_dataset(\"oscar\", \"unshuffled_deduplicated_ar\") #Oscar Arabic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d15586a3e204c1da9f96a847efcb2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/67 shards):   0%|          | 0/9006977 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save the dataset to \"ar-arrow-datasets/\" Apache Arrow datafolder\n",
    "datasetLoad.save_to_disk('ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3b37decb6d48d19048f11bff2d8d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load from file\n",
    "full_dataset = load_from_disk('/Users/ammar/Developer/git-repos/dotless/data/ar-arrow-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text'],\n",
      "        num_rows: 9006977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying maps, filters, and splits\n",
    "This process of applying maps and filters is very slow especially for big datasets. \n",
    "\n",
    "It is possible to implement a batched approach similar ot batched tokenization that would speed it up greatly. \n",
    "\n",
    "In the .map() add perameter batched=True. However the function called needs to be compatible with this approach.\n",
    "\n",
    "I will implement this soon.\n",
    "\n",
    "Generally this is not the way to handle big data. However this is fine for the AR-dotless-small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the index of the #th space.\n",
    "def num_word(text, num):\n",
    "    space_count = 0\n",
    "    index = -1\n",
    "    for i in range(num):\n",
    "        index = text.find(' ', index + 1)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "exampleToWordsLen = [] # just to keep track of how many chunks an old  made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(exampleToWordsLen[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunks examples into smaller examples, at @words_per_chuck words each\n",
    "def chunk_examples(examples, words_per_chunk=8):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"clean\"]:\n",
    "        words = sentence.split()\n",
    "        num_words = len(words)\n",
    "        exampleToWordsLen.append(math.ceil(num_words/words_per_chunk)) # just to keep track of how many chunks an old example made\n",
    "        start = 0\n",
    "        while start < num_words:\n",
    "            end = min(start + words_per_chunk, num_words)\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            start = end\n",
    "    \n",
    "    return {\"clean\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100000\n",
      "}) \n",
      "\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 100000\n",
      "}) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12067d3517ac4d0d8c483665e1d484bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean'],\n",
      "    num_rows: 100000\n",
      "}) \n",
      "\n",
      "478 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c795996cf8d94355836a62f13f9e924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean'],\n",
      "    num_rows: 4482409\n",
      "}) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0b6236696442b389500f3b303a37ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4482409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 4482409\n",
      "}) \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316eee914538493fad8ec1a025ce278c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4482409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['clean', 'dotless'],\n",
      "    num_rows: 4446331\n",
      "}) \n"
     ]
    }
   ],
   "source": [
    "dataset = full_dataset['train'].select(range(100000)) #Trim to number of examples\n",
    "dataset = dataset.remove_columns([\"id\"])\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['text']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')\n",
    "        \n",
    "dataset = dataset.map(parse_clean_text, batched=True, remove_columns=\"text\") #Apply the function (clean text)\n",
    "set_state_1 = dataset[\"clean\"][0]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 40:\n",
    "        a += 1\n",
    "print(a, '\\n')\n",
    "\n",
    "dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names) # applies teh chuncking\n",
    "set_state_2 = dataset[\"clean\"][0:exampleToWordsLen[0]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.map(parse_dotless_text_mixed, batched=True) #Apply the function (convert dotted to dotless)\n",
    "set_state_3 = dataset[\"dotless\"][0:exampleToWordsLen[0]]\n",
    "print(dataset, '\\n')\n",
    "\n",
    "dataset = dataset.filter(lambda x: not len(x['clean']) < 20) #Apply the filter to remove useless empty examples.\n",
    "print(dataset, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل معنا لتستفيد بكل جديدأهلا وسهلا بك زائرنا الكريم أنت لم تقم بتسجيل الدخول بعد يشرفنا أن تقوم بالدخول أو التسجيل إذا رغبت بالمشاركة في المنتدىنرحب بكل الزائرين ونتمى لكم قضا وقت ممتع معنا يملأه الحب والود والاستفادة المتبادلة بيننا علميا وعمليا يسعدنا تسجيلكم معنا ومشاركتنا وشعارنا دوما نحب الخير لكل الناس مهما اختلفت الألوان والديانات والأجناس لي أربع شقيقات أنا أكثرهن غنى لكن لا أدري لماذا يأتي أقاربي لزيارة أخواتي بكثرةوحينما يأتي موعد زيارتي لا يأتي سوى القليلفهم يزورون أخواتي الأربع كل يومأماأنا أكثر أخواتي عطا لمن يأتيني لا أتهم أخواتي بالتقصير أبدا ولكن الكل يعرف أني أكثرهن عطاكثيرون ينصحون أقاربي بأن يأتوني فلدي خير كثير وأعطي بكرم من يأتيني ومع ذلك يبتعدون عني فلا حياة لمن تنادي اختر منتدىقناة عجباوي التلفزيونية التسويقية البرامج التلفزيونية لقناة عجباوي التسويقية تعرف على عجباوي سيرة ذاتية سابقة أعمال أرشيف تصميمات عجباوي إعلانات الشركات والمحلات أغلفة كتب تنسيقات المتن الداخلي للكتب الدفاتر والمستندات اللافتات الإعلانية السريلات والأختام والأكلشيهات برشور فلاير ستيكر منتجات كتالوجات بوستر كروت شخصية شركات أفراح ومناسبات إمساكيات رمضان عجباوي مول سوق تجاري وخدمات متكاملة بيت التحف والهدايا تحف وأنتيكات حفر ليزر إعلانات وظائف وظائف مطلوبة وظائف معروضة إعلانات مجانية خدمات المكاتب الإدارية خدمات الشركات الصناعية خدمات زراعية خدمات احداث ومناسبات خدمات شخصية خدمات منزلية بنا وعقارات مبيعات تسويق مالية واقتصادية إعلام ونشر خدمات تعليمية رحلات وترفيه رياضة سيارات ومركبات صحة مواد غذائية عجباوي مصمم جرافيك مجموعة برامج الجرافيك الفوتوشوب الكوريل درو اليستريتور الدريم وييفر الفلاش والسويش أدوات المصمم مجموعة برامج الأوفيس تعليم الأكسيس تعليم الوورد تعليم الإكسل أسرار وفنيات عالم الطباعة والنشر تصاميم الأعضا منتدى البرامج والعاب الكمبيوتروالمحمول عجباوي مهندس صيانة الأسئلة والاستفسارات منتدى برامج الكمبيوتر منتدى برامج الحمايه للكمبيوتر منتدى الخلفيات والثيمات والايقونات لجهاز الكمبيوتر منتدى شرح البرامج وتبادل خبرات الكمبيوتر منتدى برامج الملتيميديا والفيديو منتدى العاب الكمبيوتر منتدى الستالايت والقنوات الفضائية والترددات للمحطات منتدى الجوال برامج جوال منتدى رسائل الجوال اغاني وكليبات ونغمات للجوال منتدى التنمية البشرية وتطوير الذات منتدى الأستاذ الدكتور هشام الشريف التنمية البشرية وتطوير مهارات الذات الموارد البشرية والتنمية الإدارية منتدى الحسابات المالية المنتدى التعليمي معلم الأجيال الأستاذ يحيى خضر رسالة إلى رجال التعليم حوار مع طلاب العلم درس خصوصي مرحلة رياض الأطفال المرحلة الإبتدائية الصف الأول الإبتدائي الصف الثاني الإبتدائي الصف الثالث الإبتدائي الصف الرابع الإبتدائي الصف الخامس الإبتدائي الصف السادس الإبتدائي المرحلة الإعدادية الصف الأول الإعدادي الصف الثاني الإعدادي الصف الثالث الإعدادي المرحلة الثانوية الصف الأول الثانوي الصف الثاني الثانوي الصف الثالث الثانوي مرحلة التعليم الفني العالي مرحلة التعليم الجامعي الأعمال الأدبية للأستاذ يحيى خضر عجباوي والأسرة والمجتمع والناس فن الحياة الزوجية السعيدة العلاقة الزوجية الحميمة الأسرة والطفل منتدى المرأة الحامل والطفل منتدى صحة و مستلزمات الطفل منتدى اناقة الرجل منتدى المرأة والزينة منتدى الاشغال والاعمال اليدوية منتدى الخياطة والتطريز منتدى المطبخ والمأكولات المنتدى الطبي أريد حلا منتدى التهنئة والاهداات المنتدى الأدبي عجباوي فنان المقامات الموسيقية العربية منتدى دروس نظرية في الموسيقى الايقاع النغم النوتة الموسيقية الشاعر الكبير أحمد فؤاد نجم منتدى الشاعر الموهوب وحيد الدهشان الشاعرة المبدعة إيمان بكري موضوعات أدبية عامة منتدى الغنا الراقي زمن الفن الجميل إبداعات الأعضا عمالقة الفن والمقطوعات والسماعيات الموسيقيه والمعزوفات النادرة المنتدى العام للقصص والروايات الكتب منتدى البحوث الأكاديمية منتدى المسرح المنتدى الإسلامي القرآن الكريم نور البيان في تعليم القراة بالقرآن الأحاديث النبوية الشريفة ركن شهر رمضان المبارك منتدى الصوتيات والاناشيد الاسلامية منتدى الفيديو الإسلامي أخبار العالم الإسلامي موضوعات إسلامية عامة مشروع علم ينتفع به وقف لله تعالى دروس في العقيدة تربية الأبنا منتدى الموضوعات العامة منتدى علم النفس علم النفس دراسات علم النفس كتب علم النفس ابحاث علم النفس منتدى تفسير الاحلام منتدى علوم الفلك والابراج منتدى السياحة والسفر منتدى عجباوي للحوار الجاد منتدى الحوادث جرائم قتل فضائح أخبار مشاكل وقضايا موضوعات علمية منتدى الشخصيات التاريخية عجباوي الرياضي منتدى الملتيميديا عجباوي تيوب منتدى الصور منتدى الفنون الجميلة وابداعات الاعضا منتدى صيد الكاميرا منتدى الكاريكاتير منتدى الأفلام والمسلسلات والمسرحيات عجباوي قاعد فاضي نكت طرائف ضحك فرفشة منتدى الضحك والفرفشة منتدى الالغاز والمسابقات منتدى الالعاب الارشيف والمواضيع القديمة منتدى مجاني منتدى مجاني للدعم و المساعدة إتصل بنا التبليغ عن محتوى مخالف انشئ مدونة مجانيا عند التصوير يقوم الالة بمسح الاصول مرة واحده وتخزينها بالذاكره ثم تتم عملية التصوير لاي عدد من النسخ منتدى مجاني منتدى مجاني للدعم و المساعدة إتصل بنا التبليغ عن محتوى مخالف الحصول على مدونة \n",
      "\n",
      "\n",
      "['مرحبا بك عزيز الزائر نتمنى لك أوقاتا سعيدة', 'معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل', 'معنا لتستفيد بكل جديدأهلا وسهلا بك زائرنا الكريم', 'أنت لم تقم بتسجيل الدخول بعد يشرفنا أن', 'تقوم بالدخول أو التسجيل إذا رغبت بالمشاركة في', 'المنتدىنرحب بكل الزائرين ونتمى لكم قضا وقت ممتع', 'معنا يملأه الحب والود والاستفادة المتبادلة بيننا علميا', 'وعمليا يسعدنا تسجيلكم معنا ومشاركتنا وشعارنا دوما نحب', 'الخير لكل الناس مهما اختلفت الألوان والديانات والأجناس', 'لي أربع شقيقات أنا أكثرهن غنى لكن لا', 'أدري لماذا يأتي أقاربي لزيارة أخواتي بكثرةوحينما يأتي', 'موعد زيارتي لا يأتي سوى القليلفهم يزورون أخواتي', 'الأربع كل يومأماأنا أكثر أخواتي عطا لمن يأتيني', 'لا أتهم أخواتي بالتقصير أبدا ولكن الكل يعرف', 'أني أكثرهن عطاكثيرون ينصحون أقاربي بأن يأتوني فلدي', 'خير كثير وأعطي بكرم من يأتيني ومع ذلك', 'يبتعدون عني فلا حياة لمن تنادي اختر منتدىقناة', 'عجباوي التلفزيونية التسويقية البرامج التلفزيونية لقناة عجباوي التسويقية', 'تعرف على عجباوي سيرة ذاتية سابقة أعمال أرشيف', 'تصميمات عجباوي إعلانات الشركات والمحلات أغلفة كتب تنسيقات', 'المتن الداخلي للكتب الدفاتر والمستندات اللافتات الإعلانية السريلات', 'والأختام والأكلشيهات برشور فلاير ستيكر منتجات كتالوجات بوستر', 'كروت شخصية شركات أفراح ومناسبات إمساكيات رمضان عجباوي', 'مول سوق تجاري وخدمات متكاملة بيت التحف والهدايا', 'تحف وأنتيكات حفر ليزر إعلانات وظائف وظائف مطلوبة', 'وظائف معروضة إعلانات مجانية خدمات المكاتب الإدارية خدمات', 'الشركات الصناعية خدمات زراعية خدمات احداث ومناسبات خدمات', 'شخصية خدمات منزلية بنا وعقارات مبيعات تسويق مالية', 'واقتصادية إعلام ونشر خدمات تعليمية رحلات وترفيه رياضة', 'سيارات ومركبات صحة مواد غذائية عجباوي مصمم جرافيك', 'مجموعة برامج الجرافيك الفوتوشوب الكوريل درو اليستريتور الدريم', 'وييفر الفلاش والسويش أدوات المصمم مجموعة برامج الأوفيس', 'تعليم الأكسيس تعليم الوورد تعليم الإكسل أسرار وفنيات', 'عالم الطباعة والنشر تصاميم الأعضا منتدى البرامج والعاب', 'الكمبيوتروالمحمول عجباوي مهندس صيانة الأسئلة والاستفسارات منتدى برامج', 'الكمبيوتر منتدى برامج الحمايه للكمبيوتر منتدى الخلفيات والثيمات', 'والايقونات لجهاز الكمبيوتر منتدى شرح البرامج وتبادل خبرات', 'الكمبيوتر منتدى برامج الملتيميديا والفيديو منتدى العاب الكمبيوتر', 'منتدى الستالايت والقنوات الفضائية والترددات للمحطات منتدى الجوال', 'برامج جوال منتدى رسائل الجوال اغاني وكليبات ونغمات', 'للجوال منتدى التنمية البشرية وتطوير الذات منتدى الأستاذ', 'الدكتور هشام الشريف التنمية البشرية وتطوير مهارات الذات', 'الموارد البشرية والتنمية الإدارية منتدى الحسابات المالية المنتدى', 'التعليمي معلم الأجيال الأستاذ يحيى خضر رسالة إلى', 'رجال التعليم حوار مع طلاب العلم درس خصوصي', 'مرحلة رياض الأطفال المرحلة الإبتدائية الصف الأول الإبتدائي', 'الصف الثاني الإبتدائي الصف الثالث الإبتدائي الصف الرابع', 'الإبتدائي الصف الخامس الإبتدائي الصف السادس الإبتدائي المرحلة', 'الإعدادية الصف الأول الإعدادي الصف الثاني الإعدادي الصف', 'الثالث الإعدادي المرحلة الثانوية الصف الأول الثانوي الصف', 'الثاني الثانوي الصف الثالث الثانوي مرحلة التعليم الفني', 'العالي مرحلة التعليم الجامعي الأعمال الأدبية للأستاذ يحيى', 'خضر عجباوي والأسرة والمجتمع والناس فن الحياة الزوجية', 'السعيدة العلاقة الزوجية الحميمة الأسرة والطفل منتدى المرأة', 'الحامل والطفل منتدى صحة و مستلزمات الطفل منتدى', 'اناقة الرجل منتدى المرأة والزينة منتدى الاشغال والاعمال', 'اليدوية منتدى الخياطة والتطريز منتدى المطبخ والمأكولات المنتدى', 'الطبي أريد حلا منتدى التهنئة والاهداات المنتدى الأدبي', 'عجباوي فنان المقامات الموسيقية العربية منتدى دروس نظرية', 'في الموسيقى الايقاع النغم النوتة الموسيقية الشاعر الكبير', 'أحمد فؤاد نجم منتدى الشاعر الموهوب وحيد الدهشان', 'الشاعرة المبدعة إيمان بكري موضوعات أدبية عامة منتدى', 'الغنا الراقي زمن الفن الجميل إبداعات الأعضا عمالقة', 'الفن والمقطوعات والسماعيات الموسيقيه والمعزوفات النادرة المنتدى العام', 'للقصص والروايات الكتب منتدى البحوث الأكاديمية منتدى المسرح', 'المنتدى الإسلامي القرآن الكريم نور البيان في تعليم', 'القراة بالقرآن الأحاديث النبوية الشريفة ركن شهر رمضان', 'المبارك منتدى الصوتيات والاناشيد الاسلامية منتدى الفيديو الإسلامي', 'أخبار العالم الإسلامي موضوعات إسلامية عامة مشروع علم', 'ينتفع به وقف لله تعالى دروس في العقيدة', 'تربية الأبنا منتدى الموضوعات العامة منتدى علم النفس', 'علم النفس دراسات علم النفس كتب علم النفس', 'ابحاث علم النفس منتدى تفسير الاحلام منتدى علوم', 'الفلك والابراج منتدى السياحة والسفر منتدى عجباوي للحوار', 'الجاد منتدى الحوادث جرائم قتل فضائح أخبار مشاكل', 'وقضايا موضوعات علمية منتدى الشخصيات التاريخية عجباوي الرياضي', 'منتدى الملتيميديا عجباوي تيوب منتدى الصور منتدى الفنون', 'الجميلة وابداعات الاعضا منتدى صيد الكاميرا منتدى الكاريكاتير', 'منتدى الأفلام والمسلسلات والمسرحيات عجباوي قاعد فاضي نكت', 'طرائف ضحك فرفشة منتدى الضحك والفرفشة منتدى الالغاز', 'والمسابقات منتدى الالعاب الارشيف والمواضيع القديمة منتدى مجاني', 'منتدى مجاني للدعم و المساعدة إتصل بنا التبليغ', 'عن محتوى مخالف انشئ مدونة مجانيا عند التصوير', 'يقوم الالة بمسح الاصول مرة واحده وتخزينها بالذاكره', 'ثم تتم عملية التصوير لاي عدد من النسخ', 'منتدى مجاني منتدى مجاني للدعم و المساعدة إتصل', 'بنا التبليغ عن محتوى مخالف الحصول على مدونة'] \n",
      "\n",
      "\n",
      "['مرحبا بك عرىر الزائر نتمنى لك أوقاتا سعيدة', 'معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل', 'معنا لتستفيد بكل جديدأهلا وسهلا بك زائرنا الكريم', 'انب لم تقم ببسحىل الدخول بعد ىسرفنا ان', 'بفوم بالدخول أو التسجيل إذا رغبت بالمشاركة فى', 'المنبدىنرحب بكل الراىرىن ونبمى لكم قضا وفب ممتع', 'معنا ىملاه الحب والود والاستفادة المتبادلة بيننا علميا', 'وعملىا يسعدنا تسجيلكم معنا ومساركبنا وشعارنا دوما نحب', 'الخير لكل الناس مهما اختلفت الألوان والديانات والأجناس', 'لي أربع سفىفاب أنا أكثرهن عنى لكن لا', 'أدري لمادا يأتي أقاربي لرىاره احوابى بكثرةوحينما يأتي', 'موعد رىاربى لا ىابى سوى القليلفهم يزورون أخواتي', 'الأربع كل ىوماماانا أكثر أخواتي عطا لمن يأتيني', 'لا أتهم أخواتي بالبفصىر أبدا ولكن الكل ىعرف', 'أني اكبرهن عطاكثيرون ينصحون أقاربي بأن يأتوني فلدى', 'خير كبىر وأعطي بكرم من يأتيني ومع ذلك', 'يبتعدون عني فلا حىاه لمن تنادي احبر منتدىقناة', 'عجباوي التلفزيونية البسوىفىه البرامح البلفرىونىه لفناه عجباوي التسويقية', 'تعرف على عحباوى سيرة دابىه سابقة أعمال أرشيف', 'تصميمات عجباوي إعلانات السركاب والمحلات أغلفة كتب بنسىفاب', 'المتن الداحلى للكبب الدفاتر والمستندات اللافتات الإعلانية السريلات', 'والأختام والأكلشيهات برشور فلاير ستيكر منتجات كبالوحاب بوسبر', 'كروب شخصية سركاب افراح ومناسبات إمساكيات رمصان عحباوى', 'مول سوق تجاري وخدمات متكاملة بيت البحف والهداىا', 'تحف وانبىكاب حفر ليزر إعلانات وظائف وظائف مطلوبه', 'وظائف معروضة إعلانات محانىه خدمات المكاتب الإدارية حدماب', 'الشركات الصناعية خدمات رراعىه خدمات احداث ومناسبات خدمات', 'سحصىه خدمات منرلىه بنا وعقارات مبيعات بسوىف مالىه', 'واقتصادية اعلام ونسر خدمات تعليمية رحلات وترفيه رياضة', 'سيارات ومركبات صحة مواد غذائية عجباوي مصمم جرافيك', 'مجموعة برامح الجرافيك الفوتوشوب الكوريل درو اليستريتور الدريم', 'وييفر الفلاش والسويش أدوات المصمم مجموعة برامح الأوفيس', 'تعليم الأكسيس تعليم الوورد تعليم الإكسل اسرار وفنىاب', 'عالم الطباعة والنسر تصاميم الاعصا منتدى البرامج والعاب', 'الكمبيوتروالمحمول عجباوي مهندس صىانه الأسئلة والاستفسارات منبدى برامج', 'الكمبيوتر منبدى برامح الحمايه للكمبىوبر منتدى الخلفيات والبىماب', 'والاىفوناب لجهاز الكمبيوتر منبدى شرح البرامج وتبادل خبرات', 'الكمبيوتر منتدى برامج الملتيميديا والفيديو منتدى العاب الكمبيوتر', 'منبدى الستالايت والقنوات الفصاىىه والبردداب للمحطات منتدى الجوال', 'برامج جوال منتدى رسائل الحوال اعانى وكليبات ونغمات', 'للحوال منتدى التنمية البشرية وبطوىر الداب منتدى الأستاذ', 'الدكتور هسام الشريف التنمية البشرية وتطوير مهارات الداب', 'الموارد البشرية والتنمية الإدارية منبدى الحسابات المالىه المنتدى', 'التعليمي معلم الاحىال الأستاذ يحيى خضر رسالة الى', 'رجال التعليم حوار مع طلاب العلم درس خصوصي', 'مرحله رىاص الاطفال المرحله الاببداىىه الصف الاول الإبتدائي', 'الصف الثاني الإبتدائي الصف الثالث الاببداىى الصف الرابع', 'الإبتدائي الصف الخامس الإبتدائي الصف السادس الاببداىى المرحلة', 'الاعدادىه الصف الأول الإعدادي الصف الثاني الاعدادى الصف', 'الثالث الاعدادى المرحلة الثانوية الصف الأول الثانوي الصف', 'الثاني البانوى الصف الثالث البانوى مرحله التعليم الفنى', 'العالي مرحلة البعلىم الجامعي الأعمال الأدبية للأستاذ يحيى', 'حصر عجباوي والأسرة والمجتمع والناس فن الحىاه الروحىه', 'السعيدة العلاقة الروحىه الحميمة الأسرة والطفل منبدى المرأة', 'الحامل والطفل منتدى صحة و مستلزمات الطفل منتدى', 'انافه الرجل منتدى المرأة والرىنه منبدى الاسعال والاعمال', 'اليدوية منتدى الخياطة والتطريز منبدى المطبخ والمأكولات المنبدى', 'الطبي أريد حلا منبدى البهنىه والاهداات المنبدى الأدبي', 'عجباوي فنان المقامات الموسيقية العربية منتدى دروس نظرية', 'في الموسيقى الايقاع النغم النوتة الموسيقية الشاعر الكبىر', 'أحمد فواد نجم منبدى الشاعر الموهوب وحىد الدهشان', 'الشاعرة المبدعة اىمان بكرى موضوعات أدبية عامة منتدى', 'الغنا الرافى زمن الفن الجميل ابداعاب الأعضا عمالفه', 'الفن والمقطوعات والسماعىاب الموسيقيه والمعزوفات النادرة المنتدى العام', 'للقصص والروايات الكتب منبدى البحوث الأكاديمية منبدى المسرح', 'المنتدى الإسلامي الفران الكرىم نور البيان فى تعليم', 'القراة بالقرآن الأحاديث النبوية الشريفة ركن شهر رمصان', 'المبارك منبدى الصوبىاب والاناشيد الاسلامىه منتدى الفيديو الإسلامي', 'أخبار العالم الإسلامي موضوعات إسلامية عامه مشروع علم', 'ينتفع به وفف لله بعالى دروس في العقيدة', 'بربىه الأبنا منتدى الموضوعات العامه منتدى علم النفس', 'علم النفس دراسات علم النفس كبب علم النفس', 'ابحاث علم النفس منتدى بفسىر الاحلام منتدى علوم', 'الفلك والابراج منتدى السياحة والسفر منبدى عجباوي للحوار', 'الجاد منتدى الحوادث حراىم فبل فصاىح أخبار مشاكل', 'وفصاىا موصوعاب علمية منبدى الشخصيات البارىحىه عحباوى الرىاصى', 'منتدى الملبىمىدىا عحباوى تيوب منبدى الصور منتدى الفنون', 'الجميلة وابداعات الاعضا منتدى صيد الكاميرا منبدى الكاريكاتير', 'منبدى الأفلام والمسلسلات والمسرحىاب عجباوي فاعد فاضي نكت', 'طرائف صحك فرفشة منتدى الضحك والفرفشة منتدى الالغاز', 'والمسابقات منتدى الالعاب الارشيف والمواضيع الفدىمه منبدى مجاني', 'منتدى مجاني للدعم و المساعدة ابصل بنا التبليغ', 'عن محتوى محالف انشئ مدونة مجانيا عند التصوير', 'يقوم الالة بمسح الاصول مرة واحده وتخزينها بالداكره', 'ثم تتم عملية التصوير لاى عدد من النسخ', 'منبدى محانى منتدى مجاني للدعم و المساعده إتصل', 'بنا التبليغ عن محبوى مخالف الحصول على مدونة']\n"
     ]
    }
   ],
   "source": [
    "print(set_state_1,\"\\n\\n\")\n",
    "print(set_state_2,\"\\n\\n\")\n",
    "print(set_state_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to see how many examples are under 20 characters. Goal of filterEmpty() to eliminate\n",
    "a = 0\n",
    "for i in dataset['clean']:\n",
    "    if len(i) < 20:\n",
    "        a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أنت لم تقم بتسجيل الدخول بعد يشرفنا أن\n",
      "انب لم تقم ببسحىل الدخول بعد ىسرفنا ان\n"
     ]
    }
   ],
   "source": [
    "print(dataset[3]['clean'])\n",
    "print(dataset[3]['dotless'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['clean', 'dotless'],\n",
       "        num_rows: 4446330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean = dataset.train_test_split(train_size= 0.999999999) #create 80/20 split into train and test datasets\n",
    "del dataset_clean['test'] # no longer using a test set\n",
    "dataset_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and upload to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4c803c25a7431abe8ff51af95b7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/4446330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save the cleaned dataset to \"clean-arrow-datasets/\" Apache Arrow datafolder\n",
    "dataset_clean.save_to_disk(\"AR-dotless-medium-mixed-arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f3047f50d94c9b834344cb7b1857a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7cc67079f745ac9e421d5b4bb121cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2224 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f96bcccc1443ae8646eea2e5b7dd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2224 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dot-ammar/AR-dotless-medium-mixed/commit/222c1196ddaa6dceba6da003a1da2cdcdb69d37d', commit_message='Upload dataset', commit_description='', oid='222c1196ddaa6dceba6da003a1da2cdcdb69d37d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#required token id. Run 'huggingface-cli login'\n",
    "dataset_clean.push_to_hub(\"dot-ammar/AR-dotless-medium-mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
