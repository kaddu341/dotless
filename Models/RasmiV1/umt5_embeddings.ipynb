{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The UMT5 model is really new so looks like they haven't added full Tensorflow support yet so I had to install PyTorch as well\n",
    "#%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt UMT5 model for single-language use.\n",
    "Credit: https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model and tokenizer\n",
    "from transformers import UMT5Model, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model and tokenizer using the umt5-small pretrained model\n",
    "model = UMT5Model.from_pretrained(\"google/umt5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/umt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is supposed to estimate the frequency of different tokens. Not entirely certain how it works though!\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "df_ar = pd.read_csv('ara_wikipedia_2021_1M-sentences.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "df_ar.columns = ['idx', 'text']\n",
    "cnt_ar = Counter()\n",
    "for text in tqdm(df_ar.text):\n",
    "    cnt_ar.update(tokenizer.encode(text))\n",
    "print(len(cnt_ar), len(cnt_ar)/tokenizer.vocab_size)\n",
    "# 53217 0.20763558330081935 (so basically, only 20% of the model vocabulary was used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_ar.most_common(top)) / sum(cnt_ar.values()))\n",
    "# 10000 0.986\n",
    "# 20000 0.998\n",
    "# 30000 0.999\n",
    "# this means that more than 99% of the tokens fall into the top 20K, so we don't really need all 53217 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same for english\n",
    "df_en = pd.read_csv('eng_wikipedia_2016_1M-sentences.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "df_en.columns = ['idx', 'text']\n",
    "cnt_en = Counter()\n",
    "for text in tqdm(df_en.text):\n",
    "    cnt_en.update(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary (adapted from the tutorial) is comprised as follows:\n",
    "- the top 1000 tokens regardless of language (just to be \"safe\")\n",
    "- the top 10,000 tokens for English (cuz in real life they're often mixed, especially in informal documents)\n",
    "- the top 20,000 Arabic tokens\n",
    "- The 300 special tokens that UMT5 uses (aka sentinel ids) - to be added later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "    #get the top 1000 tokens\n",
    "    new_tokens = set(range(1000))\n",
    "\n",
    "    #get the top 10,000 tokens for English\n",
    "    for i, (k, v) in enumerate(cnt_en.most_common(10_000)):\n",
    "        if k not in new_tokens:\n",
    "            new_tokens.add(k)\n",
    "\n",
    "    #get the top 20,000 Arabic tokens\n",
    "    for i, (k, v) in enumerate(cnt_ar.most_common(25_000)):\n",
    "        if len(new_tokens) == 29_700:\n",
    "            print(i, 'Arabic tokens are included')\n",
    "            break\n",
    "        if k not in new_tokens:\n",
    "            new_tokens.add(k)\n",
    "\n",
    "    print(len(new_tokens))\n",
    "    return sorted(new_tokens)\n",
    "\n",
    "kept_ids = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some reason these commands weren't working in VSCode in Jupyter so I just ran them from terminal\n",
    "\"\"\"\n",
    "%wget https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
    "%protoc --python_out=. sentencepiece_model.proto\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece_model_pb2 as spmp\n",
    "\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "print('the loaded model has pieces:', len(m.pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pieces = []\n",
    "for idx in kept_ids:\n",
    "    new_pieces.append(m.pieces[idx])\n",
    "    \n",
    "print('the new pieces:', len(new_pieces))\n",
    "\n",
    "# replace the content of the first 27K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "for i in trange(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "\n",
    "print(len(m.pieces))\n",
    "with open('new_sp.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())\n",
    "\n",
    "new_tokenizer = T5Tokenizer('new_sp.model', extra_ids=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_sentinels():\n",
    "    sentinel_old = tokenizer.get_sentinel_tokens()\n",
    "    sentinel_new = new_tokenizer.get_sentinel_tokens()\n",
    "    ids = tokenizer.get_sentinel_token_ids()\n",
    "    temporary_list = []\n",
    "    for a_token in sentinel_new:\n",
    "        token_idx = sentinel_old.index(a_token)\n",
    "        temporary_list.append(ids[token_idx])\n",
    "    kept_ids.extend(temporary_list)\n",
    "\n",
    "reorder_sentinels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #imports PyTorch\n",
    "\n",
    "#this updates the neural network by replacing the parameters of its input and output embeddings. (yes, I copied and pasted that lol)\n",
    "#since this is just a UMT5Model not a T5ForConditionalGeneration, it doesn't have a language processing head, so I deleted the lines involving lm_head\n",
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "model.shared.weight = new_emb.weight\n",
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = 'cointegrated/arabt5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained('ar-umt5-base')\n",
    "model.save_pretrained('ar-umt5-base')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_igloo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
