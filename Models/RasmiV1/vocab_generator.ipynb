{"cells":[{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_text as text\n","import functools\n","from datasets import load_dataset\n","from datasets import Dataset\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["tf.get_logger().setLevel('ERROR')\n","pwd = pathlib.Path.cwd()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loads 100k-words dataset (credit: @ammar)\n","dataset = load_dataset(\"dot-ammar/AR-dotless-small\")"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["#build tf.data.Dataset for clean column from HuggingFace dataset\n","train_clean = dataset['train'].to_tf_dataset(\n","    columns = 'clean',\n","    prefetch = False\n",")"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["#same for dotless column\n","train_dtl = dataset['train'].to_tf_dataset(\n","    columns = 'dotless',\n","    prefetch = False\n",")"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 38min 32s, sys: 20.4 s, total: 38min 53s\n","Wall time: 41min 17s\n"]}],"source":["#generate vocabulary for 'clean' text with 50000 words/subwords\n","\n","%%time\n","clean_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_clean,\n","    vocab_size = 50000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["#function to write vocab to file\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["#produce vocab file for clean_vocab\n","write_vocab_file('clean_vocab.txt', clean_vocab)"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 34min 17s, sys: 17.9 s, total: 34min 35s\n","Wall time: 38min 58s\n"]}],"source":["#generate vocabulary for 'dotless' text with 35000 words/subwords\n","\n","%%time\n","dotless_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_dtl,\n","    vocab_size = 35000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["#produce vocab file for dotless_vocab\n","write_vocab_file('dotless_vocab.txt', dotless_vocab)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMww1yRR4Bn3NJu71L7gPwM","mount_file_id":"1rakpVRUXtXSYy4THUyJi-5pl0AqYIsJQ","provenance":[{"file_id":"1w6HcbAvQHX1oIitDH8kVslThSG7WiNrr","timestamp":1691529436041}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
