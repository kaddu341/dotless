{"cells":[{"cell_type":"markdown","metadata":{},"source":["بسم الله الرحمن الرحيم \n","\n","Credit: Adapted from https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_text as text\n","import functools\n","from datasets import load_dataset\n","from datasets import Dataset\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["tf.get_logger().setLevel('ERROR')\n","pwd = pathlib.Path.cwd()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["#loads 100k-words dataset (credit: @ammar)\n","dataset = load_dataset(\"dot-ammar/AR-dotless-small\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["#build tf.data.Dataset for clean column from HuggingFace dataset\n","train_clean = dataset['train'].to_tf_dataset(\n","    columns = 'clean',\n","    prefetch = False\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["#same for dotless column\n","train_dtl = dataset['train'].to_tf_dataset(\n","    columns = 'dotless',\n","    prefetch = False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#commented out so that I don't accidentally run it\n","\n","\"\"\"\n","#generate vocabulary for 'clean' text with 50000 words/subwords\n","\n","%%time\n","clean_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_clean,\n","    vocab_size = 50000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#function to write vocab to file\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)\n","\n","#produce vocab file for clean_vocab\n","write_vocab_file('clean_vocab.txt', clean_vocab)\n","\n","#generate vocabulary for 'dotless' text with 35000 words/subwords\n","\n","%%time\n","dotless_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_dtl,\n","    vocab_size = 35000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#produce vocab file for dotless_vocab\n","write_vocab_file('dotless_vocab.txt', dotless_vocab)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["clean_tokenizer = text.BertTokenizer('clean_vocab.txt')\n","dotless_tokenizer = text.BertTokenizer('dotless_vocab.txt')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMww1yRR4Bn3NJu71L7gPwM","mount_file_id":"1rakpVRUXtXSYy4THUyJi-5pl0AqYIsJQ","provenance":[{"file_id":"1w6HcbAvQHX1oIitDH8kVslThSG7WiNrr","timestamp":1691529436041}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
