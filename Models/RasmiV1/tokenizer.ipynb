{"cells":[{"cell_type":"markdown","metadata":{},"source":["بسم الله الرحمن الرحيم \n","\n","Credit: Adapted from https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_text as text\n","import functools\n","from datasets import load_dataset\n","from datasets import Dataset\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["tf.get_logger().setLevel('ERROR')\n","pwd = pathlib.Path.cwd()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loads 100k-words dataset (credit: @ammar)\n","dataset = load_dataset(\"dot-ammar/AR-dotless-small\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#build tf.data.Dataset for clean column from HuggingFace dataset\n","train_clean = dataset['train'].to_tf_dataset(\n","    columns = 'clean',\n","    prefetch = False\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#same for dotless column\n","train_dtl = dataset['train'].to_tf_dataset(\n","    columns = 'dotless',\n","    prefetch = False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#commented out so that I don't accidentally run it\n","\n","\"\"\"\n","#generate vocabulary for 'clean' text with 50000 words/subwords\n","\n","%%time\n","clean_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_clean,\n","    vocab_size = 50000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#function to write vocab to file\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)\n","\n","#produce vocab file for clean_vocab\n","write_vocab_file('clean_vocab.txt', clean_vocab)\n","\n","#generate vocabulary for 'dotless' text with 35000 words/subwords\n","\n","%%time\n","dotless_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_dtl,\n","    vocab_size = 35000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#produce vocab file for dotless_vocab\n","write_vocab_file('dotless_vocab.txt', dotless_vocab)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def build_vocab(filepath):\n","  vocab_list = []\n","  f = open(filepath, \"r\")\n","  for aLine in f:\n","    vocab_list.append(aLine)\n","  return vocab_list"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["clean_vocab = build_vocab('clean_vocab.txt')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["clean_tokenizer = text.BertTokenizer('clean_vocab.txt')\n","dotless_tokenizer = text.BertTokenizer('dotless_vocab.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def decode_string(ints):\n","  strs = [chr(i) for i in ints]\n","  joined = [''.join(strs)]\n","  return joined"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['في حادثة تؤكد الواقع السي للصحافة السورية والاستهتار بها والحواجز التي تصطدم بها في عملها أصدر مدير فرع الهيئة المركزية للرقابة والتفتيش في السويدا قرارا يتضمن منع دخول مراسلة الوطن في المحافظة لمقر الفرعصحيفة الوطن التي أوردت الخبر تسالت عن السر الذي يمكن أن يخفيه المدير الذي يمنع صحفي من الدخول إلى دائرة حكومية متناسيا أنها ملك عام ولايحق له اتخاذ مثل هذا القرار مشيرة إلى أن المراسلة حاولت أكثر من مرة لقا المدير وسؤاله عن سبب إصدار القرار لكنه رفض لقاهاتحليلات الصحيفة وصلت إلى أن القرار يمكن أن يعود لقيامها سابقا بالإضاة على أماكن كثيرة من الخلل في عمل التفتيش عموما ومدير فرع الهيئة خصوصا إضافة إلى تسليط الضو على التجاوزات والقرارات التعسفية بحق كثير من المشتكين ممن تقدموا بشكواهم خطيا إلى الصحيفةيشار إلى أن الصحفيين في سوريا يتعرضون للكثير من المواقف المماثلة ويعانون الكثير للحصول على المعلومات في ظل قرارات حكومية لاتمكنهم الحصول عليها بيسر وسهولة حيث سبق أن منع مدير مدرسة خاصة في دمشق مراسل صحيفة تشرين من الدخول والحصول على المعلومات اللازمة لإنجاز تقرير صحفي عن المدارس الخاصةأخبارنا أخر الأخبارالسويدا الفساد الهيئة المركزية للرقابة والتفتيش قرارات الهيئة الرمكزية للرقابة والتفتيش مدير هيئة الرقابة والتفتيش في السويدا معاناة الصحفيين السوريين منع صحفي من دخول مؤسسة حكومية']]\n","[['وصف الموقع موقع للبنات يحتوي علي العديد من ألعاب فلاش ألعاب للموبايل والأقسام المميزة التي تحبها كل البنات أيضا الأطفال الصغار والأولاد هيا إستمتعوا معنا الأن بأجدد الألعاب']]\n","[['مونديال يكشف عن أرقام وإحصائيات أيوب الكعبي مع منتخب المغرب مونديال إحصائيات نتائج أرقام أخبار هيد تو هيدمونديال لأول مرة يكشف مونديال أول نظام عربي عالمي للإحصائيات والأرقام القياسية عن أرقام وإحصائيات الدولي المغربي أيوب العربي لاعب نهضة بركان الذي سجل هدفا لمنتخب بلاده في المباراة الودية التي جمعت أسود الأطلس وأوزبكستان وفيها فاز المنتخب المغربي بهدفين نظيفينوكان لقا أوزبكستان هو اللقا الدولي الثامن لأيوب الذي ارتبط إسمه بالزمالك في الفترة الماضية ووصلت لحد القول بأنه كان صفقة القرن وأحرز اللاعب أهداف في مبارياته الثمانية كان آخرها في لقا المغرب الودي أمام أوزبكستانوبحسب مونديال فبعد إحراز الكعبي لهدف في شباك أوزبكستان رفع رصيده من الأهداف إلي هدفا في هذا العام أندية ومنتخبات ليصل إلي المركز الثاني عشر في قائمة هدافي العالم متساويا مع مواطنيه وليد أزارو مهاجم الأهلي المصري وعبد الرازق حمد الله لاعب الريان القطري ومتخطيا نجم بايرن ميونيخ روبرت ليفاندوفسكي هدف ويتصدر هذه القائمة النجم البرتغالي كريستيانو رونالدو ب هدفا']]\n"]}],"source":["for cln in train_clean.take(3):\n","    # Tokenize the examples -> (batch, word, word-piece)\n","    token_batch = clean_tokenizer.tokenize(cln)\n","    # Merge the word and word-piece axes -> (batch, tokens)\n","    token_batch = token_batch.merge_dims(-2,-1)\n","\n","    words = clean_tokenizer.detokenize(token_batch)\n","    aTensor = tf.strings.reduce_join(words, separator=' ', axis=-1)\n","\n","    decoded = tf.strings.unicode_decode(aTensor, 'utf-8').numpy()\n","    decoded_list = [decode_string(ex) for ex in decoded]\n","    print(decoded_list)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMww1yRR4Bn3NJu71L7gPwM","mount_file_id":"1rakpVRUXtXSYy4THUyJi-5pl0AqYIsJQ","provenance":[{"file_id":"1w6HcbAvQHX1oIitDH8kVslThSG7WiNrr","timestamp":1691529436041}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
