{"cells":[{"cell_type":"markdown","metadata":{},"source":["بسم الله الرحمن الرحيم \n","\n","Credit: Adapted from https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_text as text\n","import functools\n","from datasets import load_dataset\n","from datasets import Dataset\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["tf.get_logger().setLevel('ERROR')\n","pwd = pathlib.Path.cwd()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loads 100k-words dataset (credit: @ammar)\n","dataset = load_dataset(\"dot-ammar/AR-dotless-small\")"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["indexList = []\n","i = 0\n","for entry in dataset[\"train\"][\"clean\"]:\n","    if len(entry.split()) >= 301:\n","        indexList.append(i)\n","    i += 1\n","\n","filtered_dataset = dataset[\"train\"].select(indexList)"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["301\n"]}],"source":["minVal = len(filtered_dataset[\"clean\"][0])\n","\n","for entry in filtered_dataset[\"clean\"]:\n","    if len(entry.split()) < minVal:\n","        minVal = len(entry.split())\n","\n","print(minVal)"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[],"source":["#build tf.data.Dataset for clean column from HuggingFace dataset\n","train_clean = filtered_dataset.to_tf_dataset(\n","    columns = 'clean',\n","    prefetch = False\n",")"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["#same for dotless column\n","train_dtl = filtered_dataset.to_tf_dataset(\n","    columns = 'dotless',\n","    prefetch = False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#commented out so that I don't accidentally run it\n","\n","\"\"\"\n","#generate vocabulary for 'clean' text with 50000 words/subwords\n","\n","%%time\n","clean_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_clean,\n","    vocab_size = 50000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#function to write vocab to file\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)\n","\n","#produce vocab file for clean_vocab\n","write_vocab_file('clean_vocab.txt', clean_vocab)\n","\n","#generate vocabulary for 'dotless' text with 35000 words/subwords\n","\n","%%time\n","dotless_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_dtl,\n","    vocab_size = 35000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#produce vocab file for dotless_vocab\n","write_vocab_file('dotless_vocab.txt', dotless_vocab)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":["def build_vocab(filepath):\n","  vocab_list = []\n","  f = open(filepath, \"r\")\n","  for aLine in f:\n","    vocab_list.append(aLine[0:len(aLine) - 1])\n","  return vocab_list"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[],"source":["clean_vocab = build_vocab('clean_vocab.txt')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["clean_tokenizer = text.BertTokenizer('clean_vocab.txt')\n","dotless_tokenizer = text.BertTokenizer('dotless_vocab.txt')"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["def decode_string(ints):\n","  strs = [chr(i) for i in ints]\n","  joined = [''.join(strs)]\n","  return joined"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[],"source":["_MAX_SEQ_LEN = 301\n","_START_TOKEN = clean_vocab.index(\"[START]\")\n","_END_TOKEN = clean_vocab.index(\"[END]\")"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[],"source":["tensorList = []\n","for clean in train_clean:\n","    # Tokenize the examples -> (batch, word, word-piece)\n","    token_batch = clean_tokenizer.tokenize(clean)\n","    # Merge the word and word-piece axes -> (batch, tokens)\n","    token_batch = token_batch.merge_dims(-2,-1)\n","\n","    tensorList.append(token_batch)\n","    \n","    #Detokenization test\n","    \"\"\"\n","    words = clean_tokenizer.detokenize(token_batch)\n","    aTensor = tf.strings.reduce_join(words, separator=' ', axis=-1)\n","    decoded = tf.strings.unicode_decode(aTensor, 'utf-8').numpy()\n","    decoded_list = [decode_string(ex) for ex in decoded]\n","    print(decoded_list)\n","    \"\"\""]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN, axis = -1)\n","trimmed = trimmer.trim(tensorList)"]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[],"source":["clean_segments_combined, clean_segments_ids = text.combine_segments(\n","  trimmed,\n","  start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"data":{"text/plain":["(<tf.RaggedTensor [[2, 359, 3, ..., 3, 3, 3]]>,\n"," <tf.RaggedTensor [[0, 0, 0, ..., 23594, 23595, 23596]]>)"]},"execution_count":184,"metadata":{},"output_type":"execute_result"}],"source":["clean_segments_combined, clean_segments_ids"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["clean_tokenized = tf.data.Dataset.from_tensors(clean_segments_combined)\n","clean_tokenized_ids = tf.data.Dataset.from_tensors(clean_segments_ids)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMww1yRR4Bn3NJu71L7gPwM","mount_file_id":"1rakpVRUXtXSYy4THUyJi-5pl0AqYIsJQ","provenance":[{"file_id":"1w6HcbAvQHX1oIitDH8kVslThSG7WiNrr","timestamp":1691529436041}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
