{"cells":[{"cell_type":"markdown","metadata":{},"source":["بسم الله الرحمن الرحيم \n","\n","Credit: Adapted from https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_text as text\n","import functools\n","from datasets import load_dataset\n","from datasets import Dataset\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["tf.get_logger().setLevel('ERROR')\n","pwd = pathlib.Path.cwd()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#loads 100k-words dataset (credit: @ammar)\n","dataset = load_dataset(\"dot-ammar/AR-dotless-small\")"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'DatasetDict' object has no attribute 'select'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[169], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         indexList\u001b[39m.\u001b[39mappend(i)\n\u001b[1;32m      6\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m filtered_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mselect(indexList)\n","\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'select'"]}],"source":["indexList = []\n","i = 0\n","for entry in dataset[\"train\"][\"clean\"]:\n","    if len(entry.split()) >= 301:\n","        indexList.append(i)\n","    i += 1\n","\n","filtered_dataset = dataset.select(indexList)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#build tf.data.Dataset for clean column from HuggingFace dataset\n","train_clean = dataset['train'].to_tf_dataset(\n","    columns = 'clean',\n","    prefetch = False\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#same for dotless column\n","train_dtl = dataset['train'].to_tf_dataset(\n","    columns = 'dotless',\n","    prefetch = False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Generate Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#commented out so that I don't accidentally run it\n","\n","\"\"\"\n","#generate vocabulary for 'clean' text with 50000 words/subwords\n","\n","%%time\n","clean_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_clean,\n","    vocab_size = 50000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#function to write vocab to file\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)\n","\n","#produce vocab file for clean_vocab\n","write_vocab_file('clean_vocab.txt', clean_vocab)\n","\n","#generate vocabulary for 'dotless' text with 35000 words/subwords\n","\n","%%time\n","dotless_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset = train_dtl,\n","    vocab_size = 35000,\n","    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")\n","\n","#produce vocab file for dotless_vocab\n","write_vocab_file('dotless_vocab.txt', dotless_vocab)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[],"source":["def build_vocab(filepath):\n","  vocab_list = []\n","  f = open(filepath, \"r\")\n","  for aLine in f:\n","    vocab_list.append(aLine[0:len(aLine) - 1])\n","  return vocab_list"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[],"source":["clean_vocab = build_vocab('clean_vocab.txt')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["clean_tokenizer = text.BertTokenizer('clean_vocab.txt')\n","dotless_tokenizer = text.BertTokenizer('dotless_vocab.txt')"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["def decode_string(ints):\n","  strs = [chr(i) for i in ints]\n","  joined = [''.join(strs)]\n","  return joined"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[],"source":["tensorList = []\n","for cln in train_clean.take(3):\n","    # Tokenize the examples -> (batch, word, word-piece)\n","    token_batch = clean_tokenizer.tokenize(cln)\n","    # Merge the word and word-piece axes -> (batch, tokens)\n","    token_batch = token_batch.merge_dims(-2,-1)\n","\n","    tensorList.append(token_batch)\n","    \n","    #Detokenization test\n","    \"\"\"\n","    words = clean_tokenizer.detokenize(token_batch)\n","    aTensor = tf.strings.reduce_join(words, separator=' ', axis=-1)\n","    decoded = tf.strings.unicode_decode(aTensor, 'utf-8').numpy()\n","    decoded_list = [decode_string(ex) for ex in decoded]\n","    print(decoded_list)\n","    \"\"\""]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[],"source":["_START_TOKEN = clean_vocab.index(\"[START]\")\n","_END_TOKEN = clean_vocab.index(\"[END]\")"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[],"source":["_MAX_SEQ_LEN = 100\n","trimmer = text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN, axis = -1)\n","trimmed = trimmer.trim(tensorList)"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/plain":["(<tf.RaggedTensor [[2, 39, 5467, 2699, 643, 15081, 10248, 607, 10001, 43089, 110, 8797,\n","   44995, 47, 41504, 110, 39, 4169, 4998, 414, 1771, 671, 2349, 18385,\n","   36101, 39, 9844, 4301, 3667, 1695, 1371, 16335, 536, 39, 1346, 3, 2149,\n","   398, 175, 4928, 1830, 97, 253, 40, 2638, 5508, 2638, 13494, 31986,\n","   46946, 3246, 47, 37275, 62, 4636, 136, 581, 5040, 41221, 12123, 6080,\n","   18644, 75, 2071, 7013, 10, 949, 15987, 1788, 3, 7234, 4291, 45, 5577,\n","   36, 47032, 6085, 18927, 51, 1809, 603, 7234, 16718, 780, 5577, 789, 100,\n","   131, 17679, 100, 5096, 28638, 528, 2368, 257, 4291, 7234, 260, 311,\n","   1457, 6516, 29370, 8383, 3]]>,\n"," <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n","   2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","   2, 2, 2, 2, 2, 2, 2, 2]]>)"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["segments_combined, segments_ids = text.combine_segments(\n","  trimmed,\n","  start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n","segments_combined, segments_ids"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMww1yRR4Bn3NJu71L7gPwM","mount_file_id":"1rakpVRUXtXSYy4THUyJi-5pl0AqYIsJQ","provenance":[{"file_id":"1w6HcbAvQHX1oIitDH8kVslThSG7WiNrr","timestamp":1691529436041}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
