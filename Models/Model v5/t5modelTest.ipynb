{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam(model, tokenizer, input_text, num_beams=5, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "\n",
    "    # Set an initial token for the decoder input\n",
    "    initial_decoder_input_ids = tf.constant([[tokenizer.pad_token_id]])\n",
    "\n",
    "    beam_output = model.generate(input_ids, decoder_input_ids=initial_decoder_input_ids, max_length=max_length, num_beams=num_beams, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=1.0)\n",
    "\n",
    "    # Decode the beam search output\n",
    "    decoded_sequences = tokenizer.batch_decode(beam_output.numpy(), skip_special_tokens=True)\n",
    "\n",
    "    # Access logits from the model output\n",
    "    logits = model(input_ids, decoder_input_ids=initial_decoder_input_ids).logits\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # Print each translation and its probability\n",
    "    for sequence, probability in zip(decoded_sequences, probabilities):\n",
    "        print(f\"Translation: {sequence}, Probability: {probability.numpy().max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ammar/Developer/git-repos/dotless/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2024-03-03 20:55:37.714327: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n",
      "2024-03-03 20:55:37.789974: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n",
      "2024-03-03 20:55:37.917122: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n",
      "2024-03-03 20:55:37.927121: I external/local_xla/xla/service/service.cc:168] XLA service 0x2b495c870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-03 20:55:37.927136: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-03-03 20:55:37.943512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709517337.985882  969352 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-03-03 20:55:37.986823: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
      "2024-03-03 20:55:38.062970: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: hello hello my name, Probability: 0.5881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 20:55:38.156548: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n",
      "2024-03-03 20:55:38.253404: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n",
      "2024-03-03 20:55:38.307290: E ./tensorflow/core/kernels/cast_op_impl.h:64] IMPORTANT! The input tensor to Cast contains values out of range for the target type. This is undefined behavior and likely a bug in your model. A crash immediately after this under ubsan is expected.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"hello my name is ammar\"\n",
    "generate_beam(model, tokenizer, input_text, num_beams=3, max_length=len(input_text.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  16449536  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  35330816  \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  41625344  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60506624 (230.81 MB)\n",
      "Trainable params: 60506624 (230.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0:\n",
      "Layer 0:<transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention object at 0x3100e66d0>\n",
      "Layer 1:<transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention object at 0x30b987fd0>\n",
      "Layer 2:<transformers.models.t5.modeling_tf_t5.TFT5LayerFF object at 0x338389410>\n",
      "\n",
      "\n",
      "Block 1:\n",
      "Layer 0:<transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention object at 0x30b90d490>\n",
      "Layer 1:<transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention object at 0x33546a1d0>\n",
      "Layer 2:<transformers.models.t5.modeling_tf_t5.TFT5LayerFF object at 0x30b925d50>\n",
      "\n",
      "\n",
      "Block 2:\n",
      "Layer 0:<transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention object at 0x30b929a90>\n",
      "Layer 1:<transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention object at 0x30b92ddd0>\n",
      "Layer 2:<transformers.models.t5.modeling_tf_t5.TFT5LayerFF object at 0x30b936350>\n",
      "\n",
      "\n",
      "Block 3:\n",
      "Layer 0:<transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention object at 0x30b93dfd0>\n",
      "Layer 1:<transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention object at 0x30b946690>\n",
      "Layer 2:<transformers.models.t5.modeling_tf_t5.TFT5LayerFF object at 0x30b94a950>\n",
      "\n",
      "\n",
      "Block 4:\n",
      "Layer 0:<transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention object at 0x30b94e490>\n",
      "Layer 1:<transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention object at 0x30b95ed50>\n",
      "Layer 2:<transformers.models.t5.modeling_tf_t5.TFT5LayerFF object at 0x30b967150>\n",
      "\n",
      "\n",
      "Block 5:\n",
      "Layer 0:<transformers.models.t5.modeling_tf_t5.TFT5LayerSelfAttention object at 0x3354f7950>\n",
      "Layer 1:<transformers.models.t5.modeling_tf_t5.TFT5LayerCrossAttention object at 0x30b977310>\n",
      "Layer 2:<transformers.models.t5.modeling_tf_t5.TFT5LayerFF object at 0x30ba03a90>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, block in enumerate(model.decoder.block):\n",
    "    print(\"Block\", str(i) + \":\")\n",
    "    for j, layer in enumerate(block.layer):\n",
    "        print(f\"Layer {j}:{layer}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
